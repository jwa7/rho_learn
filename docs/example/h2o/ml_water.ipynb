{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Learning the electron density of water"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-requisites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMPORTANT!\n",
    "\n",
    "This notebook isn't yet fully runnable as it is required to have a specific\n",
    "binary for the (not open-source) Quantum Chemistry code FHI-aims."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install the required packages\n",
    "\n",
    "1. `pip install metatensor`\n",
    "1. `pip install chemiscope`\n",
    "1. `pip install\n",
    "   git+https://github.com/luthaf/rascaline.git@b2cedfe870541e6d037357db58de1901eb116c41`\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**NOTE**: this notebook has only been tested on the HPC system \"jed\". Due to\n",
    "specific compilation of the quantum chemistry package `FHI-aims`, the Python\n",
    "interface to running QC calculations may not be generalized to other operating\n",
    "systems or hardware."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Useful standard and scientific ML libraries\n",
    "import os\n",
    "import time\n",
    "import ase.io\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import py3Dmol\n",
    "import torch\n",
    "\n",
    "# M-Stack packages\n",
    "import metatensor   # storage format for atomistic ML\n",
    "import chemiscope  # interactive molecular visualization\n",
    "import rascaline   # generating structural representations\n",
    "from metatensor import Labels, TensorBlock, TensorMap\n",
    "from rascaline.utils import clebsch_gordan\n",
    "\n",
    "# Interfacing with FHI-aims\n",
    "from rhocalc.aims import aims_calc, aims_parser\n",
    "\n",
    "# Torch-based density leaning\n",
    "from rholearn import io, data, loss, models, predictor\n",
    "import settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from settings import data_dir\n",
    "\n",
    "# The absolute paths to 3 directories need to be defined, all as callables that\n",
    "# take as input the structure index. They are defined in a hierarchical manner.\n",
    "\n",
    "# The directory where the SCF calculation outputs will be saved\n",
    "def scf_dir(A):\n",
    "    return os.path.join(data_dir, f\"{A}\")\n",
    "\n",
    "# The directory where the SCF restart will be read and RI calculation run (and\n",
    "# outputs saved)\n",
    "def ri_dir(A, restart_idx):\n",
    "    return os.path.join(scf_dir(A), f\"{restart_idx}\")\n",
    "\n",
    "# The directory where the RI calculation outputs will be processed from text\n",
    "# files to metatensor format and saved.\n",
    "def processed_dir(A, restart_idx):\n",
    "    return os.path.join(ri_dir(A, restart_idx), \"processed\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Visualize structures in dataset\n",
    "\n",
    "* Use `chemiscope`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "011d108c30c746c79517f9bbdbc61e69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ChemiscopeWidget(value='{\"meta\": {\"name\": \" \"}, \"structures\": [{\"size\": 3, \"names\": [\"O\", \"H\", \"H\"], \"x\": [0.0…"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from settings import idxs, frames, all_frames\n",
    "\n",
    "chemiscope.show(\n",
    "    frames,\n",
    "    properties={\n",
    "        \"Mean O-H bond length, Angstrom\": [np.mean([f.get_distance(0, 1), f.get_distance(0, 2)]) for f in frames],\n",
    "        \"H-O-H angle, degrees\": [f.get_angle(1, 0, 2) for f in frames],\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate learning targets\n",
    "\n",
    "* These are the RI coefficients of the HOMO eigenstate\n",
    "* We generate these in 2 steps: 1) SCF and 2) RI fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Converge SCF for each structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 11396302\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sbatch: [ESTIMATION] The estimated cost of this job is CHF 11.20\n",
      "sbatch: [CAPPING]    All users of the account cosmo-erc have consumed 117.06 CHF\n",
      "sbatch: [CAPPING]    In addition, based on queued and running jobs all users of the account cosmo-erc will consume up to 0.00 CHF\n",
      "sbatch: [CAPPING]    Your username abbott has consumed 4.99 CHF\n",
      "sbatch: [CAPPING]    In addition, based on queued and running jobs your username abbott will consume up to 0.00 CHF\n",
      "sbatch: ╭──────────────────────────────┬─────────────┬─────────────┬─────────────╮\n",
      "sbatch: │ [in CHF]                     │ Capping     │ Consumed    │ Queued      │\n",
      "sbatch: ├──────────────────────────────┼─────────────┼─────────────┼─────────────┤\n",
      "sbatch: │ username : abbott            │ 0           │ 5.0         │ 0.0         │\n",
      "sbatch: ├──────────────────────────────┼─────────────┼─────────────┼─────────────┤\n",
      "sbatch: │ account : cosmo-erc          │ 1,610.653   │ 117.1       │ 0.0         │\n",
      "sbatch: ╰──────────────────────────────┴─────────────┴─────────────┴─────────────╯\n"
     ]
    }
   ],
   "source": [
    "# Import the settings needed to run FHI-aims\n",
    "from settings import aims_path, base_aims_kwargs, scf_kwargs, ri_kwargs, sbatch_kwargs\n",
    "\n",
    "# Build a dict of settings for each calculation (i.e. structure)\n",
    "# IMPORTANT: zip() is used to pair up the structure index and the structure\n",
    "calcs = {\n",
    "    A: {\"atoms\": frame, \"run_dir\": scf_dir(A)} for A, frame in zip(idxs, frames)\n",
    "}\n",
    "\n",
    "# And the general settings for all calcs\n",
    "aims_kwargs = base_aims_kwargs.copy()\n",
    "aims_kwargs.update(scf_kwargs)\n",
    "\n",
    "# Run the SCF in AIMS\n",
    "aims_calc.run_aims_array(\n",
    "    calcs=calcs,\n",
    "    aims_path=aims_path,\n",
    "    aims_kwargs=aims_kwargs,\n",
    "    sbatch_kwargs=sbatch_kwargs,\n",
    "    run_dir=scf_dir,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All SCF calculations converged!\n"
     ]
    }
   ],
   "source": [
    "# Running this code will ensure that all calculations have finished.\n",
    "# Should take < 10 seconds (1 node, 10 cores on 'jed' HPC)\n",
    "all_finished = False\n",
    "while not all_finished:\n",
    "    calcs_finished = []\n",
    "    for A in idxs:\n",
    "        aims_out_path = os.path.join(scf_dir(A), \"aims.out\")\n",
    "        if os.path.exists(aims_out_path):\n",
    "            with open(aims_out_path, \"r\") as f:\n",
    "                calcs_finished.append(\"Leaving FHI-aims.\" in f.read())  # AIMS finished\n",
    "        else:\n",
    "            calcs_finished.append(False)\n",
    "    all_finished = np.all(calcs_finished)\n",
    "\n",
    "# Check SCF converged\n",
    "converged = []\n",
    "for A in idxs:\n",
    "    aims_out_path = os.path.join(scf_dir(A), \"aims.out\")\n",
    "    if os.path.exists(aims_out_path):\n",
    "        with open(aims_out_path, \"r\") as f:\n",
    "            converged.append(\n",
    "                \"Self-consistency cycle converged.\" in f.read()\n",
    "            )  # calculation converged\n",
    "    else:\n",
    "        converged.append(False)\n",
    "if np.all(converged):\n",
    "    print(\"All SCF calculations converged!\")\n",
    "else:\n",
    "    print(\n",
    "        \"Some SCF calculations did not converge. structure idxs: \",\n",
    "        [A for i, A in enumerate(idxs) if not converged[i]],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Perform RI fitting on the scalar field of interest (HOMO)\n",
    "\n",
    "* First we need to identify the HOMO\n",
    "* This can be done by parsing the Kohn Sham orbital information from the\n",
    "  converged SCF calculation. \n",
    "* This info got written to file \"ks_orbital_info.out\" above by using the\n",
    "  `ri_fit_write_ks_orb_info: True` keyword set in `scf_kwargs`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sbatch: [ESTIMATION] The estimated cost of this job is CHF 11.20\n",
      "sbatch: [CAPPING]    All users of the account cosmo-erc have consumed 117.07 CHF\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 11396511\n",
      "Submitted batch job 11396613\n"
     ]
    }
   ],
   "source": [
    "for restart_idx in [0, 1]:  # HOMO, LUMO\n",
    "\n",
    "    # Write KSO weights for just the HOMO to file\n",
    "    for A in idxs:\n",
    "        # Parse the Kohn-Sham\n",
    "        ks_info = aims_parser.get_ks_orbital_info(\n",
    "            os.path.join(scf_dir(A), \"ks_orbital_info.out\")\n",
    "        )\n",
    "        weights = np.zeros(ks_info.shape[0])\n",
    "        if restart_idx == 0: # Find HOMO states\n",
    "            kso_idxs = aims_parser.find_homo_kso_idxs(ks_info)\n",
    "        elif restart_idx == 1:\n",
    "            kso_idxs = aims_parser.find_lumo_kso_idxs(ks_info)\n",
    "        else:\n",
    "            raise ValueError(\"...\")\n",
    "\n",
    "        # Add a weighting of 1.0\n",
    "        for kso_idx in kso_idxs:  # these are 1-indexed in AIMS\n",
    "            weights[kso_idx - 1] = 1.0\n",
    "\n",
    "        # Save these to file so AIMS can read them in\n",
    "        np.savetxt(os.path.join(scf_dir(A), \"ks_orbital_weights.in\"), weights)\n",
    "\n",
    "    # And the general settings for all calcs\n",
    "    aims_kwargs = base_aims_kwargs.copy()\n",
    "    aims_kwargs.update(ri_kwargs)\n",
    "\n",
    "    # Run the RI fitting procedure in AIMS\n",
    "    aims_calc.run_aims_array(\n",
    "        calcs=calcs,\n",
    "        aims_path=aims_path,\n",
    "        aims_kwargs=aims_kwargs,\n",
    "        sbatch_kwargs=sbatch_kwargs,\n",
    "        run_dir=scf_dir,\n",
    "        restart_idx=restart_idx,   # Restart from a converged SCF\n",
    "        copy_files=[\"ks_orbital_weights.in\"],  # Copy the KS weights into the restart dir\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All RI calculations finished!\n",
      "All RI calculations finished!\n"
     ]
    }
   ],
   "source": [
    "for restart_idx in [0, 1]:\n",
    "    # Running this code will ensure that all calculations have finished\n",
    "    # Should take < 30 seconds (1 node, 10 cores, on 'jed' HPC)\n",
    "    all_finished = False\n",
    "    while not all_finished:\n",
    "        calcs_finished = []\n",
    "        for A in idxs:\n",
    "            aims_out_path = os.path.join(ri_dir(A, restart_idx), \"aims.out\")\n",
    "            if os.path.exists(aims_out_path):\n",
    "                with open(aims_out_path, \"r\") as f:\n",
    "                    calcs_finished.append(\"Leaving FHI-aims.\" in f.read())  # AIMS finished\n",
    "            else:\n",
    "                calcs_finished.append(False)\n",
    "        all_finished = np.all(calcs_finished)\n",
    "\n",
    "    print(\"All RI calculations finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean density fitting error across all structures (%): 0.3762254669830918\n",
      "Mean density fitting error across all structures (%): 2.6293376433963145\n"
     ]
    }
   ],
   "source": [
    "for restart_idx in [0, 1]:\n",
    "    # Process AIMS results and print density fitting error\n",
    "    # Run in serial: takes approx 0.5 seconds per structure\n",
    "    df_errors = []\n",
    "    for A, frame in zip(idxs, frames):\n",
    "        aims_parser.process_aims_ri_results(\n",
    "            frame=frame,\n",
    "            aims_output_dir=ri_dir(A, restart_idx),  # includes the restart idx\n",
    "            process_what=[\"coeffs\", \"ovlp\"],\n",
    "            structure_idx=A,\n",
    "        )\n",
    "        calc_info = io.unpickle_dict(os.path.join(processed_dir(A, restart_idx), \"calc_info.pickle\"))\n",
    "        df_errors.append(calc_info[\"df_error_percent\"][\"total\"])\n",
    "    print(\"Mean density fitting error across all structures (%):\", np.mean(df_errors))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Structural Descriptors \n",
    "\n",
    "* Here we construct $\\lambda$-SOAP equivariant descriptors for each structure\n",
    "* First, find the angular orders present in the decomposition of the target\n",
    "  scalar field onto the RI basis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'lmax': {'O': 8, 'H': 4}, 'nmax': {('O', 0): 9, ('O', 1): 10, ('O', 2): 9, ('O', 3): 8, ('O', 4): 6, ('O', 5): 4, ('O', 6): 4, ('O', 7): 2, ('O', 8): 1, ('H', 0): 9, ('H', 1): 7, ('H', 2): 6, ('H', 3): 3, ('H', 4): 1}}\n",
      "{'lmax': {'O': 8, 'H': 4}, 'nmax': {('O', 0): 9, ('O', 1): 10, ('O', 2): 9, ('O', 3): 8, ('O', 4): 6, ('O', 5): 4, ('O', 6): 4, ('O', 7): 2, ('O', 8): 1, ('H', 0): 9, ('H', 1): 7, ('H', 2): 6, ('H', 3): 3, ('H', 4): 1}}\n"
     ]
    }
   ],
   "source": [
    "for restart_idx in [0, 1]:\n",
    "    # The basis set definition should is consistent for all atoms in the dataset,\n",
    "    # given consistent AIMS settings. Print this definiton from the parsed outputs\n",
    "    # from one of the structures\n",
    "    basis_set = io.unpickle_dict(\n",
    "        os.path.join(processed_dir(A, restart_idx), \"calc_info.pickle\")\n",
    "    )[\"basis_set\"]\n",
    "\n",
    "    print(basis_set[\"def\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The maximum angular order here is $l = 8$. This should be reflected in the\n",
    "settings used to generate the equivariant descriptor - specifically in\n",
    "`cg_settings` in \"settings.py\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from settings import all_frames, rascal_settings, cg_settings\n",
    "\n",
    "# Generate a rascaline SphericalExpansion (2 body) representation. As we want to\n",
    "# retain the original structure indices, we are going to pass * all * of the\n",
    "# 1000 frames to rascaline, but only compute for the subset of structures in\n",
    "# `idxs`.\n",
    "calculator = rascaline.SphericalExpansion(**rascal_settings[\"hypers\"])\n",
    "nu_1_tensor = calculator.compute(\n",
    "    all_frames, \n",
    "    selected_samples=Labels(names=[\"structure\"], values=idxs.reshape(-1, 1)),\n",
    "    **rascal_settings[\"compute\"],\n",
    ")\n",
    "nu_1_tensor = nu_1_tensor.keys_to_properties(\"species_neighbor\")\n",
    "\n",
    "# Build a lambda-SOAP descriptor by a CLebsch-Gordan combination\n",
    "lsoap = clebsch_gordan.lambda_soap_vector(nu_1_tensor, **cg_settings)\n",
    "\n",
    "# Check the resulting structure indices match those in `idxs`\n",
    "assert np.all(\n",
    "    np.sort(idxs)\n",
    "    == metatensor.unique_metadata(lsoap, \"samples\", \"structure\").values.reshape(-1)\n",
    ")\n",
    "\n",
    "# Split into per-structure TensorMaps and save into separate directories.\n",
    "# This is useful for batched training.\n",
    "for A in idxs:\n",
    "    lsoap_A = metatensor.slice(\n",
    "        lsoap,\n",
    "        \"samples\",\n",
    "        labels=Labels(names=\"structure\", values=np.array([A]).reshape(-1, 1)),\n",
    "    )\n",
    "    for restart_idx in [0, 1]:\n",
    "        metatensor.save(os.path.join(processed_dir(A, restart_idx), \"lsoap.npz\"), lsoap_A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The settings used to build the descriptor from an `.xyz` file, as well as build\n",
    "the desired target property (the real-space scalar field) from the model\n",
    "prediction need to be stored so that the perform can make a true end-to-end\n",
    "prediction.\n",
    "\n",
    "In the `rholearn` module \"predictor.py\", the functions `descriptor_builder` and\n",
    "`target_builder` are implemented to perform these transformations on the input\n",
    "and output side of the model, respectively. Both take a\n",
    "variable input that depends on the structure being predicted on, and some\n",
    "settings for performing the relevant transformations of the data. The key\n",
    "physics-related settings used in predicting on an unseen structure shouldbe the\n",
    "same as were used for generating the data the model was trained on.\n",
    "\n",
    "Here we store the relevant settings in dictionaries, which will be used to\n",
    "initialize the ML model later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build `dataset`\n",
    "\n",
    "* For cross-validation we create a train-test-val split of the data.\n",
    "* Data is stored on the per-structure basis to help with mini-batching in training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from settings import crossval_settings, ml_settings, run_dir, torch_settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num train_idxs: 120    num test_idxs: 40    num val_idxs: 40\n"
     ]
    }
   ],
   "source": [
    "# Perform a train/test/val split of structure idxs\n",
    "train_idxs, test_idxs, val_idxs = data.group_idxs(\n",
    "    idxs=idxs,\n",
    "    n_groups=crossval_settings[\"n_groups\"],\n",
    "    group_sizes=crossval_settings[\"group_sizes\"],\n",
    "    shuffle=crossval_settings[\"shuffle\"],\n",
    "    seed=crossval_settings[\"seed\"],\n",
    ")\n",
    "print(\n",
    "    \"num train_idxs:\",\n",
    "    len(train_idxs),\n",
    "    \"   num test_idxs:\",\n",
    "    len(test_idxs),\n",
    "    \"   num val_idxs:\",\n",
    "    len(val_idxs),\n",
    ")\n",
    "np.savez(\n",
    "    os.path.join(run_dir, \"idxs.npz\"),\n",
    "    idxs=idxs,\n",
    "    train_idxs=train_idxs,\n",
    "    test_idxs=test_idxs,\n",
    "    val_idxs=val_idxs,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Visualise the dataset again, this time colored by its cross-validation category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d05013c67ba45cab4d8edb4d778ca98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ChemiscopeWidget(value='{\"meta\": {\"name\": \" \"}, \"structures\": [{\"size\": 3, \"names\": [\"O\", \"H\", \"H\"], \"x\": [0.0…"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crossval_category = lambda A: 0 if A in train_idxs else (1 if A in test_idxs else 2)\n",
    "\n",
    "chemiscope.show(\n",
    "    frames,\n",
    "    properties={\n",
    "        \"Mean O-H bond length, Angstrom\": [\n",
    "            np.mean([frame.get_distance(0, 1), frame.get_distance(0, 2)])\n",
    "            for A, frame in zip(idxs, frames)\n",
    "        ],\n",
    "        \"H-O-H angle, degrees\": [\n",
    "            frame.get_angle(1, 0, 2) for A, frame in zip(idxs, frames)\n",
    "        ],\n",
    "        \"0: train, 1: test, 2: val\": [crossval_category(A) for A, frame in zip(idxs, frames)]\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct dataset, defining callables to access the input, output, and overlap\n",
    "# data from the structure indices\n",
    "rho_data = data.RhoData(\n",
    "    idxs=idxs,\n",
    "    in_path=lambda A: os.path.join(processed_dir(A), \"lsoap.npz\"),\n",
    "    out_path=lambda A: os.path.join(processed_dir(A), \"ri_coeffs.npz\"),\n",
    "    aux_path=lambda A: os.path.join(processed_dir(A), \"ri_ovlp.npz\"),\n",
    "    keep_in_mem=ml_settings[\"loading\"][\"train\"][\"keep_in_mem\"],\n",
    "    **torch_settings,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize model\n",
    "\n",
    "* As typically the magnitude of the invariant RI-coefficients are many orders of\n",
    "  magnitude larger than the covariant coefficients, it often helps model\n",
    "  training if the invariant block models predict a baselined quantity. The\n",
    "  baseline is then added back in on the TensorMap level before loss evaluation,\n",
    "  acting as a kind of non-learnable bias.\n",
    "* We can compute the mean features of the invariant blocks of the training data\n",
    "  and initialize the model with this. Alternatively, one could use free-atom\n",
    "  superpositions of the scalar field of interest as the baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ml_settings[\"model\"][\"use_invariant_baseline\"]:\n",
    "    invariant_baseline = rho_data.get_invariant_means(\n",
    "        idxs=train_idxs, which_data=\"output\"\n",
    "    )\n",
    "else:\n",
    "    invariant_baseline = None\n",
    "\n",
    "for block in invariant_baseline:\n",
    "    print(block)\n",
    "    print(block.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We can also calculate the standard deviation of the training data. This is\n",
    "  defined relative to this invariant baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stddev = rho_data.get_standard_deviation(\n",
    "    idxs=train_idxs,\n",
    "    which_data=\"output\",\n",
    "    invariant_baseline=invariant_baseline,\n",
    "    use_overlaps=True,\n",
    ")\n",
    "np.savez(os.path.join(run_dir, \"stddev.npz\"), stddev=stddev.detach().numpy())\n",
    "stddev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In order to perform end-to-end predictions, we also need to initialize the\n",
    "  model with the settings used to build the equivariant descriptor and final\n",
    "  target property. These will be the same as above, used to generate the\n",
    "  training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from settings import rascal_settings, cg_settings, base_aims_kwargs\n",
    "\n",
    "# For descriptor building, we need to store the rascaline settings for\n",
    "# generating a SphericalExpansion and performing Clebsch-Gordan combinations.\n",
    "# The `descriptor_builder` function in \"predictor.py\" contains the 'recipe' for\n",
    "# using these settings to transform an ASE Atoms object.\n",
    "descriptor_kwargs = {\n",
    "    \"rascal_settings\": rascal_settings,\n",
    "    \"cg_settings\": cg_settings,\n",
    "}\n",
    "\n",
    "# For target building, the base AIMS settings need to be stored, along with the\n",
    "# basis set definition.\n",
    "basis_set = io.unpickle_dict(os.path.join(processed_dir(A), \"calc_info.pickle\"))[\"basis_set\"]\n",
    "\n",
    "target_kwargs = {\n",
    "    \"aims_kwargs\": {**base_aims_kwargs},\n",
    "    \"basis_set\": {**basis_set},\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "model = models.RhoModel(\n",
    "    # Standard model architecture\n",
    "    model_type=ml_settings[\"model\"][\"model_type\"],  # \"linear\" or \"nonlinear\"\n",
    "    input=rho_data[idxs[0]][1],   # example input data for init metadata\n",
    "    output=rho_data[idxs[0]][2],  # example output data for init metadata\n",
    "    bias_invariants=ml_settings[\"model\"][\"bias_invariants\"],\n",
    "\n",
    "    # Architecture settings if using a nonlinear base model\n",
    "    hidden_layer_widths=ml_settings[\"model\"].get(\"hidden_layer_widths\"),\n",
    "    activation_fn=ml_settings[\"model\"].get(\"activation_fn\"),\n",
    "    bias_nn=ml_settings[\"model\"].get(\"bias_nn\"),\n",
    "\n",
    "    # Settings for descriptor/target building\n",
    "    descriptor_kwargs=descriptor_kwargs,\n",
    "    target_kwargs=target_kwargs,\n",
    "\n",
    "    # Torch tensor settings\n",
    "    **torch_settings\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize training objects: loaders, loss, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct dataloaders\n",
    "train_loader = data.RhoLoader(\n",
    "    rho_data,\n",
    "    idxs=train_idxs,\n",
    "    get_aux_data=True,  # load the overlap matrix\n",
    "    batch_size=ml_settings[\"loading\"][\"train\"][\"batch_size\"],\n",
    ")\n",
    "test_loader = data.RhoLoader(\n",
    "    rho_data,\n",
    "    idxs=test_idxs,\n",
    "    get_aux_data=True,   # load the overlap matrix\n",
    "    batch_size=ml_settings[\"loading\"][\"test\"][\"batch_size\"],\n",
    ")\n",
    "# For the validation set, we want to evaluate the performance of the model\n",
    "# against the real-space scalar field (requires calling AIMS), so the overlaps\n",
    "# do not need to be loaded.\n",
    "val_loader = data.RhoLoader(\n",
    "    rho_data,\n",
    "    idxs=val_idxs,\n",
    "    get_aux_data=False,\n",
    "    batch_size=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize loss fxn and optimizer (don't use a scheduler for now)\n",
    "loss_fn = loss.L2Loss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Finally, let's update the model with some settings needed to rebuild the\n",
    "  real-space density from RI-coefficients in AIMS, needed for computing the L1 error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings specific to RI rebuild procedure\n",
    "ri_kwargs = {\n",
    "    # Force no SCF\n",
    "    \"sc_iter_limit\": 0,\n",
    "    \"postprocess_anyway\": True,\n",
    "    \"ri_fit_assume_converged\": True,\n",
    "    # What we want to do\n",
    "    \"ri_fit_rebuild_from_coeffs\": True,\n",
    "    # What we want to output\n",
    "    \"ri_fit_write_rebuilt_field\": True,\n",
    "    \"ri_fit_write_rebuilt_field_cube\": True,\n",
    "    \"output\": [\"cube ri_fit\"],  # needed for cube files\n",
    "}\n",
    "\n",
    "# Update the AIMS and SBATCH kwargs\n",
    "tmp_aims_kwargs = {**model.target_kwargs[\"aims_kwargs\"]}\n",
    "tmp_aims_kwargs.update(ri_kwargs)\n",
    "\n",
    "# Settings for slurm\n",
    "sbatch_kwargs = {\n",
    "    \"job-name\": \"h2o-pred\",\n",
    "    \"nodes\": 1,\n",
    "    \"time\": \"01:00:00\",\n",
    "    \"mem-per-cpu\": 2000,\n",
    "    \"partition\": \"standard\",\n",
    "    \"ntasks-per-node\": 10,\n",
    "}\n",
    "model.update_target_kwargs(\n",
    "    {\n",
    "        \"aims_path\": aims_path,\n",
    "        \"aims_kwargs\": tmp_aims_kwargs,\n",
    "        \"sbatch_kwargs\": sbatch_kwargs,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run model training\n",
    "\n",
    "* We train the model by gradient descent, evaluating the loss of the prediction\n",
    "  against the density-fitted quantity. As the learning target is the real-space\n",
    "  density, which has been expanded on a non-orthogonal basis, and not just the\n",
    "  RI coefficients, the overlap matrices must be used in loss evaluation:\n",
    "\n",
    "* For a structure $A$, the L2 loss of the predicted scalar field relative to the **RI\n",
    "  approximated scalar field** is defined as:\n",
    "\n",
    "  * $ \\mathcal{L}_A^{\\text{L2}} = \\left( \\textbf{c}_A^{\\text{ML}} - \\textbf{c}_A^{\\text{RI}} \\right) \\ . \\ \\hat{S}_A \\ . \\ \\left( \\textbf{c}_A^{\\text{ML}} - \\textbf{c}_A^{\\text{RI}} \\right)  $\n",
    "\n",
    "\n",
    "* And L1 (MAE) error of the of the ML prediction relative to the **true QM\n",
    "  scalar field** is defined as: \n",
    "\n",
    "  * $ \\mathcal{L}_A^{\\text{L1}} = \\frac{1}{\\int d\\textbf{r} \\rho_A^{\\text{QM}} (\\textbf{r})} \\int d\\textbf{r} \\vert \\rho_A^{\\text{ML}} (\\textbf{r}) - \\rho_A^{\\text{QM}} (\\textbf{r}) \\vert $\n",
    "  \n",
    "* While the L2 loss is used to train the model, we will also periodically\n",
    "  calculate the L1 loss to assess the performance of the model relative to the\n",
    "  actual true QM scalar field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rholearn import train\n",
    "from settings import run_dir, ml_settings\n",
    "\n",
    "# Define a directory to save model checkpoints\n",
    "chkpt_dir = os.path.join(run_dir, \"checkpoints\")\n",
    "if not os.path.exists(chkpt_dir):\n",
    "    os.makedirs(chkpt_dir)\n",
    "\n",
    "# Define a dir to save predictions - needs to be a callable that takes tructure\n",
    "# idx as an argument\n",
    "if not os.path.exists(os.path.join(run_dir, \"predictions\")):\n",
    "    os.makedirs(os.path.join(run_dir, \"predictions\"))\n",
    "\n",
    "def pred_dir(A):\n",
    "    return os.path.join(run_dir, \"predictions\", f\"{A}\")\n",
    "\n",
    "# Define a log file for writing losses at each epoch\n",
    "log_path = os.path.join(run_dir, \"training.log\")\n",
    "io.log(log_path, \"# epoch train_L2_loss test_L2_loss train_L1_error test_L1_error time\")\n",
    "\n",
    "# Run training loop\n",
    "for epoch in range(1, ml_settings[\"training\"][\"n_epochs\"] + 1):\n",
    "    t0 = time.time()\n",
    "    # Training step\n",
    "    train_loss_epoch, test_loss_epoch = train.training_step(\n",
    "        train_loader=train_loader,\n",
    "        test_loader=test_loader,\n",
    "        model=model,\n",
    "        loss_fn=loss_fn,\n",
    "        optimizer=optimizer,\n",
    "        scheduler=scheduler,\n",
    "        check_args=True\n",
    "        if epoch == 1\n",
    "        else False,  # Switch off metadata checks after 1st epoch\n",
    "    )\n",
    "\n",
    "    # Calculate also the L1 error of train/test predictions against the\n",
    "    # real-space QM quantity. Store values of -1 if not evaluating the L1 error\n",
    "    # this epoch\n",
    "    mean_maes = {\"train\": -1, \"test\": -1}\n",
    "    if (epoch - 1) % ml_settings[\"validation\"][\"interval\"] == 0:\n",
    "\n",
    "        mean_maes = {\"train\": [], \"test\": []}\n",
    "        \n",
    "        # Get frames and make prediction\n",
    "        tmp_idxs = np.concatenate([train_idxs, test_idxs])\n",
    "        tmp_frames = [all_frames[A] for A in tmp_idxs]\n",
    "        pred_coeffs, pred_fields = model.predict(\n",
    "            structure_idxs=tmp_idxs,\n",
    "            frames=tmp_frames,\n",
    "            save_dir=pred_dir,\n",
    "        )\n",
    "\n",
    "        # Evaluate mean L1 Error\n",
    "        for A, pred_field in zip(tmp_idxs, pred_fields):\n",
    "            # Get grids and check they're the same in the SCF and ML directories\n",
    "            grid = np.loadtxt(os.path.join(ri_dir(A), \"partition_tab.out\"))\n",
    "            assert np.allclose(\n",
    "                grid,\n",
    "                np.loadtxt(os.path.join(pred_dir(A), \"partition_tab.out\")),\n",
    "            )\n",
    "\n",
    "            # Get L1 error vs real-space QM scalar field\n",
    "            target_field = np.loadtxt(os.path.join(ri_dir(A), \"rho_ref.out\"))\n",
    "            mae = aims_parser.get_percent_mae_between_fields(\n",
    "                input=pred_field,\n",
    "                target=target_field,\n",
    "                grid=grid,\n",
    "            )\n",
    "            if A in train_idxs:\n",
    "                mean_maes[\"train\"].append(mae)\n",
    "            elif A in test_idxs:\n",
    "                mean_maes[\"test\"].append(mae)\n",
    "        for category in [\"train\", \"test\"]:\n",
    "            mean_maes[category] = np.mean(mean_maes[category])\n",
    "\n",
    "    dt = time.time() - t0\n",
    "    print(\n",
    "        f\"epoch {epoch}  \",\n",
    "        f\"train_L2_loss {np.round(train_loss_epoch.detach().numpy(), 5)}  \"\n",
    "        f\"test_L2_loss {np.round(test_loss_epoch.detach().numpy(), 5)}  \"\n",
    "        f\"time {dt}\",\n",
    "    )\n",
    "    io.log(\n",
    "        log_path,\n",
    "        f\"{epoch} {train_loss_epoch} {test_loss_epoch} {mean_maes['train']} {mean_maes['test']} {dt}\",\n",
    "    )\n",
    "    if (epoch - 1) % ml_settings[\"training\"][\"save_interval\"] == 0:\n",
    "        torch.save(model, os.path.join(chkpt_dir, f\"model_{epoch}.pt\"))\n",
    "        torch.save(optimizer.state_dict, os.path.join(chkpt_dir, f\"opt_{epoch}.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training log\n",
    "losses = np.loadtxt(os.path.join(run_dir, \"training.log\"))\n",
    "\n",
    "# Unpack data from each row\n",
    "epochs, train_loss, test_loss, train_mae, test_mae, times = losses.T\n",
    "\n",
    "# Plot the various errors\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(epochs, train_loss, label=\"L2 Loss (ML/RI), train\")\n",
    "ax.plot(epochs, test_loss, label=\"L2 Loss (ML/RI), test\")\n",
    "ax.scatter(\n",
    "    epochs[np.where(train_mae != -1)],\n",
    "    train_mae[np.where(train_mae != -1)] / 100,\n",
    "    label=\"L1 Error (ML/QM), train\",\n",
    "    marker=\".\",\n",
    ")\n",
    "ax.scatter(\n",
    "    epochs[np.where(test_mae != -1)],\n",
    "    test_mae[np.where(test_mae != -1)] / 100,\n",
    "    label=\"L1 Error (ML/QM), test\",\n",
    "    marker=\".\",\n",
    ")\n",
    "\n",
    "ax.set_xlabel(\"Epochs\")\n",
    "ax.set_ylabel(\"Error per structure\")\n",
    "ax.set_yscale(\"log\")\n",
    "ax.set_title(\"HOMO-learning w/ NN | water monomer | AIMS cluster calculation\")\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* This process is slow in a notebook - better to make use of HPC resources! The\n",
    "  training procedure can essentially be copied to a seperate python script and\n",
    "  run using a job scheduler - see \"run_training.py\".\n",
    "* We can load a model that has been pre-trained and validate its performance\n",
    "* Here we load a model trained to over 1500 epochs, only a dataset of only 10\n",
    "  water molecules."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate model performance on validation set\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained model and the training log file\n",
    "model = torch.load(os.path.join(run_dir, \"trained_models\", \"model_1611.pt\"))\n",
    "log_file = np.loadtxt(os.path.join(run_dir, \"trained_models\", \"training.log\"))\n",
    "log_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unpack data from each row\n",
    "epochs, train_loss, test_loss, train_mae, test_mae, val_mae, times = log_file.T\n",
    "\n",
    "# Plot the various errors\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(epochs, train_loss, label=\"L2 Loss (ML/RI), train\")\n",
    "ax.plot(epochs, test_loss, label=\"L2 Loss (ML/RI), test\")\n",
    "ax.scatter(\n",
    "    epochs[np.where(train_mae != -1)],\n",
    "    train_mae[np.where(train_mae != -1)] / 100,\n",
    "    label=\"L1 Error (ML/QM), train\",\n",
    "    marker=\".\",\n",
    ")\n",
    "ax.scatter(\n",
    "    epochs[np.where(test_mae != -1)],\n",
    "    test_mae[np.where(test_mae != -1)] / 100,\n",
    "    label=\"L1 Error (ML/QM), test\",\n",
    "    marker=\".\",\n",
    ")\n",
    "\n",
    "ax.set_xlabel(\"Epochs\")\n",
    "ax.set_ylabel(\"Error per structure\")\n",
    "ax.set_yscale(\"log\")\n",
    "ax.set_title(\"HOMO-learning w/ NN | water monomer | AIMS cluster calculation\")\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End-to-end prediction on the validation set\n",
    "\n",
    "* Now let's make an end-to-end prediction (from xyz -> real-space scalar field)\n",
    "  on the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the validation frames as ASE Atoms objects\n",
    "val_frames = [all_frames[A] for A in val_idxs]\n",
    "\n",
    "def val_dir(A):\n",
    "    return os.path.join(run_dir, \"validation\", f\"{A}\")\n",
    "\n",
    "# Make predictions for the validation set. Note: we could predict from the\n",
    "# descriptors that are already constructed here, but do so from ASE frames for\n",
    "# demonstrative purposes\n",
    "pred_coeffs, pred_fields = model.predict(structure_idxs=val_idxs, frames=val_frames, save_dir=val_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate density fitting error of predictions relative to SCF-converged\n",
    "# real-space scalar field\n",
    "val_df_errors = {}\n",
    "for A, pred_field in zip(val_idxs, pred_fields):\n",
    "    # Get grids and check they're the same in the SCF and ML directories\n",
    "    grid = np.loadtxt(os.path.join(ri_dir(A), \"partition_tab.out\"))\n",
    "    assert np.allclose(grid, np.loadtxt(os.path.join(val_dir(A), \"partition_tab.out\")))\n",
    "\n",
    "    # Get DF error\n",
    "    target_field = np.loadtxt(os.path.join(ri_dir(A), \"rho_ref.out\"))  # QM scalar field\n",
    "    df_error = aims_parser.get_percent_mae_between_fields(\n",
    "        input=pred_field,\n",
    "        target=target_field,\n",
    "        grid=grid,\n",
    "    )\n",
    "    print(f\"Val structure {A}, DF error (%): {df_error}\")\n",
    "    val_df_errors[A] = df_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Let's visualize these predictions using the cube file outputs\n",
    "* Pick one validation structure and vizualize the target QM and predicted ML\n",
    "  HOMO scalar fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Visualize the predicted density\n",
    "A = val_idxs[0]\n",
    "\n",
    "qm_cube = os.path.join(ri_dir(A), \"rho_ref.cube\")\n",
    "ml_cube = os.path.join(val_dir(A), \"rho_rebuilt.cube\")\n",
    "\n",
    "for cube_file in [qm_cube, ml_cube]:\n",
    "    v = py3Dmol.view()\n",
    "    v.addModelsAsFrames(open(cube_file, \"r\").read(), \"cube\")\n",
    "    v.setStyle({\"stick\": {}})\n",
    "    v.addVolumetricData(\n",
    "        open(cube_file, \"r\").read(),\n",
    "        \"cube\",\n",
    "        {\"isoval\": 0.001, \"color\": \"blue\", \"opacity\": 0.8},\n",
    "    )\n",
    "    v.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Let's calculate the difference between ML and QM RI-coefficients, and use\n",
    "  these 'delta-coeffs' to reconstruct a delta-scalar field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First define a new directory to store the delta HOMOs\n",
    "if not os.path.exists(os.path.join(run_dir, \"delta\")):\n",
    "    os.mkdir(os.path.join(run_dir, \"delta\"))\n",
    "\n",
    "def delta_dir(A):\n",
    "    return os.path.join(run_dir, \"delta\", f\"{A}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the target coeffs\n",
    "target_coeffs = [\n",
    "    metatensor.load(os.path.join(processed_dir(A), \"ri_coeffs.npz\"))\n",
    "    for A in val_idxs\n",
    "]\n",
    "\n",
    "# Calculate the delta coeffs\n",
    "delta_coeffs = [\n",
    "    metatensor.subtract(pred_coeff, target_coeff) for pred_coeff, target_coeff in zip(pred_coeffs, target_coeffs)\n",
    "]\n",
    "\n",
    "# Build the delta-HOMO from RI coefficients using AIMS\n",
    "delta_fields = predictor.target_builder(\n",
    "    structure_idxs=val_idxs, \n",
    "    frames=val_frames, \n",
    "    predictions=delta_coeffs, \n",
    "    save_dir=delta_dir, \n",
    "    **model.target_kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the delta-HOMO of one of the validation structures\n",
    "A = val_idxs[0]\n",
    "delta_cube = os.path.join(delta_dir(A), \"rho_rebuilt.cube\")\n",
    "\n",
    "for cube_file in [delta_cube]:\n",
    "    v = py3Dmol.view()\n",
    "    v.addModelsAsFrames(open(cube_file, \"r\").read(), \"cube\")\n",
    "    v.setStyle({\"stick\": {}})\n",
    "    v.addVolumetricData(\n",
    "        open(cube_file, \"r\").read(),\n",
    "        \"cube\",\n",
    "        {\"isoval\": 0.001, \"color\": \"blue\", \"opacity\": 0.8},\n",
    "    )\n",
    "    v.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rho",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
