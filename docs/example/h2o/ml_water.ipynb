{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# How to Train Your Model (Live Demo #1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1. Import M-stack packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'metatensor.core'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/Users/joe.abbott/Documents/phd/code/rho/rho_learn/docs/example/new_water/ml_water.ipynb Cell 3\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/joe.abbott/Documents/phd/code/rho/rho_learn/docs/example/new_water/ml_water.ipynb#W2sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmetatensor\u001b[39;00m   \u001b[39m# storage format for atomistic ML\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/joe.abbott/Documents/phd/code/rho/rho_learn/docs/example/new_water/ml_water.ipynb#W2sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mchemiscope\u001b[39;00m  \u001b[39m# interactive molecular visualization\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/joe.abbott/Documents/phd/code/rho/rho_learn/docs/example/new_water/ml_water.ipynb#W2sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mrascaline\u001b[39;00m   \u001b[39m# generating structural representations\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/joe.abbott/Documents/phd/code/rho/rho_learn/docs/example/new_water/ml_water.ipynb#W2sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39m# import qstack      # quantum chemistry toolkit\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/joe.abbott/Documents/phd/code/rho/rho_learn/docs/example/new_water/ml_water.ipynb#W2sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmetatensor\u001b[39;00m \u001b[39mimport\u001b[39;00m Labels, TensorBlock, TensorMap\n",
      "File \u001b[0;32m/opt/miniforge3/envs/rho/lib/python3.10/site-packages/rascaline/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m utils  \u001b[39m# noqa\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mcalculator_base\u001b[39;00m \u001b[39mimport\u001b[39;00m CalculatorBase  \u001b[39m# noqa\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mlog\u001b[39;00m \u001b[39mimport\u001b[39;00m set_logging_callback  \u001b[39m# noqa\u001b[39;00m\n",
      "File \u001b[0;32m/opt/miniforge3/envs/rho/lib/python3.10/site-packages/rascaline/utils/__init__.py:3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mos\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmetatensor\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m clebsch_gordan \n\u001b[1;32m      6\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mpower_spectrum\u001b[39;00m \u001b[39mimport\u001b[39;00m PowerSpectrum\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'metatensor.core'"
     ]
    }
   ],
   "source": [
    "# %load_ext autoreload\n",
    "# %autoreload 2\n",
    "\n",
    "# %load_ext line_profiler\n",
    "\n",
    "# Useful standard and scientific ML libraries\n",
    "import os\n",
    "import time\n",
    "import ase.io\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "# import pyscf\n",
    "# import py3Dmol\n",
    "import torch\n",
    "\n",
    "# M-Stack packages\n",
    "\n",
    "import metatensor   # storage format for atomistic ML\n",
    "import chemiscope  # interactive molecular visualization\n",
    "import rascaline   # generating structural representations\n",
    "# import qstack      # quantum chemistry toolkit\n",
    "\n",
    "from metatensor import Labels, TensorBlock, TensorMap\n",
    "from rascaline.utils import clebsch_gordan, old_clebsch_gordan, rotations\n",
    "\n",
    "# Torch-based density leaning\n",
    "from rholearn import io, data, loss, models, training, utils\n",
    "from settings import lsoap_settings, data_settings, ml_settings, torch_settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "4%3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate $\\lambda$-SOAP equivariant structural representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = ase.io.read(\"/Users/joe.abbott/Documents/phd/code/rho/rho_learn/docs/example/new_water/data/coords_1k.xyz\", \":\")\n",
    "chemiscope.show(frames, mode=\"structure\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = ase.io.read(\"data/water_monomers_1k.xyz\", \":\")\n",
    "lsoap = clebsch_gordan.lambda_soap_vector(\n",
    "    frames,\n",
    "    rascal_hypers=lsoap_settings[\"rascal_hypers\"],\n",
    "    lambda_filter=lsoap_settings[\"lambdas\"],\n",
    "    sigma_filter=lsoap_settings[\"sigmas\"],\n",
    "    lambda_cut=lsoap_settings[\"lambda_cut\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1000):\n",
    "    # os.mkdir(f\"data/lsoap/{i}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coeffs = metatensor.join(metatensor.load(\"data/dft/0/ri_coeffs.npz\")\n",
    "coeffs.block(spherical_harmonics_l=4, species_center=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = metatensor.core.io.load_custom_array(\n",
    "    \"/Users/joe.abbott/Documents/phd/code/rho/rho_learn/docs/example/water/data/lsoap/0/x.npz\",\n",
    "    create_array=metatensor.core.io.create_torch_array,\n",
    ")\n",
    "output = metatensor.core.io.load_custom_array(\n",
    "    \"/Users/joe.abbott/Documents/phd/code/rho/rho_learn/docs/example/water/data/rho/0/c.npz\",\n",
    "    create_array=metatensor.core.io.create_torch_array,\n",
    ")\n",
    "inv_means = metatensor.core.io.load_custom_array(\n",
    "    \"/Users/joe.abbott/Documents/phd/code/rho/rho_learn/docs/example/water/data/rho/inv_means.npz\",\n",
    "    create_array=metatensor.core.io.create_torch_array,\n",
    ")\n",
    "\n",
    "torch_settings = {\n",
    "    \"dtype\": torch.float64,\n",
    "    \"requires_grad\": True,\n",
    "    \"device\": torch.device(type=\"cpu\"),\n",
    "}\n",
    "\n",
    "model = models.RhoModel(\n",
    "    model_type=\"nonlinear\",\n",
    "    input=input,\n",
    "    output=output,\n",
    "    bias_invariants=True,\n",
    "    hidden_layer_widths=[128, 56],\n",
    "    activation_fn=torch.nn.Tanh(),\n",
    "    out_train_inv_means=inv_means,\n",
    "    **torch_settings\n",
    ")\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.1)\n",
    "scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[100])\n",
    "\n",
    "TO_TORCH_ATTRS = [\"_in_metadata\", \"_out_metadata\", \"_out_train_inv_means\"]\n",
    "\n",
    "# Convert each attribute to torch\n",
    "# if model._out_train_inv_means is None:\n",
    "#     attrs = TO_TORCH_ATTRS[:2]\n",
    "# else:\n",
    "#     attrs = TO_TORCH_ATTRS\n",
    "# for attr in attrs:\n",
    "#     setattr(\n",
    "#         model,\n",
    "#         attr,\n",
    "#         metatensor.to(\n",
    "#             getattr(model, attr),\n",
    "#             \"torch\",\n",
    "#             dtype=model._torch_settings[\"dtype\"],\n",
    "#             device=model._torch_settings[\"device\"],\n",
    "#         ),\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models.RhoModel?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, \"model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chkpt_dict = {\n",
    "        \"model\": model,\n",
    "        \"optimizer\": optimizer,\n",
    "    }\n",
    "    if scheduler is not None:\n",
    "        chkpt_dict.update({\"scheduler\": scheduler})\n",
    "    torch.save(chkpt_dict, \"chekpoint.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.load(\"checkpoint.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.models[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, \"model.pt\")\n",
    "model(input).block(0).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = load_rho_model(\"model.pt\")\n",
    "# model2.load_to_torch(\n",
    "#     dtype=torch.float64, requires_grad=True, device=torch.device(\"cpu\")\n",
    "# )\n",
    "model2(input).block(0).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model._torch_settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2. Visualize and explore dataset: `chemiscope`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Read the water molecules from file\n",
    "# frames = ase.io.read(os.path.join(data_settings[\"data_dir\"], \"water_monomers_1k.xyz\"), index=\":\")\n",
    "# structure_idxs = np.arange(len(frames))\n",
    "\n",
    "# Display molecules with chemiscope\n",
    "# chemiscope.show(\n",
    "#     frames,\n",
    "#     properties={\n",
    "#         \"Mean O-H bond length, Angstrom\": [np.mean([f.get_distance(0, 1), f.get_distance(0, 2)]) for f in frames],\n",
    "#         \"H-O-H angle, degrees\": [f.get_angle(1, 0, 2) for f in frames],\n",
    "#     },\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Generate $\\lambda$-SOAP descriptors\n",
    "\n",
    "### Calculate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from rascaline.utils import clebsch_gordan, old_clebsch_gordan\n",
    "# ...\n",
    "# rascal_hypers = {...}\n",
    "# lambdas = np.arange(rascal_hypers[\"max_angular\"] + 1)\n",
    "\n",
    "# # Generate lambda-SOAP -- new version\n",
    "# lsoap = clebsch_gordan.lambda_soap_vector(\n",
    "#     frames,\n",
    "#     rascal_hypers=rascal_hypers,\n",
    "#     lambdas=lambdas,\n",
    "#     only_keep_parity=+1,\n",
    "# )\n",
    "\n",
    "# # Generate lambda-SOAP -- old version\n",
    "# lsoap = old_clebsch_gordan.lambda_soap_vector(\n",
    "#     frames,\n",
    "#     rascal_hypers=rascal_hypers,\n",
    "#     lambdas=lambdas,\n",
    "#     only_keep_parity=+1,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the equivariance condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Check equivariance\n",
    "\n",
    "# # Generate Wigner-D matrices, initialized with random angles\n",
    "# wig = rotations.WignerDReal(lmax=rascal_hypers[\"max_angular\"])\n",
    "# print(\"Random rotation angles (rad):\", wig.angles)\n",
    "\n",
    "# # Apply an O(3) transformation to each frame \n",
    "# frames_o3 = [rotations.transform_frame_o3(frame, wig.angles) for frame in frames]\n",
    "# assert not np.allclose(frames[0].positions, frames_o3[0].positions)\n",
    "\n",
    "# # Generate lambda-SOAP for the transformed frames\n",
    "# lsoap_o3 = clebsch_gordan.lambda_soap_vector(\n",
    "#     frames_o3,\n",
    "#     rascal_hypers=rascal_hypers,\n",
    "#     lambdas=lambdas,\n",
    "#     only_keep_parity=+1,\n",
    "# )\n",
    "\n",
    "# # Apply the O(3) transformation to the TensorMap\n",
    "# lsoap_transformed = wig.transform_tensormap_o3(lsoap)\n",
    "\n",
    "# # Check for equivariance!\n",
    "# assert metatensor.equal_metadata(lsoap_transformed, lsoap_o3)\n",
    "# assert metatensor.allclose(lsoap_transformed, lsoap_o3)\n",
    "# print(\"O(3) EQUIVARIANT!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create a dir for lambda-SOAP\n",
    "# lsoap_dir = os.path.join(data_settings[\"data_dir\"], \"lsoap\")\n",
    "# if not os.path.exists(lsoap_dir):\n",
    "#     os.mkdir(path=lsoap_dir)\n",
    "\n",
    "# # Split into separate TensorMaps for each structure\n",
    "# lsoap_split = metatensor.split(\n",
    "#     lsoap,\n",
    "#     axis=\"samples\",\n",
    "#     grouped_labels=[\n",
    "#         Labels(names=\"structure\", values=np.array([A]).reshape(-1, 1))\n",
    "#         for A in range(n_frames)\n",
    "#     ],\n",
    "# )\n",
    "\n",
    "# # Save the lambda-SOAP features for each structure to a separate dir\n",
    "# for A, frame in enumerate(frames[:n_frames]):\n",
    "#     # Create dir\n",
    "#     struct_dir = os.path.join(lsoap_dir, f\"{A}\")\n",
    "#     if not os.path.exists(struct_dir):\n",
    "#         os.mkdir(path=struct_dir)\n",
    "#     # Save\n",
    "#     metatensor.save(os.path.join(struct_dir, \"x.npz\"), lsoap_split[A])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Split data and standardize invariants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get the grouped indices for train/test(/val) splits\n",
    "# train_idxs, test_idxs, val_idxs = data.group_idxs(\n",
    "#     structure_idxs,\n",
    "#     n_groups=data_settings[\"n_groups\"],\n",
    "#     group_sizes=data_settings[\"group_sizes\"],\n",
    "#     shuffle=data_settings[\"shuffle\"],\n",
    "#     seed=data_settings[\"seed\"],\n",
    "# )\n",
    "\n",
    "# # Define new dir for storing standardized features\n",
    "# rho_std_dir = os.path.join(data_settings[\"data_dir\"], \"rho_std\")\n",
    "# if not os.path.exists(rho_std_dir):\n",
    "#     os.mkdir(rho_std_dir)\n",
    "\n",
    "# # Save the grouped indices\n",
    "# np.savez(\n",
    "#     os.path.join(rho_std_dir, \"idxs.npz\"),\n",
    "#     train=train_idxs,\n",
    "#     test=test_idxs,\n",
    "#     val=val_idxs,\n",
    "# )\n",
    "\n",
    "# # Load all the structures into a single TensorMap\n",
    "# c_list = [\n",
    "#     metatensor.load(os.path.join(data_settings[\"data_dir\"], \"rho\", f\"{i}\", \"c.npz\"))\n",
    "#     for i in structure_idxs\n",
    "# ]\n",
    "# c_all = metatensor.join(c_list, axis=\"samples\", remove_tensor_name=True)\n",
    "\n",
    "# # Split to get another TensroMap with only the training structures\n",
    "# c_train = metatensor.slice(\n",
    "#     c_all,\n",
    "#     axis=\"samples\",\n",
    "#     labels=Labels(names=[\"structure\"], values=np.array([train_idxs]).reshape(-1, 1)),\n",
    "# )\n",
    "\n",
    "# # Get the invariant means and save\n",
    "# inv_means = features.get_invariant_means(c_train)\n",
    "# metatensor.save(os.path.join(rho_std_dir, \"inv_means.npz\"), inv_means)\n",
    "\n",
    "# # Standardize the invariants of all strutcures\n",
    "# c_all_std = features.standardize_invariants(c_all, inv_means)\n",
    "\n",
    "# # Split into individual TensorMaps\n",
    "# c_all_split = metatensor.split(\n",
    "#     c_all_std,\n",
    "#     axis=\"samples\",\n",
    "#     grouped_labels=[\n",
    "#         Labels(names=[\"structure\"], values=np.array([i]).reshape(-1, 1))\n",
    "#         for i in structure_idxs\n",
    "#     ],\n",
    "# )\n",
    "\n",
    "# # Save each structure in a separate directory\n",
    "# for A, c_std in enumerate(c_all_split):\n",
    "#     assert c_std.block(0).samples[\"structure\"][0] == A\n",
    "#     c_dir = os.path.join(rho_std_dir, f\"{A}\")\n",
    "#     if not os.path.exists(c_dir):\n",
    "#         os.mkdir(c_dir)\n",
    "#     metatensor.save(os.path.join(c_dir, \"c.npz\"), c_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Build the torch dataset and dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "import metatensor\n",
    "\n",
    "from rholearn import io, data, training\n",
    "from settings import data_settings, ml_settings, torch_settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define all the structure indices\n",
    "all_idxs = np.arange(1000)\n",
    "\n",
    "# Get the grouped indices for train/test(/val) splits\n",
    "train_idxs, test_idxs, val_idxs = data.group_idxs(\n",
    "    all_idxs=all_idxs,\n",
    "    n_groups=data_settings[\"n_groups\"],\n",
    "    group_sizes=data_settings[\"group_sizes\"],\n",
    "    shuffle=data_settings[\"shuffle\"],\n",
    "    seed=data_settings[\"seed\"],\n",
    ")\n",
    "\n",
    "# Define a training subset\n",
    "if data_settings[\"n_train_subsets\"]:\n",
    "    subset_sizes = data.get_log_subset_sizes(len(train_idxs), data_settings[\"n_train_subsets\"])\n",
    "    train_idxs = train_idxs[:subset_sizes[data_settings.get(\"i_train_subset\")]]\n",
    "\n",
    "# Define a test subset if not doing test batching\n",
    "if ml_settings[\"loading\"][\"test\"][\"do_batching\"] is False:\n",
    "    if ml_settings[\"loading\"][\"test\"][\"batch_size\"] < len(test_idxs):\n",
    "        test_idxs = test_idxs[:ml_settings[\"loading\"][\"test\"][\"batch_size\"]]\n",
    "\n",
    "print(\"num train structures:\", len(train_idxs))\n",
    "print(\"num test structures:\", len(test_idxs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build density dataset\n",
    "rho_data = data.RhoData(\n",
    "    idxs=np.concatenate([train_idxs, test_idxs]),\n",
    "    input_dir=data_settings[\"input_dir\"],\n",
    "    output_dir=data_settings[\"output_dir\"],\n",
    "    overlap_dir=data_settings[\"overlap_dir\"],\n",
    "    keep_in_mem=True,\n",
    "    standardize_invariants=data_settings[\"standardize_invariants\"],\n",
    "    train_idxs=train_idxs,\n",
    "    **torch_settings,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize objects or load from checkpoint\n",
    "restart_epoch = ml_settings[\"training\"][\"restart_epoch\"]\n",
    "out_inv_means = None if \"output\" not in data_settings[\"standardize_invariants\"] else rho_data.out_invariant_means\n",
    "if restart_epoch == 0:\n",
    "    objects = training.init_training_objects(\n",
    "        ml_settings,\n",
    "        input=rho_data[rho_data._idxs[0]][1],\n",
    "        output=rho_data[rho_data._idxs[0]][2],\n",
    "        out_invariant_means=out_inv_means,\n",
    "    )\n",
    "else:\n",
    "    objects = training.load_from_checkpoint(\n",
    "        path=os.path.join(ml_settings[\"run_dir\"], f\"checkpoint_{restart_epoch}.pt\"),\n",
    "        ml_settings=ml_settings,\n",
    "        input=rho_data[rho_data._idxs[0]][1],\n",
    "        output=rho_data[rho_data._idxs[0]][2],\n",
    "    )\n",
    "# Unpack objects\n",
    "model, optimizer, rho_loss_fn, coeff_loss_fn, scheduler = objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Run training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    # Make a run dir for saving results\n",
    "    if not os.path.exists(ml_settings[\"run_dir\"]):\n",
    "        os.mkdir(ml_settings[\"run_dir\"])\n",
    "\n",
    "    # Define a log file\n",
    "    log_file = os.path.join(ml_settings[\"run_dir\"], \"log.txt\")\n",
    "    io.log(log_file, \"# Model training\")\n",
    "    io.log(log_file, \"# epoch train_loss test_loss lr time learning_on_rho\")\n",
    "\n",
    "    # Initialize the train and test loaders\n",
    "    train_loader = data.RhoLoader(\n",
    "        rho_data,\n",
    "        idxs=train_idxs,\n",
    "        get_overlaps=False,\n",
    "        batch_size=ml_settings[\"loading\"][\"train\"][\"batch_size\"],\n",
    "    )\n",
    "    test_loader = data.RhoLoader(\n",
    "        rho_data,\n",
    "        idxs=test_idxs,\n",
    "        get_overlaps=True,\n",
    "        batch_size=ml_settings[\"loading\"][\"test\"][\"batch_size\"],\n",
    "    )\n",
    "\n",
    "    # Pre-collate test data if not performing test batching\n",
    "    if ml_settings[\"loading\"][\"test\"][\"do_batching\"] is False:\n",
    "        test_batch_idxs, x_test, c_test, s_test = next(iter(test_loader))\n",
    "\n",
    "    # Start training\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    use_rho_loss = False\n",
    "    for epoch in range(\n",
    "        ml_settings[\"training\"][\"restart_epoch\"] + 1,\n",
    "        ml_settings[\"training\"][\"n_epochs\"] + 1,\n",
    "    ):\n",
    "        # Start timer\n",
    "        t0 = time.time()\n",
    "\n",
    "        # Set some epoch-dependent settings\n",
    "        check_args = True if epoch == 0 or ml_settings[\"training\"][\"learn_on_rho_at_epoch\"] else False\n",
    "\n",
    "        # If we're now switching to learning on rho, reinitialize the train loader\n",
    "        if epoch == ml_settings[\"training\"][\"learn_on_rho_at_epoch\"]:\n",
    "            use_rho_loss = True\n",
    "            train_loader = data.RhoLoader(\n",
    "                rho_data,\n",
    "                idxs=train_idxs,\n",
    "                get_overlaps=True,\n",
    "                batch_size=ml_settings[\"loading\"][\"train\"][\"batch_size\"],\n",
    "            )\n",
    "\n",
    "        # ===== Iterate over training batches\n",
    "        for train_batch in train_loader:\n",
    "            # Reset gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Unpack train batch\n",
    "            if use_rho_loss:\n",
    "                train_batch_idxs, x_train, c_train, s_train = train_batch\n",
    "            else:\n",
    "                train_batch_idxs, x_train, c_train = train_batch\n",
    "\n",
    "            # Make a prediction\n",
    "            c_train_pred = model(x_train, check_args=check_args)\n",
    "\n",
    "            # Evaluate the loss with either CoeffLoss or RhoLoss\n",
    "            if use_rho_loss:\n",
    "                train_loss = rho_loss_fn(\n",
    "                    c_train_pred, c_train, s_train, check_args=check_args\n",
    "                )\n",
    "            else:  # use CoeffLoss\n",
    "                train_loss = coeff_loss_fn(c_train_pred, c_train, check_args=check_args)\n",
    "\n",
    "            # Calculate gradient and update parameters\n",
    "            train_loss.backward(retain_graph=True)\n",
    "            optimizer.step()\n",
    "\n",
    "            # Store training loss, divided by the number of structures in the batch\n",
    "            train_losses.append(train_loss.detach().numpy() / len(train_batch_idxs))\n",
    "\n",
    "        # ===== Evaluate test loss *on the density*\n",
    "        with torch.no_grad():\n",
    "            # Option 1) perform test batching\n",
    "            if ml_settings[\"loading\"][\"test\"][\"do_batching\"]:\n",
    "                # Iterate over test batches: calculate the test loss\n",
    "                for test_batch in test_loader:\n",
    "                    # Unpack test batch\n",
    "                    test_batch_idxs, x_test, c_test, s_test = test_batch\n",
    "                    # Make a prediction\n",
    "                    c_test_pred = model(x_test, check_args=check_args)\n",
    "                    # Evaluate and store test loss per structure\n",
    "                    test_loss = rho_loss_fn(\n",
    "                        c_test_pred, c_test, s_test, check_args=check_args\n",
    "                    )\n",
    "                    test_losses.append(\n",
    "                        test_loss.detach().numpy() / len(test_batch_idxs)\n",
    "                    )\n",
    "\n",
    "            # Option 2) use a single batch, pre-collated\n",
    "            else:\n",
    "                # Make a prediction\n",
    "                c_test_pred = model(x_test, check_args=check_args)\n",
    "                # Evaluate and store test loss per structure\n",
    "                test_loss = rho_loss_fn(\n",
    "                    c_test_pred, c_test, s_test, check_args=check_args\n",
    "                )\n",
    "                test_losses.append(test_loss.detach().numpy() / len(test_batch_idxs))\n",
    "\n",
    "        # Save checkpoint\n",
    "        if epoch % ml_settings[\"training\"][\"save_interval\"] == 0:\n",
    "            training.save_checkpoint(\n",
    "                ml_settings[\"run_dir\"], epoch, model, optimizer, scheduler=scheduler\n",
    "            )\n",
    "\n",
    "        # Write log for the epoch\n",
    "        io.log(\n",
    "            log_file,\n",
    "            f\"{epoch} \"\n",
    "            f\"{np.round(train_losses[-1], 7)} \"\n",
    "            f\"{np.round(test_losses[-1], 7)} \"\n",
    "            f\"{np.round(scheduler.get_last_lr()[0], 7)} \"\n",
    "            f\"{np.round(time.time() - t0, 7)} \"\n",
    "            f\"{1 if use_rho_loss else 0} \",\n",
    "        )\n",
    "        scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Plot results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load the results\n",
    "# results = np.loadtxt(os.path.join(ml_settings[\"run_dir\"], \"log.txt\"))\n",
    "\n",
    "# # Plot train and test loss versus epoch\n",
    "# fig, ax = plt.subplots()\n",
    "# ax.loglog(results[:, 0], results[:, 1], label=\"train\")\n",
    "# ax.loglog(results[:, 0], results[:, 2], label=\"test\")\n",
    "# ax.legend()\n",
    "# ax.set_xlabel(\"epoch\")\n",
    "# ax.set_ylabel(\"loss per batch\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Make a prediction on the validation structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Load the input and output validation TensorMaps\n",
    "# in_val = io.load_tensormap_to_torch(\n",
    "#     os.path.join(data_settings[\"data_dir\"], \"in_val.npz\"), **ml_settings[\"torch\"]\n",
    "# )\n",
    "# out_val = metatensor.load(os.path.join(data_settings[\"data_dir\"], \"out_val.npz\"))\n",
    "\n",
    "# # Retrieve the unique structure\n",
    "# val_idx = metatensor.unique_metadata(in_val, axis=\"samples\", names=\"structure\")[0][0]\n",
    "# val_frame = ase.io.read(\n",
    "#     os.path.join(data_settings[\"data_dir\"], \"water_monomers_1k.xyz\"), index=val_idx\n",
    "# )\n",
    "\n",
    "# # Build a pyscf Molecule object\n",
    "# val_mol = pyscf.gto.Mole().build(\n",
    "#     atom=[\n",
    "#         (i, j) for i, j in zip(val_frame.get_chemical_symbols(), val_frame.positions)\n",
    "#     ],\n",
    "#     basis=\"ccpvqz jkfit\",\n",
    "# )\n",
    "\n",
    "# # Predict the density\n",
    "# out_val_pred, coeffs = predictor.predict_density_from_mol(\n",
    "#     in_val,\n",
    "#     val_mol,\n",
    "#     model_path=os.path.join(ml_settings[\"run_dir\"], \"epoch_10\", \"model.pt\"),\n",
    "#     inv_means_path=os.path.join(data_settings[\"data_dir\"], \"inv_means.npz\"),\n",
    "# )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parity plot: target vs predicted coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Calculate the MSE Error\n",
    "# with torch.no_grad():\n",
    "#     val_loss = loss.MSELoss(reduction=\"sum\")(\n",
    "#         utils.tensor_to_torch(out_val, **ml_settings[\"torch\"]), \n",
    "#         utils.tensor_to_torch(out_val_pred, **ml_settings[\"torch\"])\n",
    "#     ).detach().numpy()\n",
    "\n",
    "# # Plot the target vs predicted coefficients, standardized\n",
    "# fig, ax = plots.parity_plot(\n",
    "#     target=out_val,\n",
    "#     predicted=out_val_pred,\n",
    "#     color_by=\"spherical_harmonics_l\",\n",
    "# )\n",
    "# lim = [-0.05, 0.1]\n",
    "# ax.set_xlim(lim)\n",
    "# ax.set_ylim(lim)\n",
    "# ax.set_aspect(\"equal\")\n",
    "# ax.set_xlabel(\"target density coefficient\")\n",
    "# ax.set_ylabel(\"predicted density coefficient\")\n",
    "# ax.set_title(f\"Validation MSE Error: {round(val_loss * 1e6, 3)}\"r\" $\\times 10^{-6}$\")\n",
    "# ax.legend()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Process densities with `Q-stack` and visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Build a delta density TensorMap\n",
    "# out_val_delta = metatensor.abs(metatensor.subtract(out_val_pred, out_val))\n",
    "\n",
    "# # Vectorize the coefficients from each of the TensorMaps\n",
    "# new_key_names = [\"spherical_harmonics_l\", \"element\"]\n",
    "# vect_coeffs_target = qstack.equio.tensormap_to_vector(\n",
    "#     val_mol,\n",
    "#     utils.rename_tensor(\n",
    "#         utils.drop_metadata_name(out_val, \"samples\", \"structure\"),\n",
    "#         keys_names=new_key_names,\n",
    "#     ),\n",
    "# )\n",
    "# vect_coeffs_input = qstack.equio.tensormap_to_vector(\n",
    "#     val_mol,\n",
    "#     utils.rename_tensor(\n",
    "#         utils.drop_metadata_name(out_val_pred, \"samples\", \"structure\"),\n",
    "#         keys_names=new_key_names,\n",
    "#     ),\n",
    "# )\n",
    "# vect_coeffs_delta = qstack.equio.tensormap_to_vector(\n",
    "#     val_mol,\n",
    "#     utils.rename_tensor(\n",
    "#         utils.drop_metadata_name(out_val_delta, \"samples\", \"structure\"),\n",
    "#         keys_names=new_key_names,\n",
    "#     ),\n",
    "# )\n",
    "\n",
    "# # Convert the basis function coefficients to a cube file\n",
    "# plot_dir = os.path.join(ml_settings[\"run_dir\"], \"plots\")\n",
    "# io.check_or_create_dir(plot_dir)\n",
    "# n = 60  # grid points per dimension\n",
    "# for (coeffs, filename) in [\n",
    "#     (vect_coeffs_target, \"out_val.cube\"),\n",
    "#     (vect_coeffs_input, \"out_val_pred.cube\"),\n",
    "#     (vect_coeffs_delta, \"out_val_delta.cube\"),\n",
    "# ]:\n",
    "#     qstack.fields.density2file.coeffs_to_cube(\n",
    "#         val_mol,\n",
    "#         coeffs,\n",
    "#         os.path.join(plot_dir, filename),\n",
    "#         nx=n,\n",
    "#         ny=n,\n",
    "#         nz=n,\n",
    "#         resolution=None,\n",
    "#     )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Predicted electron density"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Visualize the predicted density\n",
    "# v = py3Dmol.view()\n",
    "# v.addModelsAsFrames(open(os.path.join(plot_dir, \"out_val_pred.cube\"), \"r\").read(), \"cube\")\n",
    "# v.setStyle({\"stick\": {}})\n",
    "# v.addVolumetricData(\n",
    "#     open(os.path.join(plot_dir, \"out_val_pred.cube\"), \"r\").read(),\n",
    "#     \"cube\",\n",
    "#     {\"isoval\": 0.05, \"color\": \"blue\", \"opacity\": 0.8},\n",
    "# )\n",
    "# v.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \"Delta electron density\" - i.e. the ML error (100x magnification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Visualize the delta density\n",
    "# v = py3Dmol.view()\n",
    "# v.addModelsAsFrames(open(os.path.join(plot_dir, \"out_val_delta.cube\"), \"r\").read(), \"cube\")\n",
    "# v.setStyle({\"stick\": {}})\n",
    "# v.addVolumetricData(\n",
    "#     open(os.path.join(plot_dir, \"out_val_delta.cube\"), \"r\").read(),\n",
    "#     \"cube\",\n",
    "#     {\"isoval\": 0.0005, \"color\": \"blue\", \"opacity\": 0.8},\n",
    "# )\n",
    "# v.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rho",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
