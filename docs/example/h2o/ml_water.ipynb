{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Learning the electron density of water"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-requisites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install the relevant packages\n",
    "\n",
    "1. `pip install metatensor`\n",
    "1. `pip install chemiscope`\n",
    "1. `pip install git+https://github.com/luthaf/rascaline.git@b2cedfe870541e6d037357db58de1901eb116c41`\n",
    "\n",
    "\n",
    "**NOTE**: this notebook has only been tested on the HPC system \"jed\". Due to\n",
    "specific compilation of the quantum chemistry package `FHI-aims`, the Python\n",
    "interface to running QC calculations may not be generalized to other operating\n",
    "systems or hardware."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Useful standard and scientific ML libraries\n",
    "import os\n",
    "import ase.io\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import py3Dmol\n",
    "import torch\n",
    "\n",
    "# M-Stack packages\n",
    "import metatensor   # storage format for atomistic ML\n",
    "import chemiscope  # interactive molecular visualization\n",
    "import rascaline   # generating structural representations\n",
    "from metatensor import Labels, TensorBlock, TensorMap\n",
    "from rascaline.utils import clebsch_gordan\n",
    "\n",
    "# Interfacing with FHI-aims\n",
    "from rhocalc.aims import aims_calc, aims_parser\n",
    "\n",
    "# Torch-based density leaning\n",
    "from rholearn import io, data, loss, models, predictor\n",
    "import settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from settings import data_dir\n",
    "\n",
    "# Define a function that returns the data directory for a given structure based\n",
    "# on its structure index\n",
    "def struct_dir(A):\n",
    "    return os.path.join(data_dir, f\"{A}/\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Visualize structures in dataset\n",
    "\n",
    "* Use `chemiscope`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from settings import all_idxs, frames\n",
    "\n",
    "chemiscope.show(\n",
    "    frames,\n",
    "    properties={\n",
    "        \"Mean O-H bond length, Angstrom\": [np.mean([f.get_distance(0, 1), f.get_distance(0, 2)]) for f in frames],\n",
    "        \"H-O-H angle, degrees\": [f.get_angle(1, 0, 2) for f in frames],\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate learning targets\n",
    "\n",
    "* These are the RI coefficients of the HOMO eigenstate\n",
    "* We generate these in 2 steps: 1) SCF and 2) RI fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Converge SCF for each structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the settings needed to run FHI-aims\n",
    "from settings import aims_path, base_aims_kwargs, scf_kwargs, ri_kwargs, sbatch_kwargs\n",
    "\n",
    "# Build a dict of settings for each calculation (i.e. structure)\n",
    "# IMPORTANT: zip() is used to pair up the structure index and the structure\n",
    "calcs = {\n",
    "    A: {\"atoms\": frame, \"run_dir\": struct_dir(A)} for A, frame in zip(all_idxs, frames)\n",
    "}\n",
    "\n",
    "# And the general settings for all calcs\n",
    "aims_kwargs = base_aims_kwargs.copy()\n",
    "aims_kwargs.update(scf_kwargs)\n",
    "\n",
    "# Run the SCF in AIMS\n",
    "aims_calc.run_aims_array(\n",
    "    calcs=calcs,\n",
    "    aims_path=aims_path,\n",
    "    aims_kwargs=aims_kwargs,\n",
    "    sbatch_kwargs=sbatch_kwargs,\n",
    "    run_dir=struct_dir,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running this code will ensure that all calculations have finished.\n",
    "# Should take < 10 seconds (1 node, 10 cores on 'jed' HPC)\n",
    "all_finished = False\n",
    "while not all_finished:\n",
    "    calcs_finished = []\n",
    "    for A in all_idxs:\n",
    "        aims_out_path = os.path.join(struct_dir(A), \"aims.out\")\n",
    "        if os.path.exists(aims_out_path):\n",
    "            with open(aims_out_path, \"r\") as f:\n",
    "                calcs_finished.append(\"Leaving FHI-aims.\" in f.read())  # AIMS finished\n",
    "        else:\n",
    "            calcs_finished.append(False)\n",
    "    all_finished = np.all(calcs_finished)\n",
    "\n",
    "# Check SCF converged\n",
    "converged = []\n",
    "for A in all_idxs:\n",
    "    aims_out_path = os.path.join(struct_dir(A), \"aims.out\")\n",
    "    if os.path.exists(aims_out_path):\n",
    "        with open(aims_out_path, \"r\") as f:\n",
    "            converged.append(\n",
    "                \"Self-consistency cycle converged.\" in f.read()\n",
    "            )  # calculation converged\n",
    "    else:\n",
    "        converged.append(False)\n",
    "if np.all(converged):\n",
    "    print(\"All SCF calculations converged!\")\n",
    "else:\n",
    "    print(\n",
    "        \"Some SCF calculations did not converge. structure idxs: \",\n",
    "        [A for i, A in enumerate(all_idxs) if not converged[i]],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Perform RI fitting on the scalar field of interest (HOMO)\n",
    "\n",
    "* First we need to identify the HOMO\n",
    "* This can be done by parsing the Kohn Sham orbital information from the\n",
    "  converged SCF calculation. \n",
    "* This info got written to file \"ks_orbital_info.out\" above by using the\n",
    "  `ri_fit_write_ks_orb_info: True` keyword set in `scf_kwargs`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write KSO weights for just the HOMO to file\n",
    "for A in all_idxs:\n",
    "    # Parse the Kohn-Sham\n",
    "    ks_info = aims_parser.get_ks_orbital_info(\n",
    "        os.path.join(struct_dir(A), \"ks_orbital_info.out\")\n",
    "    )\n",
    "    kso_idxs = aims_parser.find_homo_kso_idxs(ks_info)\n",
    "    weights = np.zeros(ks_info.shape[0])\n",
    "\n",
    "    # For the HOMO states, add a weighting of 1.0\n",
    "    for kso_idx in kso_idxs:  # these are 1-indexed in AIMS\n",
    "        weights[kso_idx - 1] = 1.0\n",
    "\n",
    "    # Save these to file so AIMS can read them in\n",
    "    np.savetxt(os.path.join(struct_dir(A), \"ks_orbital_weights.in\"), weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And the general settings for all calcs\n",
    "aims_kwargs = base_aims_kwargs.copy()\n",
    "aims_kwargs.update(ri_kwargs)\n",
    "\n",
    "# Run the RI fitting procedure in AIMS\n",
    "restart_idx = 0\n",
    "aims_calc.run_aims_array(\n",
    "    calcs=calcs,\n",
    "    aims_path=aims_path,\n",
    "    aims_kwargs=aims_kwargs,\n",
    "    sbatch_kwargs=sbatch_kwargs,\n",
    "    run_dir=struct_dir,\n",
    "    restart_idx=0,   # Restart from a converged SCF\n",
    "    copy_files=[\"ks_orbital_weights.in\"],  # Copy the KS weights into the restart dir\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running this code will ensure that all calculations have finished\n",
    "# Should take < 30 seconds (1 node, 10 cores, on 'jed' HPC)\n",
    "all_finished = False\n",
    "while not all_finished:\n",
    "    calcs_finished = []\n",
    "    for A in all_idxs:\n",
    "        aims_out_path = os.path.join(struct_dir(A), f\"{restart_idx}/aims.out\")\n",
    "        if os.path.exists(aims_out_path):\n",
    "            with open(aims_out_path, \"r\") as f:\n",
    "                calcs_finished.append(\"Leaving FHI-aims.\" in f.read())  # AIMS finished\n",
    "        else:\n",
    "            calcs_finished.append(False)\n",
    "    all_finished = np.all(calcs_finished)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_dir = lambda A: os.path.join(struct_dir(A), \"0/processed/\")  # includes restart index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process AIMS results and print density fitting error\n",
    "# Run in serial: takes approx 30 seconds per structure\n",
    "df_errors = []\n",
    "for A, frame in zip(all_idxs, frames):\n",
    "    aims_parser.process_aims_ri_results(\n",
    "        frame=frame,\n",
    "        aims_output_dir=os.path.join(struct_dir(A), \"0\"),  # includes the restart idx\n",
    "        process_what=[\"coeffs\", \"ovlp\"],\n",
    "        structure_idx=A,\n",
    "    )\n",
    "    calc_info = io.unpickle_dict(os.path.join(processed_dir(A), \"calc_info.pickle\"))\n",
    "    df_errors.append(calc_info[\"df_error_percent\"][\"total\"])\n",
    "print(\"Mean density fitting error across all structures (%):\", np.mean(df_errors))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Structural Descriptors \n",
    "\n",
    "* Here we construct $\\lambda$-SOAP equivariant descriptors for each structure\n",
    "* First, find the angular orders present in the decomposition of the target\n",
    "  scalar field onto the RI basis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The basis set definition should is consistent for all atoms in the dataset,\n",
    "# given consistent AIMS settings. Print this definiton from the parsed outputs\n",
    "# from the first structure (A = 0)\n",
    "basis_set = io.unpickle_dict(\n",
    "    os.path.join(processed_dir(A=0), \"calc_info.pickle\")\n",
    ")[\"basis_set\"]\n",
    "\n",
    "basis_set[\"def\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The maximum angular order here is $l = 8$. This should be reflected in the\n",
    "settings used to generate the equivariant descriptor - specifically in\n",
    "`cg_settings` in \"settings.py\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from settings import rascal_settings, cg_settings\n",
    "\n",
    "# Generate a rascaline SphericalExpansion (2 body) representation\n",
    "calculator = rascaline.SphericalExpansion(**rascal_settings[\"hypers\"])\n",
    "nu_1_tensor = calculator.compute(frames, **rascal_settings[\"compute\"])\n",
    "nu_1_tensor = nu_1_tensor.keys_to_properties(\"species_neighbor\")\n",
    "\n",
    "# Build a lambda-SOAP descriptor by a CLebsch-Gordan combination\n",
    "lsoap = clebsch_gordan.lambda_soap_vector(nu_1_tensor, **cg_settings)\n",
    "\n",
    "# Split into per-structure TensorMaps and save into separate directories.\n",
    "# This is useful for batched training.\n",
    "for A in all_idxs:\n",
    "    lsoap_A = metatensor.slice(\n",
    "        lsoap,\n",
    "        \"samples\",\n",
    "        labels=Labels(names=\"structure\", values=np.array([A]).reshape(-1, 1)),\n",
    "    )\n",
    "    metatensor.save(os.path.join(processed_dir(A), \"lsoap.npz\"), lsoap_A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The settings used to build the descriptor from an `.xyz` file, as well as build\n",
    "the desired target property (the real-space scalar field) from the model\n",
    "prediction need to be stored so that the perform can make a true end-to-end\n",
    "prediction.\n",
    "\n",
    "In the `rholearn` module \"predictor.py\", the functions `descriptor_builder` and\n",
    "`target_builder` are implemented to perform these transformations on the input\n",
    "and output side of the model, respectively. Both take a\n",
    "variable input that depends on the structure being predicted on, and some\n",
    "settings for performing the relevant transformations of the data. The key\n",
    "physics-related settings used in predicting on an unseen structure shouldbe the\n",
    "same as were used for generating the data the model was trained on.\n",
    "\n",
    "Here we store the relevant settings in dictionaries, which will be used to\n",
    "initialize the ML model later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For descriptor building, we need to store the rascaline settings for\n",
    "# generating a SphericalExpansion and performing Clebsch-Gordan combinations.\n",
    "# The `descriptor_builder` function in \"predictor.py\" contains the 'recipe' for\n",
    "# using these settings to transform an ASE Atoms object.\n",
    "descriptor_kwargs = {\n",
    "    \"rascal_settings\": rascal_settings,\n",
    "    \"cg_settings\": cg_settings,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For target building, the base AIMS settings need to be stored, along with the\n",
    "# basis set definition.\n",
    "target_kwargs = {\n",
    "    \"aims_kwargs\": {**base_aims_kwargs},\n",
    "    \"basis_set\": {**basis_set},\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create `dataset` and `dataloader`\n",
    "\n",
    "* For cross-validation we create a train-test-val split of the data.\n",
    "* Then we initialize `metatensor`/`torch` DataSet and DataLoader objects to\n",
    "  allow for easy with mini-batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from settings import crossval_settings\n",
    "\n",
    "train_idxs, test_idxs, val_idxs = data.group_idxs(\n",
    "    all_idxs=all_idxs,\n",
    "    n_groups=crossval_settings[\"n_groups\"],\n",
    "    group_sizes=crossval_settings[\"group_sizes\"],\n",
    "    shuffle=crossval_settings[\"shuffle\"],\n",
    "    seed=crossval_settings[\"seed\"],\n",
    ")\n",
    "print(\"num train_idxs:\", len(train_idxs), \"   num test_idxs: \", len(test_idxs), \"   num val_idxs:\", len(val_idxs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from settings import torch_settings, ml_settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the torch dataset\n",
    "rho_data = data.RhoData(\n",
    "    all_idxs=np.concatenate([train_idxs, test_idxs, val_idxs]),\n",
    "    train_idxs=train_idxs,\n",
    "    in_path=lambda A: os.path.join(processed_dir(A), \"lsoap.npz\"),\n",
    "    out_path=lambda A: os.path.join(processed_dir(A), \"ri_coeffs.npz\"),\n",
    "    aux_path=lambda A: os.path.join(processed_dir(A), \"ri_ovlp.npz\"),\n",
    "    keep_in_mem=ml_settings[\"loading\"][\"train\"][\"keep_in_mem\"],\n",
    "    # calc_out_train_inv_means=True,\n",
    "    # calc_out_train_std_dev=True,\n",
    "    **torch_settings,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we initialize the model with the model architecture options, as well as\n",
    "# the descriptor/target builder settings needed for end-to-end predictions.\n",
    "model = models.RhoModel(\n",
    "    model_type=ml_settings[\"model\"][\"model_type\"],\n",
    "    input=rho_data[0][1],   # for initializing ... \n",
    "    output=rho_data[0][2],  # ... the metadata of the model\n",
    "    bias_invariants=ml_settings[\"model\"][\"bias_invariants\"],\n",
    "    # hidden_layer_widths=[8, 8],\n",
    "    # activation_fn=torch.nn.Tanh(),\n",
    "    descriptor_kwargs=descriptor_kwargs,\n",
    "    target_kwargs=target_kwargs,\n",
    "    **torch_settings\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training\n",
    "\n",
    "* Now we are ready to train a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct dataloaders for training and testing\n",
    "batch_size = 5\n",
    "train_loader = data.RhoLoader(\n",
    "    rho_data,\n",
    "    idxs=train_idxs,\n",
    "    get_aux_data=True,\n",
    "    batch_size=batch_size,\n",
    ")\n",
    "test_loader = data.RhoLoader(\n",
    "    rho_data,\n",
    "    idxs=test_idxs,\n",
    "    get_aux_data=True,\n",
    "    batch_size=batch_size,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize optimizer and loss function\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "loss_fn = loss.L2Loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 1\n",
    "\n",
    "# Start training loop\n",
    "losses = {\"train\": [], \"test\": []}\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    # Switch off metadata checking after the first epoch\n",
    "    check_args = True if epoch == 0 else False\n",
    "\n",
    "    # Iterate over train batches\n",
    "    train_loss_epoch = 0\n",
    "    for train_batch in train_loader:\n",
    "        # Reset gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Unpack train batch, respectively: structure idxs, lsoap, RI coeffs, RI\n",
    "        # overlap matrices\n",
    "        idxs_train, in_train, out_train, aux_train = train_batch\n",
    "\n",
    "        # Make a prediction\n",
    "        out_train_pred = model(in_train, check_args=check_args)\n",
    "\n",
    "        # Evaluate training loss\n",
    "        train_loss_batch = loss_fn(\n",
    "            input=out_train_pred,\n",
    "            target=out_train,\n",
    "            overlap=aux_train,\n",
    "            check_args=check_args,\n",
    "        )\n",
    "\n",
    "        # Calculate gradient and update parameters\n",
    "        train_loss_batch.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss_epoch += train_loss_batch\n",
    "\n",
    "    train_loss_epoch /= len(idxs_train)\n",
    "    losses[\"train\"].append(train_loss_epoch.detach().numpy())\n",
    "\n",
    "    # Iterate over test batches: calculate the test loss\n",
    "    with torch.no_grad():  # don't track gradients for test loss\n",
    "        test_loss_epoch = 0\n",
    "        for test_batch in test_loader:\n",
    "            # Unpack test batch\n",
    "            idxs_test, in_test, out_test, aux_test = test_batch\n",
    "\n",
    "            # Make a prediction\n",
    "            out_test_pred = model(in_test, check_args=check_args)\n",
    "\n",
    "            # Evaluate test loss\n",
    "            test_loss_batch = loss_fn(\n",
    "                input=out_test_pred,\n",
    "                target=out_test,\n",
    "                overlap=aux_test,\n",
    "                check_args=check_args,\n",
    "            )\n",
    "            test_loss_epoch += test_loss_batch\n",
    "\n",
    "        test_loss_epoch /= len(idxs_test)\n",
    "        losses[\"test\"].append(test_loss_epoch.detach().numpy())\n",
    "\n",
    "    print(\n",
    "        f\"Epoch {epoch} | Train loss: {train_loss_epoch} | Test loss: {test_loss_epoch}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot losses\n",
    "fig, ax = plt.subplots()\n",
    "for trace in [\"train\", \"test\"]:\n",
    "    ax.loglog(np.arange(n_epochs), losses[trace], label=trace)\n",
    "\n",
    "ax.legend()\n",
    "ax.set_xlabel(\"Epoch\")\n",
    "ax.set_ylabel(\"Density loss per structure\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate model performance on validation set\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a callable for directories to save predictions by structure index\n",
    "def save_dir(A):\n",
    "    return os.path.join(\"/scratch/abbott/h2o_homo/ml/predictions\", f\"{A}\")\n",
    "\n",
    "# Settings specific to RI rebuild procedure\n",
    "ri_kwargs = {\n",
    "    # Force no SCF\n",
    "    \"sc_iter_limit\": 0,\n",
    "    \"postprocess_anyway\": True,\n",
    "    \"ri_fit_assume_converged\": True,\n",
    "    # What we want to do \n",
    "    \"ri_fit_rebuild_from_coeffs\": True,\n",
    "    # What we want to output\n",
    "    \"ri_fit_write_rebuilt_field\": True,\n",
    "    \"ri_fit_write_rebuilt_field_cube\": True,\n",
    "    \"output\": [\"cube ri_fit\"],  # needed for cube files\n",
    "}\n",
    "\n",
    "# Update the AIMS and SBATCH kwargs\n",
    "tmp_aims_kwargs = {**model.target_kwargs[\"aims_kwargs\"]}\n",
    "tmp_aims_kwargs.update(ri_kwargs)\n",
    "\n",
    "# Settings for slurm\n",
    "sbatch_kwargs = {\n",
    "    \"job-name\": \"h2o-pred\",\n",
    "    \"nodes\": 1,\n",
    "    \"time\": \"01:00:00\",\n",
    "    \"mem-per-cpu\": 2000,\n",
    "    \"partition\": \"standard\",\n",
    "    \"ntasks-per-node\": 10,\n",
    "}\n",
    "\n",
    "model.update_target_kwargs(\n",
    "    {\n",
    "        \"aims_path\": aims_path,\n",
    "        \"aims_kwargs\": tmp_aims_kwargs,\n",
    "        \"sbatch_kwargs\": sbatch_kwargs,\n",
    "        \"save_dir\": save_dir,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the validation set, we already have generated the descriptors so we just\n",
    "# need to load them. First, build the validation dataloader\n",
    "val_loader = data.RhoLoader(\n",
    "    rho_data,\n",
    "    idxs=val_idxs,\n",
    "    get_aux_data=False,\n",
    "    batch_size=None,\n",
    ")\n",
    "idxs_val, in_val, out_val = next(iter(val_loader))\n",
    "\n",
    "# Load the validation frames as ASE Atoms objects\n",
    "val_frames = [frames[A] for A in idxs_val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions for the validation set\n",
    "predictions, targets = model.predict(structure_idxs=idxs_val, frames=val_frames, descriptor=in_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate density fitting error of predictions relative to SCF-converged\n",
    "# quantities\n",
    "for A, target in zip(idxs_val, targets):\n",
    "    # Get grids and check they're the same in the SCF and ML directories\n",
    "    grid = np.loadtxt(os.path.join(struct_dir(A), \"0/partition_tab.out\"))\n",
    "    assert np.allclose(grid, np.loadtxt(os.path.join(model.target_kwargs[\"save_dir\"](A), \"partition_tab.out\")))\n",
    "\n",
    "    # Get DF error\n",
    "    df_error = aims_parser.get_percent_mae_between_fields(\n",
    "        input=target,\n",
    "        target=np.loadtxt(os.path.join(struct_dir(A), \"0/rho_ref.out\")), \n",
    "        grid=grid,\n",
    "    )\n",
    "    print(f\"Val structure {A}, DF error (%): {df_error}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.loadtxt(os.path.join(struct_dir(A), \"0/rho_ri.out\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make a prediction on unseen structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load some structures we don't have a DFT reference for\n",
    "unseen_structures = ase.io.read(\n",
    "    os.path.join(data_dir, \"water_monomers_1k.xyz\"), f\"{n_frames}:{n_frames + 5}\"\n",
    ")\n",
    "\n",
    "# Make an end-to-end prediction: XYZ coordinates ---> RI coefficients\n",
    "model.predict(frames=unseen_structures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Visualize the predicted density\n",
    "A = idxs_val[0]\n",
    "val_cube = os.path.join(model.target_kwargs[\"save_dir\"](A), \"rho_rebuilt.cube\")\n",
    "v = py3Dmol.view()\n",
    "v.addModelsAsFrames(open(val_cube, \"r\").read(), \"cube\")\n",
    "v.setStyle({\"stick\": {}})\n",
    "v.addVolumetricData(\n",
    "    open(val_cube, \"r\").read(),\n",
    "    \"cube\",\n",
    "    {\"isoval\": 0.005, \"color\": \"blue\", \"opacity\": 0.8},\n",
    ")\n",
    "v.show()\n",
    "\n",
    "# Visualize the delta density"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the predicted density\n",
    "A = idxs_val[0]\n",
    "val_cube = os.path.join(struct_dir(A), \"0/rho_ref.cube\")\n",
    "v = py3Dmol.view()\n",
    "v.addModelsAsFrames(open(val_cube, \"r\").read(), \"cube\")\n",
    "v.setStyle({\"stick\": {}})\n",
    "v.addVolumetricData(\n",
    "    open(val_cube, \"r\").read(),\n",
    "    \"cube\",\n",
    "    {\"isoval\": 0.005, \"color\": \"blue\", \"opacity\": 0.8},\n",
    ")\n",
    "v.show()\n",
    "\n",
    "# Visualize the delta density"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rho",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
