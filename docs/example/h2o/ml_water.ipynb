{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Learning the electron density of water"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-requisites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMPORTANT!\n",
    "\n",
    "This notebook isn't yet fully runnable as it is required to have a specific\n",
    "binary for the (not open-source) Quantum Chemistry code FHI-aims."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install the required packages\n",
    "\n",
    "1. `pip install metatensor`\n",
    "1. `pip install chemiscope`\n",
    "1. `pip install\n",
    "   git+https://github.com/luthaf/rascaline.git@b2cedfe870541e6d037357db58de1901eb116c41`\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**NOTE**: this notebook has only been tested on the HPC system \"jed\". Due to\n",
    "specific compilation of the quantum chemistry package `FHI-aims`, the Python\n",
    "interface to running QC calculations may not be generalized to other operating\n",
    "systems or hardware."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Useful standard and scientific ML libraries\n",
    "import os\n",
    "import time\n",
    "import ase.io\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import py3Dmol\n",
    "import torch\n",
    "\n",
    "# M-Stack packages\n",
    "import metatensor   # storage format for atomistic ML\n",
    "import chemiscope  # interactive molecular visualization\n",
    "import rascaline   # generating structural representations\n",
    "from metatensor import Labels, TensorBlock, TensorMap\n",
    "from rascaline.utils import clebsch_gordan\n",
    "\n",
    "# Interfacing with FHI-aims\n",
    "from rhocalc.aims import aims_calc, aims_parser\n",
    "\n",
    "# Torch-based density leaning\n",
    "from rholearn import io, data, loss, models, predictor\n",
    "import settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from settings import data_dir\n",
    "\n",
    "# Define a function that returns the data directory for a given structure based\n",
    "# on its structure index\n",
    "def struct_dir(A):\n",
    "    return os.path.join(data_dir, f\"{A}/\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Visualize structures in dataset\n",
    "\n",
    "* Use `chemiscope`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eece55332b994a338cb09cb30eff6cc2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ChemiscopeWidget(value='{\"meta\": {\"name\": \" \"}, \"structures\": [{\"size\": 3, \"names\": [\"O\", \"H\", \"H\"], \"x\": [0.0â€¦"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from settings import idxs, frames, all_frames\n",
    "\n",
    "chemiscope.show(\n",
    "    frames,\n",
    "    properties={\n",
    "        \"Mean O-H bond length, Angstrom\": [np.mean([f.get_distance(0, 1), f.get_distance(0, 2)]) for f in frames],\n",
    "        \"H-O-H angle, degrees\": [f.get_angle(1, 0, 2) for f in frames],\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate learning targets\n",
    "\n",
    "* These are the RI coefficients of the HOMO eigenstate\n",
    "* We generate these in 2 steps: 1) SCF and 2) RI fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Converge SCF for each structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 11350005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sbatch: [ESTIMATION] The estimated cost of this job is CHF 5.60\n",
      "sbatch: [CAPPING]    All users of the account cosmo-erc have consumed 106.27 CHF\n"
     ]
    }
   ],
   "source": [
    "# Import the settings needed to run FHI-aims\n",
    "from settings import aims_path, base_aims_kwargs, scf_kwargs, ri_kwargs, sbatch_kwargs\n",
    "\n",
    "# Build a dict of settings for each calculation (i.e. structure)\n",
    "# IMPORTANT: zip() is used to pair up the structure index and the structure\n",
    "calcs = {\n",
    "    A: {\"atoms\": frame, \"run_dir\": struct_dir(A)} for A, frame in zip(idxs, frames)\n",
    "}\n",
    "\n",
    "# And the general settings for all calcs\n",
    "aims_kwargs = base_aims_kwargs.copy()\n",
    "aims_kwargs.update(scf_kwargs)\n",
    "\n",
    "# Run the SCF in AIMS\n",
    "aims_calc.run_aims_array(\n",
    "    calcs=calcs,\n",
    "    aims_path=aims_path,\n",
    "    aims_kwargs=aims_kwargs,\n",
    "    sbatch_kwargs=sbatch_kwargs,\n",
    "    run_dir=struct_dir,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All SCF calculations converged!\n"
     ]
    }
   ],
   "source": [
    "# Running this code will ensure that all calculations have finished.\n",
    "# Should take < 10 seconds (1 node, 10 cores on 'jed' HPC)\n",
    "all_finished = False\n",
    "while not all_finished:\n",
    "    calcs_finished = []\n",
    "    for A in idxs:\n",
    "        aims_out_path = os.path.join(struct_dir(A), \"aims.out\")\n",
    "        if os.path.exists(aims_out_path):\n",
    "            with open(aims_out_path, \"r\") as f:\n",
    "                calcs_finished.append(\"Leaving FHI-aims.\" in f.read())  # AIMS finished\n",
    "        else:\n",
    "            calcs_finished.append(False)\n",
    "    all_finished = np.all(calcs_finished)\n",
    "\n",
    "# Check SCF converged\n",
    "converged = []\n",
    "for A in idxs:\n",
    "    aims_out_path = os.path.join(struct_dir(A), \"aims.out\")\n",
    "    if os.path.exists(aims_out_path):\n",
    "        with open(aims_out_path, \"r\") as f:\n",
    "            converged.append(\n",
    "                \"Self-consistency cycle converged.\" in f.read()\n",
    "            )  # calculation converged\n",
    "    else:\n",
    "        converged.append(False)\n",
    "if np.all(converged):\n",
    "    print(\"All SCF calculations converged!\")\n",
    "else:\n",
    "    print(\n",
    "        \"Some SCF calculations did not converge. structure idxs: \",\n",
    "        [A for i, A in enumerate(idxs) if not converged[i]],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Perform RI fitting on the scalar field of interest (HOMO)\n",
    "\n",
    "* First we need to identify the HOMO\n",
    "* This can be done by parsing the Kohn Sham orbital information from the\n",
    "  converged SCF calculation. \n",
    "* This info got written to file \"ks_orbital_info.out\" above by using the\n",
    "  `ri_fit_write_ks_orb_info: True` keyword set in `scf_kwargs`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write KSO weights for just the HOMO to file\n",
    "for A in idxs:\n",
    "    # Parse the Kohn-Sham\n",
    "    ks_info = aims_parser.get_ks_orbital_info(\n",
    "        os.path.join(struct_dir(A), \"ks_orbital_info.out\")\n",
    "    )\n",
    "    kso_idxs = aims_parser.find_homo_kso_idxs(ks_info)\n",
    "    weights = np.zeros(ks_info.shape[0])\n",
    "\n",
    "    # For the HOMO states, add a weighting of 1.0\n",
    "    for kso_idx in kso_idxs:  # these are 1-indexed in AIMS\n",
    "        weights[kso_idx - 1] = 1.0\n",
    "\n",
    "    # Save these to file so AIMS can read them in\n",
    "    np.savetxt(os.path.join(struct_dir(A), \"ks_orbital_weights.in\"), weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And the general settings for all calcs\n",
    "aims_kwargs = base_aims_kwargs.copy()\n",
    "aims_kwargs.update(ri_kwargs)\n",
    "\n",
    "# Run the RI fitting procedure in AIMS\n",
    "restart_idx = 0\n",
    "aims_calc.run_aims_array(\n",
    "    calcs=calcs,\n",
    "    aims_path=aims_path,\n",
    "    aims_kwargs=aims_kwargs,\n",
    "    sbatch_kwargs=sbatch_kwargs,\n",
    "    run_dir=struct_dir,\n",
    "    restart_idx=0,   # Restart from a converged SCF\n",
    "    copy_files=[\"ks_orbital_weights.in\"],  # Copy the KS weights into the restart dir\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running this code will ensure that all calculations have finished\n",
    "# Should take < 30 seconds (1 node, 10 cores, on 'jed' HPC)\n",
    "all_finished = False\n",
    "while not all_finished:\n",
    "    calcs_finished = []\n",
    "    for A in idxs:\n",
    "        aims_out_path = os.path.join(struct_dir(A), f\"{restart_idx}/aims.out\")\n",
    "        if os.path.exists(aims_out_path):\n",
    "            with open(aims_out_path, \"r\") as f:\n",
    "                calcs_finished.append(\"Leaving FHI-aims.\" in f.read())  # AIMS finished\n",
    "        else:\n",
    "            calcs_finished.append(False)\n",
    "    all_finished = np.all(calcs_finished)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process AIMS results and print density fitting error\n",
    "# Run in serial: takes approx 0.5 seconds per structure\n",
    "df_errors = []\n",
    "for A, frame in zip(idxs, frames):\n",
    "    aims_parser.process_aims_ri_results(\n",
    "        frame=frame,\n",
    "        aims_output_dir=os.path.join(struct_dir(A), \"0\"),  # includes the restart idx\n",
    "        process_what=[\"coeffs\", \"ovlp\"],\n",
    "        structure_idx=A,\n",
    "    )\n",
    "    calc_info = io.unpickle_dict(os.path.join(os.path.join(struct_dir(A), \"0/processed/\"), \"calc_info.pickle\"))\n",
    "    df_errors.append(calc_info[\"df_error_percent\"][\"total\"])\n",
    "print(\"Mean density fitting error across all structures (%):\", np.mean(df_errors))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Structural Descriptors \n",
    "\n",
    "* Here we construct $\\lambda$-SOAP equivariant descriptors for each structure\n",
    "* First, find the angular orders present in the decomposition of the target\n",
    "  scalar field onto the RI basis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The basis set definition should is consistent for all atoms in the dataset,\n",
    "# given consistent AIMS settings. Print this definiton from the parsed outputs\n",
    "# from one of the structures\n",
    "basis_set = io.unpickle_dict(\n",
    "    os.path.join(os.path.join(struct_dir(A=idxs[0]), \"0/processed/\"), \"calc_info.pickle\")\n",
    ")[\"basis_set\"]\n",
    "\n",
    "basis_set[\"def\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The maximum angular order here is $l = 8$. This should be reflected in the\n",
    "settings used to generate the equivariant descriptor - specifically in\n",
    "`cg_settings` in \"settings.py\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from settings import all_frames, rascal_settings, cg_settings\n",
    "\n",
    "# Generate a rascaline SphericalExpansion (2 body) representation. As we want to\n",
    "# retain the original structure indices, we are going to pass * all * of the\n",
    "# 1000 frames to rascaline, but only compute for the subset of structures in\n",
    "# `idxs`.\n",
    "calculator = rascaline.SphericalExpansion(**rascal_settings[\"hypers\"])\n",
    "nu_1_tensor = calculator.compute(\n",
    "    all_frames, \n",
    "    selected_samples=Labels(names=[\"structure\"], values=idxs.reshape(-1, 1)),\n",
    "    **rascal_settings[\"compute\"],\n",
    ")\n",
    "nu_1_tensor = nu_1_tensor.keys_to_properties(\"species_neighbor\")\n",
    "\n",
    "# Build a lambda-SOAP descriptor by a CLebsch-Gordan combination\n",
    "lsoap = clebsch_gordan.lambda_soap_vector(nu_1_tensor, **cg_settings)\n",
    "\n",
    "# Check the resulting structure indices match those in `idxs`\n",
    "assert np.all(\n",
    "    np.sort(idxs)\n",
    "    == metatensor.unique_metadata(lsoap, \"samples\", \"structure\").values.reshape(-1)\n",
    ")\n",
    "\n",
    "# Split into per-structure TensorMaps and save into separate directories.\n",
    "# This is useful for batched training.\n",
    "for A in idxs:\n",
    "    lsoap_A = metatensor.slice(\n",
    "        lsoap,\n",
    "        \"samples\",\n",
    "        labels=Labels(names=\"structure\", values=np.array([A]).reshape(-1, 1)),\n",
    "    )\n",
    "    metatensor.save(os.path.join(os.path.join(struct_dir(A), \"0/processed/\"), \"lsoap.npz\"), lsoap_A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The settings used to build the descriptor from an `.xyz` file, as well as build\n",
    "the desired target property (the real-space scalar field) from the model\n",
    "prediction need to be stored so that the perform can make a true end-to-end\n",
    "prediction.\n",
    "\n",
    "In the `rholearn` module \"predictor.py\", the functions `descriptor_builder` and\n",
    "`target_builder` are implemented to perform these transformations on the input\n",
    "and output side of the model, respectively. Both take a\n",
    "variable input that depends on the structure being predicted on, and some\n",
    "settings for performing the relevant transformations of the data. The key\n",
    "physics-related settings used in predicting on an unseen structure shouldbe the\n",
    "same as were used for generating the data the model was trained on.\n",
    "\n",
    "Here we store the relevant settings in dictionaries, which will be used to\n",
    "initialize the ML model later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create `dataset` and `dataloader`\n",
    "\n",
    "* For cross-validation we create a train-test-val split of the data.\n",
    "* Then we initialize `metatensor`/`torch` DataSet and DataLoader objects to\n",
    "  allow for easy with mini-batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from settings import crossval_settings, torch_settings, ml_settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform a train/test/val split of structure idxs\n",
    "train_idxs, test_idxs, val_idxs = data.group_idxs(\n",
    "    idxs=idxs,\n",
    "    n_groups=crossval_settings[\"n_groups\"],\n",
    "    group_sizes=crossval_settings[\"group_sizes\"],\n",
    "    shuffle=crossval_settings[\"shuffle\"],\n",
    "    seed=crossval_settings[\"seed\"],\n",
    ")\n",
    "print(\"num train_idxs:\", len(train_idxs), \"   num test_idxs: \", len(test_idxs), \"   num val_idxs:\", len(val_idxs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crossval_category = lambda A: 0 if A in train_idxs else (1 if A in test_idxs else 2)\n",
    "\n",
    "chemiscope.show(\n",
    "    frames,\n",
    "    properties={\n",
    "        \"Mean O-H bond length, Angstrom\": [\n",
    "            np.mean([frame.get_distance(0, 1), frame.get_distance(0, 2)])\n",
    "            for A, frame in zip(idxs, frames)\n",
    "        ],\n",
    "        \"H-O-H angle, degrees\": [\n",
    "            frame.get_angle(1, 0, 2) for A, frame in zip(idxs, frames)\n",
    "        ],\n",
    "        \"0: train, 1: test, 2: val\": [crossval_category(A) for A, frame in zip(idxs, frames)]\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define callable for path to processed data (i.e. TensorMaps)\n",
    "processed_dir = lambda A: os.path.join(struct_dir(A), \"0/processed/\")  # includes restart index\n",
    "\n",
    "# Construct dataset\n",
    "rho_data = data.RhoData(\n",
    "    idxs=np.concatenate([train_idxs, test_idxs, val_idxs]),\n",
    "    train_idxs=train_idxs,\n",
    "    in_path=lambda A: os.path.join(processed_dir(A), \"lsoap.npz\"),\n",
    "    out_path=lambda A: os.path.join(processed_dir(A), \"ri_coeffs.npz\"),\n",
    "    aux_path=lambda A: os.path.join(processed_dir(A), \"ri_ovlp.npz\"),\n",
    "    keep_in_mem=ml_settings[\"loading\"][\"train\"][\"keep_in_mem\"],\n",
    "    calc_out_train_inv_means=True,\n",
    "    calc_out_train_std_dev=True,\n",
    "    **torch_settings,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct dataloaders\n",
    "train_loader = data.RhoLoader(\n",
    "    rho_data,\n",
    "    idxs=train_idxs,\n",
    "    get_aux_data=True,\n",
    "    batch_size=ml_settings[\"loading\"][\"train\"][\"batch_size\"],\n",
    ")\n",
    "test_loader = data.RhoLoader(\n",
    "    rho_data,\n",
    "    idxs=test_idxs,\n",
    "    get_aux_data=True,\n",
    "    batch_size=ml_settings[\"loading\"][\"test\"][\"batch_size\"],\n",
    ")\n",
    "# For the validation set, we want to evaluate the performance of the model\n",
    "# against the real-space scalar field (requires calling AIMS), so the overlaps\n",
    "# do not need to be loaded.\n",
    "val_loader = data.RhoLoader(\n",
    "    rho_data,\n",
    "    idxs=val_idxs,\n",
    "    get_aux_data=False,\n",
    "    batch_size=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from settings import rascal_settings, cg_settings, base_aims_kwargs\n",
    "\n",
    "# For descriptor building, we need to store the rascaline settings for\n",
    "# generating a SphericalExpansion and performing Clebsch-Gordan combinations.\n",
    "# The `descriptor_builder` function in \"predictor.py\" contains the 'recipe' for\n",
    "# using these settings to transform an ASE Atoms object.\n",
    "descriptor_kwargs = {\n",
    "    \"rascal_settings\": rascal_settings,\n",
    "    \"cg_settings\": cg_settings,\n",
    "}\n",
    "\n",
    "# For target building, the base AIMS settings need to be stored, along with the\n",
    "# basis set definition.\n",
    "basis_set = io.unpickle_dict(\n",
    "    os.path.join(os.path.join(struct_dir(A=idxs[0]), \"0/processed/\"), \"calc_info.pickle\")\n",
    ")[\"basis_set\"]\n",
    "\n",
    "target_kwargs = {\n",
    "    \"aims_kwargs\": {**base_aims_kwargs},\n",
    "    \"basis_set\": {**basis_set},\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we initialize the model with the model architecture options, as well as\n",
    "# the descriptor/target builder settings needed for end-to-end predictions.\n",
    "model = models.RhoModel(\n",
    "    model_type=ml_settings[\"model\"][\"model_type\"],\n",
    "    input=rho_data[idxs[0]][1],   # for initializing ... \n",
    "    output=rho_data[idxs[0]][2],  # ... the metadata of the model\n",
    "    bias_invariants=ml_settings[\"model\"][\"bias_invariants\"],\n",
    "    hidden_layer_widths=ml_settings[\"model\"][\"hidden_layer_widths\"],\n",
    "    activation_fn=ml_settings[\"model\"][\"activation_fn\"],\n",
    "    descriptor_kwargs=descriptor_kwargs,\n",
    "    target_kwargs=target_kwargs,\n",
    "    **torch_settings\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training\n",
    "\n",
    "* Now we are ready to train a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize optimizer and loss function\n",
    "loss_fn = loss.L2Loss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "scheduler = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rholearn import train\n",
    "\n",
    "n_epochs = 100\n",
    "\n",
    "chkpt_dir = os.path.join(\"ml\", \"checkpoints\")\n",
    "if not os.path.exists(chkpt_dir):\n",
    "    os.makedirs(chkpt_dir)\n",
    "\n",
    "log_path = \"ml/training.log\"\n",
    "io.log(log_path, \"# epoch train_loss test_loss\")\n",
    "\n",
    "# Start training loop\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "\n",
    "    # Training step\n",
    "    train_loss_epoch, test_loss_epoch = train.training_step(\n",
    "        train_loader=train_loader,\n",
    "        test_loader=test_loader,\n",
    "        model=model,\n",
    "        loss_fn=loss_fn,\n",
    "        optimizer=optimizer,\n",
    "        scheduler=scheduler,\n",
    "        check_args=True if epoch == 1 else False,  # Switch off metadata checks after 1st epoch\n",
    "    )\n",
    "    print(\n",
    "        f\"epoch {epoch} train_loss {train_loss_epoch} test_loss {test_loss_epoch}\"\n",
    "    )\n",
    "    io.log(log_path, f\"{epoch} {train_loss_epoch} {test_loss_epoch}\")\n",
    "    torch.save(model, os.path.join(chkpt_dir, f\"model_{epoch}.pt\"))\n",
    "    torch.save(optimizer.state_dict, os.path.join(chkpt_dir, f\"opt_{epoch}.pt\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate model performance on validation set\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained model\n",
    "torch.load(\"ml/checkpoints/model_1001.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot losses\n",
    "\n",
    "losses = np.loadtxt(\"/scratch/abbott/h2o_homo/ml/training.log\")\n",
    "fig, ax = plt.subplots()\n",
    "for i, trace in enumerate([\"train\", \"test\"], start=1):\n",
    "    ax.loglog(losses[:, 0], losses[:, i], label=trace)\n",
    "\n",
    "ax.legend()\n",
    "ax.set_xlabel(\"Epoch\")\n",
    "ax.set_ylabel(\"L2 loss per structure\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from settings import aims_path\n",
    "\n",
    "# Create a callable for directories to save predictions by structure index\n",
    "save_dir = lambda A: os.path.join(\"/scratch/abbott/h2o_homo/ml/predictions\", f\"{A}\")\n",
    "\n",
    "# Settings specific to RI rebuild procedure\n",
    "ri_kwargs = {\n",
    "    # Force no SCF\n",
    "    \"sc_iter_limit\": 0,\n",
    "    \"postprocess_anyway\": True,\n",
    "    \"ri_fit_assume_converged\": True,\n",
    "    # What we want to do \n",
    "    \"ri_fit_rebuild_from_coeffs\": True,\n",
    "    # What we want to output\n",
    "    \"ri_fit_write_rebuilt_field\": True,\n",
    "    \"ri_fit_write_rebuilt_field_cube\": True,\n",
    "    \"output\": [\"cube ri_fit\"],  # needed for cube files\n",
    "}\n",
    "\n",
    "# Update the AIMS and SBATCH kwargs\n",
    "tmp_aims_kwargs = {**model.target_kwargs[\"aims_kwargs\"]}\n",
    "tmp_aims_kwargs.update(ri_kwargs)\n",
    "\n",
    "# Settings for slurm\n",
    "sbatch_kwargs = {\n",
    "    \"job-name\": \"h2o-pred\",\n",
    "    \"nodes\": 1,\n",
    "    \"time\": \"01:00:00\",\n",
    "    \"mem-per-cpu\": 2000,\n",
    "    \"partition\": \"standard\",\n",
    "    \"ntasks-per-node\": 10,\n",
    "}\n",
    "\n",
    "model.update_target_kwargs(\n",
    "    {\n",
    "        \"aims_path\": aims_path,\n",
    "        \"aims_kwargs\": tmp_aims_kwargs,\n",
    "        \"sbatch_kwargs\": sbatch_kwargs,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the validation frames as ASE Atoms objects\n",
    "val_frames = [frames[A] for A in val_idxs]\n",
    "\n",
    "# Make predictions for the validation set. Note: we could predict from the\n",
    "# descriptors that are already constructed here, but do so from ASE frames for\n",
    "# demonstrative purposes\n",
    "pred_coeffs, pred_fields = model.predict(structure_idxs=val_idxs, frames=val_frames, save_dir=save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate density fitting error of predictions relative to SCF-converged\n",
    "# real-space scalar field\n",
    "val_df_errors = {}\n",
    "for A, pred_field in zip(val_idxs, pred_fields):\n",
    "    # Get grids and check they're the same in the SCF and ML directories\n",
    "    scf_grid = np.loadtxt(os.path.join(struct_dir(A), \"0/partition_tab.out\"))\n",
    "    ml_grid = np.loadtxt(os.path.join(model.target_kwargs[\"save_dir\"](A), \"partition_tab.out\"))\n",
    "    assert np.allclose(scf_grid, ml_grid)\n",
    "\n",
    "    # Get DF error\n",
    "    target_field = np.loadtxt(os.path.join(struct_dir(A), \"0/rho_ref.out\"))\n",
    "    df_error = aims_parser.get_percent_mae_between_fields(\n",
    "        input=pred_field,\n",
    "        target=target_field, \n",
    "        grid=scf_grid,\n",
    "    )\n",
    "    print(f\"Val structure {A}, DF error (%): {df_error}\")\n",
    "    val_df_errors[A] = df_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chemiscope.show(\n",
    "    val_frames,\n",
    "    properties={\n",
    "        \"Mean O-H bond length, Angstrom\": [\n",
    "            np.mean([frame.get_distance(0, 1), frame.get_distance(0, 2)])\n",
    "            for A, frame in zip(val_idxs, val_frames)\n",
    "        ],\n",
    "        \"H-O-H angle, degrees\": [frame.get_angle(1, 0, 2) for A, frame in zip(val_idxs, val_frames)],\n",
    "        \"DF Error\": [val_df_errors[A] for A, frame in zip(val_idxs, val_frames)],\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End-to-end prediction on an unseen structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load some structures we don't have a DFT reference for\n",
    "idxs_unseen = np.arange(np.max(idxs) + 1, np.max(idxs) + 2)\n",
    "frames_unseen = ase.io.read(os.path.join(data_dir, \"water_monomers_1k.xyz\"), \":\")\n",
    "frames_unseen = [frames_unseen[A] for A in idxs_unseen]\n",
    "\n",
    "# Make an end-to-end prediction: XYZ coordinates ---> RI coefficients\n",
    "predictions, targets = model.predict(structure_idxs=idxs_unseen, frames=frames_unseen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Visualize the predicted density\n",
    "A = idxs_unseen[0]\n",
    "val_cube = os.path.join(model.target_kwargs[\"save_dir\"](A), \"rho_rebuilt.cube\")\n",
    "v = py3Dmol.view()\n",
    "v.addModelsAsFrames(open(val_cube, \"r\").read(), \"cube\")\n",
    "v.setStyle({\"stick\": {}})\n",
    "v.addVolumetricData(\n",
    "    open(val_cube, \"r\").read(),\n",
    "    \"cube\",\n",
    "    {\"isoval\": 0.005, \"color\": \"blue\", \"opacity\": 0.8},\n",
    ")\n",
    "v.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the predicted density\n",
    "A = idxs_unseen[0]\n",
    "val_cube = os.path.join(struct_dir(A), \"0/rho_ref.cube\")\n",
    "v = py3Dmol.view()\n",
    "v.addModelsAsFrames(open(val_cube, \"r\").read(), \"cube\")\n",
    "v.setStyle({\"stick\": {}})\n",
    "v.addVolumetricData(\n",
    "    open(val_cube, \"r\").read(),\n",
    "    \"cube\",\n",
    "    {\"isoval\": 0.005, \"color\": \"blue\", \"opacity\": 0.8},\n",
    ")\n",
    "v.show()\n",
    "\n",
    "# Visualize the delta density"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rho",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
