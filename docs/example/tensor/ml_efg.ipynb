{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Equivariant learning of tensorial properties: EFG tensors\n",
    "\n",
    "This is a demonstration of equivariant of a tensorial property - EFG (or NMR\n",
    "shielding) tensors using pre-calculated data expressed in the angular basis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Requirements\n",
    "\n",
    "* python > 3.10\n",
    "\n",
    "### Required packages\n",
    "\n",
    "For stability, it is best to install `rascaline` and `rho_learn` from specific commits.\n",
    "\n",
    "1. `pip install ase chemiscope metatensor torch==2.1.0 wigners`\n",
    "1. `pip install git+https://github.com/luthaf/rascaline.git@b2cedfe870541e6d037357db58de1901eb116c41`\n",
    "1. `pip install git+https://github.com/jwa7/rho_learn.git@bb2e565f0c289291976e9b3371eb1c35832a805f`\n",
    "\n",
    "### Set-up\n",
    "\n",
    "* Inspect `settings.py` and change the `TOP_DIR` global variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import time\n",
    "import ase.io\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# From the COSMO-stack\n",
    "import metatensor   # storage format for atomistic ML\n",
    "import chemiscope  # interactive molecular visualization\n",
    "import rascaline   # generating structural representations\n",
    "from metatensor import Labels, TensorBlock, TensorMap\n",
    "from rascaline.utils import clebsch_gordan\n",
    "\n",
    "# Torch-based equivariant leaning\n",
    "from rholearn import io, data, loss, models\n",
    "import settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from settings import TOP_DIR, DATA_DIR, ML_DIR, DATA_SETTINGS\n",
    "\n",
    "print(\"Top directory defined as: \", TOP_DIR)\n",
    "\n",
    "# Load the frames in the complete dataset\n",
    "all_frames = DATA_SETTINGS[\"all_frames\"]\n",
    "\n",
    "# Shuffle the total set of structure indices\n",
    "idxs = np.arange(len(all_frames))\n",
    "np.random.default_rng(seed=DATA_SETTINGS[\"seed\"]).shuffle(idxs)\n",
    "\n",
    "# Take a subset of the frames if desired\n",
    "idxs = idxs[:DATA_SETTINGS[\"n_frames\"]]\n",
    "frames = [all_frames[A] for A in idxs]\n",
    "\n",
    "# Define a callable that returns the path of a dir containing the data\n",
    "# for a single structure\n",
    "def struct_dir(A):\n",
    "    return os.path.join(DATA_DIR, f\"{A}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Visualize structures in dataset\n",
    "\n",
    "* Use `chemiscope`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chemiscope.show(\n",
    "    frames,\n",
    "    mode=\"structure\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert EFG tensors to metatensor format\n",
    "\n",
    "* Here we read the tensorial properties from the info line of the `.xyz` file\n",
    "  and convert the data to `metatensor.TensorMap` format. \n",
    "  \n",
    "* This data features EFG tensors in the angular basis, of angular order $l \\in\n",
    "  {0, 2}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will create a TensorMap for each frame, then combine them into a single\n",
    "# TensorMap at the end\n",
    "structure_tms = []\n",
    "for A, frame in zip(idxs, frames):\n",
    "    # Get the number of atoms in the frame\n",
    "    n_atoms = frame.get_global_number_of_atoms()\n",
    "\n",
    "    # Store the target data by l value and chemical species (i.e. atomic number)\n",
    "    data_dict = {}\n",
    "    for atom_i, atomic_number in enumerate(frame.get_atomic_numbers()):\n",
    "        for l, data_arr_all_atoms in zip(\n",
    "            [0, 2], [frames[0].arrays[\"efg_L0\"], frames[0].arrays[\"efg_L2\"]]\n",
    "        ):\n",
    "            key = (l, atomic_number)\n",
    "            data_arr = data_arr_all_atoms[atom_i]\n",
    "            if isinstance(data_arr, float):\n",
    "                data_arr = np.array([data_arr])\n",
    "            # Store the data array\n",
    "            if data_dict.get(key) is None:\n",
    "                data_dict[key] = {atom_i: data_arr}\n",
    "            else:\n",
    "                data_dict[key][atom_i] = data_arr\n",
    "\n",
    "    # Build the keys of the resulting TensorMap\n",
    "    keys = Labels(\n",
    "        names=[\"spherical_harmonics_l\", \"species_center\"],\n",
    "        values=np.array(\n",
    "            [[l, species_center] for l, species_center in data_dict.keys()]\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    # Construct the TensorMap blocks for each of these keys\n",
    "    blocks = []\n",
    "    for l, species_center in keys.values:\n",
    "        # Retrive the raw block data\n",
    "        data_block = data_dict[(l, species_center)]\n",
    "\n",
    "        # Get a list of sorted samples (i.e. atom indices) for the block\n",
    "        n_atoms_block = len(data_block)\n",
    "        ordered_atom_idxs = sorted(data_block.keys())\n",
    "\n",
    "        # Sort the raw block data\n",
    "        block_data = np.array([data_block[atom_i] for atom_i in ordered_atom_idxs]).reshape(\n",
    "            n_atoms_block, 2 * l + 1, 1\n",
    "        )\n",
    "\n",
    "        # Construct a TensorBlock, where the raw data is labelled with metadata\n",
    "        # Note here that we keep track of the structure index - this is\n",
    "        # important for later when we join the TensorMaps\n",
    "        block = TensorBlock(\n",
    "            values=block_data,\n",
    "            samples=Labels(\n",
    "                names=[\"structure\", \"center\"],\n",
    "                values=np.array([[A, atom_i] for atom_i in ordered_atom_idxs]),\n",
    "            ),\n",
    "            components=[\n",
    "                Labels(\n",
    "                    names=[\"spherical_harmonics_m\"],\n",
    "                    values=np.arange(-l, l + 1).reshape(-1, 1),\n",
    "                )\n",
    "            ],\n",
    "            properties=Labels(\n",
    "                names=[\"efg\"],\n",
    "                values=np.array([[0]]).reshape(-1, 1),\n",
    "            ),\n",
    "        )\n",
    "        # Store the block\n",
    "        blocks.append(block)\n",
    "\n",
    "    # Construct a TensorMap for this structure from the keys and blocks\n",
    "    efg = TensorMap(keys=keys, blocks=blocks)\n",
    "\n",
    "    # Save the TensorMap to file\n",
    "    if not os.path.exists(struct_dir(A)):\n",
    "        os.mkdir(struct_dir(A))\n",
    "    metatensor.save(os.path.join(struct_dir(A), \"efg.npz\"), efg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Structural Descriptors \n",
    "\n",
    "* Here we construct $\\lambda$-SOAP equivariant descriptors for each structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from settings import RASCAL_SETTINGS, CG_SETTINGS\n",
    "\n",
    "# Generate a rascaline SphericalExpansion (2 body) representation. As we want to\n",
    "# retain the original structure indices, we are going to pass * all * of the\n",
    "# 1000 frames to rascaline, but only compute for the subset of structures in\n",
    "# `idxs`.\n",
    "calculator = rascaline.SphericalExpansion(**RASCAL_SETTINGS[\"hypers\"])\n",
    "nu_1_tensor = calculator.compute(\n",
    "    all_frames, \n",
    "    selected_samples=Labels(names=[\"structure\"], values=idxs.reshape(-1, 1)),\n",
    "    **RASCAL_SETTINGS[\"compute\"],\n",
    ")\n",
    "nu_1_tensor = nu_1_tensor.keys_to_properties(\"species_neighbor\")\n",
    "\n",
    "# Build a lambda-SOAP descriptor by a CLebsch-Gordan combination\n",
    "lsoap = clebsch_gordan.lambda_soap_vector(nu_1_tensor, **CG_SETTINGS)\n",
    "\n",
    "# Check the resulting structure indices match those in `idxs`\n",
    "assert np.all(\n",
    "    np.sort(idxs)\n",
    "    == metatensor.unique_metadata(lsoap, \"samples\", \"structure\").values.reshape(-1)\n",
    ")\n",
    "\n",
    "# Split into per-structure TensorMaps and save into separate directories.\n",
    "# This is useful for batched training.\n",
    "for A in idxs:\n",
    "    lsoap_A = metatensor.slice(\n",
    "        lsoap,\n",
    "        \"samples\",\n",
    "        labels=Labels(names=\"structure\", values=np.array([A]).reshape(-1, 1)),\n",
    "    )\n",
    "    metatensor.save(os.path.join(struct_dir(A), \"lsoap.npz\"), lsoap_A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build `dataset`\n",
    "\n",
    "* For cross-validation we create a train-test-val split of the data.\n",
    "* Data is stored on the per-structure basis to help with mini-batching in\n",
    "  training.\n",
    "\n",
    "\n",
    "* Although we have generated data for both the HOMO and LUMO, let's just learn\n",
    "  the HOMO. Fix the \"restart_idx\" to 0 (for the HOMO), then we can build a\n",
    "  dataset and start training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define dir where model checkpoints are saved\n",
    "chkpt_dir = os.path.join(ML_DIR, \"checkpoints\")\n",
    "\n",
    "if not os.path.exists(ML_DIR):\n",
    "    os.makedirs(ML_DIR)\n",
    "if not os.path.exists(chkpt_dir):\n",
    "    os.makedirs(chkpt_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from settings import CROSSVAL_SETTINGS, ML_SETTINGS, TORCH_SETTINGS\n",
    "\n",
    "# Perform a train/test/val split of structure idxs\n",
    "train_idxs, test_idxs, val_idxs = data.group_idxs(\n",
    "    idxs=idxs,\n",
    "    n_groups=CROSSVAL_SETTINGS[\"n_groups\"],\n",
    "    group_sizes=CROSSVAL_SETTINGS[\"group_sizes\"],\n",
    "    shuffle=CROSSVAL_SETTINGS[\"shuffle\"],\n",
    "    seed=DATA_SETTINGS[\"seed\"],\n",
    ")\n",
    "print(\n",
    "    \"num train_idxs:\",\n",
    "    len(train_idxs),\n",
    "    \"   num test_idxs:\",\n",
    "    len(test_idxs),\n",
    "    \"   num val_idxs:\",\n",
    "    len(val_idxs),\n",
    ")\n",
    "np.savez(\n",
    "    os.path.join(ML_DIR, \"idxs.npz\"),\n",
    "    idxs=idxs,\n",
    "    train_idxs=train_idxs,\n",
    "    test_idxs=test_idxs,\n",
    "    val_idxs=val_idxs,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct dataset, defining callables to access the input, output, and overlap\n",
    "# data from the structure indices\n",
    "efg_data = data.RhoData(\n",
    "    idxs=idxs,\n",
    "    in_path=lambda A: os.path.join(struct_dir(A), \"lsoap.npz\"),\n",
    "    out_path=lambda A: os.path.join(struct_dir(A), \"efg.npz\"),\n",
    "    keep_in_mem=ML_SETTINGS[\"loading\"][\"keep_in_mem\"],\n",
    "    **TORCH_SETTINGS,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize model\n",
    "\n",
    "* In order to make an end-to-end prediction, we want to store the settings\n",
    "  needed to build the equivariant descriptor from ASE frames, and well as the\n",
    "  'recipe' for doing so.\n",
    "\n",
    "* In the module `predictor.py`, the function `descriptor_builder` contains this\n",
    "  recipe, identical to the one shown above. The function takes as input the ASE\n",
    "  frames and settings and builds the $\\lambda$-SOAP equivariant. A prediction\n",
    "  can then be made on this.\n",
    "\n",
    "* By storing the settings in the model upon initialization, the\n",
    "  `model.predict()` method can build descriptors using the same settings as used\n",
    "  to generate the training data (required), by calling the `descriptor_builder`\n",
    "  function.\n",
    "\n",
    "* This is a convenience method, allowing an easy end-to-end prediction and\n",
    "  hiding all of the intermediate steps. Be aware however that this is dependent\n",
    "  on an external method (i.e. `descriptor_builder`) built into the source code:\n",
    "  ideally, we would like the descriptor calculators themselves (not just the\n",
    "  settings) to be savable as part of the model. \n",
    "\n",
    "* With the current implementation, there is a risk that the `descriptor_builder`\n",
    "  function becomes obselete upon API changes - specifically with respect to the\n",
    "  CG iteration code which is still in development.\n",
    "\n",
    "* Of course, using the software versions from commit numbers as above will\n",
    "  ensure things work, but this is worth being aware of.\n",
    "  \n",
    "* One can always extract the descriptor settings from a model (atribute\n",
    "  `model.descriptor_kwargs`) and manually build a descriptor, as we did above,\n",
    "  before calling the `model.forward()` method. If making a prediction this\n",
    "  should be done within the `with torch.no_grad()` context manager or by setting\n",
    "  the model to evaluation mode with `model.eval()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptor_kwargs = {\n",
    "    \"rascal_settings\": RASCAL_SETTINGS,\n",
    "    \"cg_settings\": CG_SETTINGS,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Here we intialize a global model that makes predicitons on the TensorMap level\n",
    "\n",
    "* For demo purposes we set each block model to be a linear model. A learnable\n",
    "  bias is used for invariant blocks, but cannot be applied to covariant blocks\n",
    "  ($\\lambda > 0$) as this is a non-linear transformation that would break equivariance.\n",
    "\n",
    "* One could also use nonlinear block models which incorporate nonlinear\n",
    "  transformations of invariant features using neural networks of (in\n",
    "  principle)arbitrary complexity.\n",
    "  \n",
    "* The `RhoModel` class is built to be able to use these different types of block\n",
    "  models without breaking equivariance. For instance, covariant blocks cannot\n",
    "  themselves be passed through neural networks (nonlinear transformations that\n",
    "  woudl break equivariance), but passing the corresponding invariant block for\n",
    "  that species and using that as a linear multiplier is possible. This is the\n",
    "  method implemented in this class, inspired by paper\n",
    "  https://doi.org/10.1063/1.5090481, in particular Figure 5.\n",
    "\n",
    "* Upon initialization, an example input/output pair from the dataset is passed\n",
    "  to the constructor. The relevant metadata for each block (i.e. the components\n",
    "  and properties) is extracted. This is used to initialize the dimensions of the\n",
    "  underlying pure torch models.\n",
    "  torch models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from settings import ML_SETTINGS\n",
    "\n",
    "# Initialize model\n",
    "model = models.RhoModel(\n",
    "    # Model architecture\n",
    "    model_type=ML_SETTINGS[\"model\"][\"model_type\"],  # \"linear\" or \"nonlinear\"\n",
    "    input=efg_data[idxs[0]][1],   # example input data for init metadata\n",
    "    output=efg_data[idxs[0]][2],  # example output data for init metadata\n",
    "    bias_invariants=ML_SETTINGS[\"model\"][\"bias_invariants\"],\n",
    "\n",
    "    # Extra architecture settings if using a nonlinear base model\n",
    "    # hidden_layer_widths=ML_SETTINGS[\"model\"].get(\"hidden_layer_widths\"),\n",
    "    # activation_fn=ML_SETTINGS[\"model\"].get(\"activation_fn\"),\n",
    "    # bias_nn=ML_SETTINGS[\"model\"].get(\"bias_nn\"),\n",
    "\n",
    "    # For end-to-end predictions\n",
    "    descriptor_kwargs=descriptor_kwargs,\n",
    "    \n",
    "    # Torch tensor settings\n",
    "    **TORCH_SETTINGS,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the input metadata of the model - notice the components / properties\n",
    "# are stored, but not the samples. No data is stored either (block values are\n",
    "# just zero)\n",
    "model.in_metadata.keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the output metadata\n",
    "model.out_metadata.block(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the block models - these are stored as a torch ModuleList; one for\n",
    "# each of the keys the model is defined for. For instance, the first block model\n",
    "# corresponds to key `model.in_metadata.keys[0]`\n",
    "model.models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Equivariance Check!\n",
    "\n",
    "* Both the code that performs the transformations `.xyz` data -> equivariant descriptor (i.e. the\n",
    "  $\\lambda$-SOAP builder) and descriptor -> property (i.e. the model) should\n",
    "  preserve equivariance.\n",
    "\n",
    "* We can test this by performing an end-to-end prediction (i.e. `.xyz` ->\n",
    "  descriptor -> property) for a rotated and unrotated form of the same\n",
    "  structure, and checking that the predicted target property has transformed\n",
    "  equivariantly with the input structure - i.e. the behaviour under rotation is\n",
    "  described by a set of Wigner-D matrices for each angular order $\\lambda$ the\n",
    "  descriptor/model/target is decomposed into.\n",
    "\n",
    "* A model should be equivariant regardless of the accuracy of the prediction.\n",
    "  Therefore, we can test equivariance on a untrained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Wigner-D matrices, initialized with random angles\n",
    "wig = clebsch_gordan.WignerDReal(lmax=2*RASCAL_SETTINGS[\"hypers\"][\"max_angular\"])\n",
    "print(\"Random rotation angles (rad):\", wig.angles)\n",
    "\n",
    "# Apply an O(3) transformation to each frame \n",
    "frames_o3 = [clebsch_gordan.transform_frame_o3(frame, wig.angles) for frame in frames]\n",
    "assert not np.allclose(frames[0].positions, frames_o3[0].positions)\n",
    "\n",
    "# Generate lambda-SOAP for the transformed frames\n",
    "predictions = model.predict(structure_idxs=idxs, frames=frames)\n",
    "predictions_o3 = model.predict(structure_idxs=idxs, frames=frames_o3)\n",
    "\n",
    "# Iterate over structures\n",
    "for pred, pred_o3 in zip(predictions, predictions_o3):\n",
    "\n",
    "    # Apply the O(3) transformation to the TensorMap\n",
    "    pred_transf = wig.transform_tensormap_o3(pred)\n",
    "\n",
    "    # Check for equivariance!\n",
    "    assert metatensor.equal_metadata(pred_transf, pred_o3)\n",
    "    assert metatensor.allclose(pred_transf, pred_o3)\n",
    "print(\"O(3) EQUIVARIANT!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize training objects: loaders, loss, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct dataloaders\n",
    "train_loader = data.RhoLoader(\n",
    "    efg_data,\n",
    "    idxs=train_idxs,\n",
    "    get_aux_data=False,\n",
    "    batch_size=ML_SETTINGS[\"loading\"][\"batch_size\"],\n",
    ")\n",
    "val_loader = data.RhoLoader(\n",
    "    efg_data,\n",
    "    idxs=val_idxs,\n",
    "    get_aux_data=False,\n",
    "    batch_size=ML_SETTINGS[\"loading\"][\"batch_size\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize loss fxn and optimizer (don't use a scheduler for now)\n",
    "loss_fn = loss.L2Loss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# A scheduler can be optionally used too\n",
    "scheduler = None\n",
    "# scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "#     optimizer, patience=10, factor=0.75\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run model training\n",
    "\n",
    "* We train the model by gradient descent, evaluating an L2 loss of the prediction\n",
    "  against the QM quantity.\n",
    "\n",
    "  * $ \\mathcal{L}_A^{\\text{L2}} = \\left( \\textbf{c}_A^{\\text{ML}} -\\textbf{c}_A^{\\text{EFG}} \\right) ^2 $\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rholearn import train\n",
    "from settings import ML_SETTINGS\n",
    "\n",
    "# Define a log file for writing losses at each epoch\n",
    "log_path = os.path.join(ML_DIR, \"training.log\")\n",
    "\n",
    "if ML_SETTINGS[\"training\"].get(\"restart_epoch\") is not None:\n",
    "    start_epoch = ML_SETTINGS[\"training\"][\"restart_epoch\"]\n",
    "    optimizer.load_state_dict(\n",
    "        torch.load(os.path.join(chkpt_dir, f\"optimizer_{start_epoch}.pt\"))\n",
    "    )\n",
    "    # scheduler.load_state_dict(\n",
    "    #     torch.load(os.path.join(chkpt_dir, f\"scheduler_{start_epoch}.pt\"))\n",
    "    # )\n",
    "    start_epoch += 1\n",
    "else:\n",
    "    start_epoch = 1\n",
    "\n",
    "# Print header of log file\n",
    "if scheduler is None:\n",
    "    io.log(log_path, \"# epoch train_L2_loss val_L2_loss time\")\n",
    "else:\n",
    "    io.log(log_path, \"# epoch train_L2_loss val_L2_loss time lr\")\n",
    "\n",
    "# Start training\n",
    "for epoch in range(start_epoch, ML_SETTINGS[\"training\"][\"n_epochs\"] + 1):\n",
    "    # Training step\n",
    "    t0 = time.time()\n",
    "    train_loss_epoch, val_loss_epoch = train.training_step(\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        model=model,\n",
    "        loss_fn=loss_fn,\n",
    "        optimizer=optimizer,\n",
    "        scheduler=scheduler,\n",
    "        check_args=True\n",
    "        if epoch == 1\n",
    "        else False,  # Switch off metadata checks after 1st epoch\n",
    "    )\n",
    "\n",
    "    dt = time.time() - t0\n",
    "    if scheduler is None:\n",
    "        io.log(\n",
    "            log_path,\n",
    "            f\"{epoch} {train_loss_epoch} {val_loss_epoch} {dt}\",\n",
    "        )\n",
    "    else:\n",
    "        io.log(\n",
    "            log_path,\n",
    "            f\"{epoch} {train_loss_epoch} {val_loss_epoch} {dt} {scheduler._last_lr[0]}\",\n",
    "        )\n",
    "    if epoch % ML_SETTINGS[\"training\"][\"save_interval\"] == 0:\n",
    "        torch.save(model, os.path.join(chkpt_dir, f\"model_{epoch}.pt\"))\n",
    "        torch.save(optimizer.state_dict(), os.path.join(chkpt_dir, f\"optimizer_{epoch}.pt\"))\n",
    "        # torch.save(scheduler.state_dict(), os.path.join(chkpt_dir, f\"scheduler_{epoch}.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training log\n",
    "losses = np.loadtxt(os.path.join(ML_DIR, \"training.log\"))\n",
    "\n",
    "# Plot the various errors\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Unpack data from each row\n",
    "if scheduler is None:\n",
    "    epochs, train_loss, val_loss, times = losses.T\n",
    "else:\n",
    "    epochs, train_loss, val_loss, times, lr = losses.T\n",
    "    ax.plot(epochs, lr, label=\"learning rate\")\n",
    "\n",
    "ax.plot(epochs, train_loss, label=\"L2 Loss (ML/RI), train\")\n",
    "ax.plot(epochs, val_loss, label=\"L2 Loss (ML/RI), val\")\n",
    "\n",
    "# Format the plot\n",
    "ax.set_xlabel(\"Epochs\")\n",
    "ax.set_ylabel(\"Loss per structure\")\n",
    "ax.set_xscale(\"log\")\n",
    "ax.set_yscale(\"log\")\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End-to-end prediction on the test set\n",
    "\n",
    "* Now let's make an end-to-end prediction (from xyz -> target EFG tensors)\n",
    "  on the test set, up until now unseen by the model.\n",
    "\n",
    "* We have already generated a descriptor for these frames, but for demonstrative\n",
    "  purposes we will input the ASE frames to the `predict()` method.\n",
    "\n",
    "* Returned is a list of predictions for each structure passed in `frames`.\n",
    "\n",
    "* First, load a pre-trained model from file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained model\n",
    "model = torch.load(os.path.join(chkpt_dir, \"model_10.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_frames = [all_frames[A] for A in test_idxs]\n",
    "efg_pred_from_xyz = model.predict(structure_idxs=test_idxs, frames=test_frames)\n",
    "efg_pred_from_xyz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* As mentioned above, we could also just call the forward method, passing the\n",
    "  TensorMap of descriptors instead of the ASE frames, but with gradient tracking\n",
    "  switched off:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a prediction on the pre-computed descriptors\n",
    "with torch.no_grad():\n",
    "\n",
    "    efg_pred_from_desc = []\n",
    "    for A in test_idxs:\n",
    "        _, in_test, out_test = efg_data[A]\n",
    "        efg_pred_from_desc.append(model(in_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the outputs give the same results\n",
    "[\n",
    "    metatensor.allclose(from_xyz, from_desc)\n",
    "    for from_xyz, from_desc in zip(efg_pred_from_xyz, efg_pred_from_desc)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rho",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
