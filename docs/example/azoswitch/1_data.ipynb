{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1: Molecules, Structural Representations, and Training Sets\n",
    "\n",
    "* Time to run the cells: ~ 1 minute\n",
    "\n",
    "First thing's first; set the absolute path of the ``rho_learn`` directory on\n",
    "your local machine, for instance:\n",
    "\n",
    "``RHOLEARN_DIR = \"/Users/joe.abbott/Documents/phd/code/qml/rho_learn/\"``\n",
    "\n",
    "We also need to import a ``rholearn`` module here for use later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rholearn.features import lambda_soap_vector\n",
    "\n",
    "RHOLEARN_DIR = \"/Users/joe.abbott/Documents/phd/code/qml/rho_learn/\"\n",
    "# RHOLEARN_DIR = \"/path/to/rho_learn/\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference Data\n",
    "\n",
    "### Electron Densities"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data used here is a 10-molecule subset of a largest dataset of azoswitch\n",
    "molecules used in the electron density learning of excited state properties. You\n",
    "can read the paper at __\"Learning the Exciton Properties of Azo-dyes\"__, J.\n",
    "Phys. Chem. Lett. 2021, 12, 25, 5957â€“5962. DOI:\n",
    "[10.1021/acs.jpclett.1c01425](https://doi.org/10.1021/acs.jpclett.1c01425). \n",
    "\n",
    "For the purposes of this workflow we are focussing on predicting only the\n",
    "ground-state electron density, but can easily be extended to first- and\n",
    "second-excited state hole and particle densities, for which there is reference\n",
    "QM data at the above source.\n",
    "\n",
    "All the data needed to run this proof-of-concept workflow is shipped in the\n",
    "GitHub repo, stored in the ``rho_learn/docs/example/azoswitch/data/`` directory.\n",
    "Inspect this directory. There is a file called ``mollist.dat`` containing the\n",
    "filenames of 10 structures, a subfolder ``xyz/`` containing these ``.xyz``\n",
    "files, a folder containing some QM-calculated Coulomb repulsion matrices, and\n",
    "the QM-calculated (i.e. reference) ground state electron density coefficients of\n",
    "the moelcules included in the training set.\n",
    "\n",
    "Both the Coulomb matrices and electron density are stored in equistore TensorMap\n",
    "format. Let's load and inspect the structure of the electron density."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import equistore.io\n",
    "\n",
    "data_dir = os.path.join(RHOLEARN_DIR, \"docs/example/azoswitch/data\")\n",
    "e_density = equistore.io.load(os.path.join(data_dir, \"gs_edensity.npz\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorMaps are main object users interact with when using equistore, storing in\n",
    "principle any kind of data useful in atomistic simulations and their associated\n",
    "metadata. \n",
    "\n",
    "A TensorMap is a collection of TensorBlocks, each of which is indexed by a key\n",
    "and contains atomistic data on a subset of a system of interest. In our case,\n",
    "the electron density TensorMap has blocks for each combination of spherical\n",
    "harmonic channel, $l$, and chemical species. \n",
    "\n",
    "Run the cell below. Notice how the $l$ values run from 0 -> 5 (inclusive) and\n",
    "the chemical species (or 'species_center') span values 1, 6, 7, 8, 16, for\n",
    "elements H, C, N, O, S respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e_density.keys"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's look at a specific block. TensorBlock contain three axis: the first is\n",
    "a single dimension, the samples. The last is also a single axis, the properties.\n",
    "And all other intermediate dimensions are the components. In general, samples\n",
    "are used to describe what we are representing, i.e. atomic environments in a\n",
    "given structure, and properties are used to describe how we are representing it.\n",
    "\n",
    "In this example, a set of coefficients for the expansion of the electron density\n",
    "on a set of basis functions are given as the learning targets and therefore the\n",
    "data that appears in the TensorMaps. For a given structure, $A$\n",
    "\n",
    "$ \\rho_A (x) = \\sum_{inlm} c^i_{nlm} \\phi_{nlm}(x - r_i)$\n",
    "\n",
    "where $c^i_{nlm}$ are the expansion coefficients, $\\phi$ the basis functions.\n",
    "$i$ is an atomic index for the atoms in a molecule, $n$ the radial index, and\n",
    "$l$ and $m$ the spherical harmonics indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e_density.block(0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The samples contain 'structure' (i.e. $A$ in the equation above) and 'center' ($i$) indices. The\n",
    "components contains 'spherical_harmonics_m' ($m$) indices, and the properties\n",
    "contains 'n' (i.e. radial channel $n$) indices. Remember from above that the\n",
    "keys of the TensorMap store the sparse indices for 'spherical_harmonics_l' (i.e.\n",
    "$l$) as well as 'species_center' - the latter because often different basis\n",
    "functions are used for different chemical species."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coulomb Matrices\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each structure in the training set, a Coulomb repulsion metric can be\n",
    "calculated betweeen pairs of basis functions indexed by ${n_1l_1m_1}$ and\n",
    "${n_2l_2m_2}$. The provided Coulomb matrices contains these repulsions, measured\n",
    "in Hartree units of energy.\n",
    "\n",
    "These metrics will be used to define a physically-inspired loss\n",
    "function used in model training (in the second example notebook).\n",
    "\n",
    "Because these matrices are quite large, they had to be split up in order to be\n",
    "stored on GitHub. Run the cell below to recombine them, and observe the keys.\n",
    "Notice how each block is indexed by a pair of $l$ values and chemical species now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azoswitch_utils import recombine_coulomb_matrices\n",
    "\n",
    "coulomb_matrices = recombine_coulomb_matrices(data_dir, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just view the first 10 keys (as there are > 600 of them)\n",
    "coulomb_matrices.keys[:10]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now inspect a single block. Samples monitor a single structure index, and the 2\n",
    "atomic center indices the basis functions belong to. Note only a single\n",
    "structure index is present here because it doesn't make sense to calculate\n",
    "repulsion between atoms in different structures. The components index the $m$\n",
    "value for the 2 basis functions, and properties indexes the $n$ values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coulomb_matrices.block(0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structural Descriptors\n",
    "\n",
    "Now we can build a $\\lambda$-SOAP structural representation of the input data,\n",
    "using only the ``.xyz`` files. First, we load the filenames from\n",
    "``mollist.dat``. The order of the filenames as listed dictates their structure\n",
    "index, of which all will run from 0 -> 9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the filenames from mollist.dat \n",
    "with open(os.path.join(data_dir, \"molecule_list.dat\"), \"r\") as molecule_list:\n",
    "    xyz_files = molecule_list.read().splitlines()\n",
    "xyz_files"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each of these ``.xyz`` files can be read into an ASE object, or 'frame', and\n",
    "these frames can be visualized with chemiscope. Use the slider to have a look at\n",
    "each molecule in turn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ase.io\n",
    "import chemiscope\n",
    "\n",
    "# Read into ASE frames\n",
    "frames = [ase.io.read(os.path.join(data_dir, \"xyz\", f)) for f in xyz_files]\n",
    "\n",
    "# Display molecules with chemiscope\n",
    "cs = chemiscope.show(frames, mode=\"structure\")\n",
    "display(cs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the unique chemical species present in the dataset\n",
    "unique_species = list(set([specie for f in frames for specie in f.get_atomic_numbers()]))\n",
    "unique_species"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rascaline hypers\n",
    "rascal_hypers = {\n",
    "    \"cutoff\": 5.0,  # Angstrom\n",
    "    \"max_radial\": 6,  # Exclusive\n",
    "    \"max_angular\": 5,  # Inclusive\n",
    "    \"atomic_gaussian_width\": 0.2,\n",
    "    \"radial_basis\": {\"Gto\": {}},\n",
    "    \"cutoff_function\": {\"ShiftedCosine\": {\"width\": 0.5}},\n",
    "    \"center_atom_weight\": 1.0,\n",
    "}\n",
    "\n",
    "# Compute lambda-SOAP: uses rascaline to compute a SphericalExpansion\n",
    "# runtime approx 15 seconds\n",
    "input = lambda_soap_vector(\n",
    "    frames, rascal_hypers, save_dir=data_dir, neighbor_species=unique_species\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean the $\\lambda$-SOAP descriptor"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we've generated our descriptor let's load the it (our 'input') and the\n",
    "ground-state electron density ('output') TensorMaps from file using the\n",
    "``equistore.io.load`` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from equistore import io\n",
    "\n",
    "input = io.load(os.path.join(data_dir, \"lambda_soap.npz\"))\n",
    "output = io.load(os.path.join(data_dir, \"gs_edensity.npz\"))\n",
    "print(\"Lambda SOAP key names: \", input.keys.names, \"\\nNumber of blocks: \", len(input.keys))\n",
    "print(\"GS e-density key names: \", output.keys.names, \"\\nNumber of blocks: \", len(output.keys))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, higher-order correlations are described by combining descriptors of\n",
    "lower order. The `SphericalExpansion` calculator in `rascaline` computes an\n",
    "atom-centered density correlation for each structure in the input molecular\n",
    "dataset. These correspond to $\\nu = 1$ order features. SOAP-based\n",
    "representations are by definition $\\nu = 2$ order features as they measure\n",
    "pairwise atom density correlations. As such, using Clebsch-Gordan iterations,\n",
    "they are generated by combining $\\nu=1$ features with themselves. $\\lambda$-SOAP\n",
    "features, by extension, are a given by projecting (and rotationally averaging)\n",
    "$\\nu = 2$ SOAP features on a hierarchy of spherical harmonics which behave\n",
    "equivariantly under rotations.\n",
    "\n",
    "In principle, one could build a descriptor that is comprised of different order\n",
    "of $\\nu$. This is why, in the ``input`` TensorMap, there exists a key monitoring\n",
    "``'order_nu'``. In our case, using $\\lambda$-SOAP, the order of all blocks in\n",
    "the TensorMap is by definition $\\nu = 2$. We can therefore drop this key, as it\n",
    "is redundant.\n",
    "\n",
    "Furthermore, when the $\\nu = 2$ descriptor is generated by combining $\\nu = 1$\n",
    "features, inversion symmetry is accounted for such that the resulting\n",
    "descriptor transforms covariantly under both proper and improper rotations,\n",
    "belonging to the SO(3) and O(3) symmetry groups, respectively. As we are\n",
    "interested in modelling the electron density, which transforms rigidly and\n",
    "covariantly with the molecule to which it belongs, we only need to consider\n",
    "descriptors that are equivariant under actions of the SO(3) (proper) rotations\n",
    "group. We can therefore drop all blocks in the ``input`` $\\lambda$-SOAP\n",
    "TensorMap that have odd parity, i.e. $\\sigma = -1$, indicated by the key\n",
    "``'inversion_sigma'``.\n",
    "\n",
    "A final bit of cleaning needs to be performed. When generating the reference\n",
    "data, the electron denisty coefficients (now stored in the ``output`` TensorMap)\n",
    "for the Hydrogen (``'species_center' = 1``) basis functions were only calculated\n",
    "up to angular momentum channel $0 \\leq \\lambda \\leq 4$, whereas the other atoms\n",
    "present in the molecular dataset (C=6, N=7, O=8, S=16) were calcualated up to $0\n",
    "\\leq \\lambda \\leq 5$. Therefore, in order to map inputs to outputs, we need to\n",
    "drop the block in the ``input`` TensorMap corresponding to the key\n",
    "``('spherical_harmonics_l', 'species_center') = (5, 1)``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azoswitch_utils import clean_azoswitch_lambda_soap\n",
    "\n",
    "input_cleaned = clean_azoswitch_lambda_soap(input)\n",
    "input_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the cleaned lambda-SOAP descriptor\n",
    "io.save(os.path.join(data_dir, \"lambda_soap_cleaned.npz\"), input_cleaned)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's perform some checks on the input and output TensorMaps. In order to do\n",
    "supervised ML in PyTorch, it is required that all dimensions of input and output\n",
    "tensors, except the last, are exactly equivalent, both in terms of size and the\n",
    "ordering of the data they correspond to. The final dimension, i.e. the\n",
    "properties/features (in equistore/torch terminology) need not match as learning\n",
    "the mapping of input properties onto output properties is the goal of supervised\n",
    "ML.\n",
    "\n",
    "As we are using ``equistore`` and storing our atomistic ML data in ``TensorMap``\n",
    "objects, we can use the ``Labels`` metadata to check our data before training.\n",
    "PyTorch doesn't track metadata, so we need to be certain that:\n",
    "\n",
    "* a) input/output keys are equivalent, but order doesn't matter as each\n",
    "  ``TensorBlock`` indexed by these keys will be a separate input to its own\n",
    "  model.\n",
    "* b) the input/output samples of each block indexed by a given key are *exactly*\n",
    "  equivalent, in size, values, and order.\n",
    "* c) the input/output components of each block indexed by a given key are *exactly*\n",
    "  equivalent, in size, values, and order.\n",
    "\n",
    "We can perform these checks by first checking the keys ``Labels`` objects of the\n",
    "input/output ``TensorMaps``, then iterating over these keys, extracting the\n",
    "input/output ``TensorBlocks`` and comparing their samples and components\n",
    "``Labels``. The code cell below does this - if everything is ok no error should\n",
    "be raised."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rholearn import utils\n",
    "\n",
    "utils.equal_metadata(input_cleaned, output)\n",
    "print(\"input and output TensorMaps checked - consistent metadata, checks passed\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform data partitioning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input and output data has been defined, cleaned, and checked for metadata\n",
    "consistency. Now we need to perform a train-test-validation split and, in order\n",
    "to perform a learning exercise, create some subsets of the training data.\n",
    "\n",
    "We will define a dict of settings that we will provide to the function that will\n",
    "execute this partitioning; ``partition_data``."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``settings`` is a nested dict. For each of the nested dicts indexed by the\n",
    "following keys:\n",
    "\n",
    "* ``\"io\"``: stores the paths of the input (i.e. lambda-SOAP) and output (i.e.\n",
    "ground-state electron density) TensorMaps and the directory where the\n",
    "partitioned data will be stored.\n",
    "\n",
    "* ``\"numpy\"``: stores the random seed used to control reproducible shuffling of\n",
    "  the structure indices when the data is partitioned.\n",
    "\n",
    "* ``\"train_test_split\"``: stores the settings for how to perform the train-test\n",
    "  split. In this case, we want to split our TensorMaps along the samples axis,\n",
    "  splitting by structure. As we want a train-test-validation split, we specify\n",
    "  ``n_groups: 3``, and indicate the absolute group sizes of 7, 2, and 1 of the\n",
    "  full data in the train, test, and validation TensorMaps, respectively. We\n",
    "  could also pass ``\"group_sizes_rel\": [0.7, 0.2, 0.1]`` here with the same\n",
    "  outcome. If we just wanted a train-test split, with no validation set, we\n",
    "  would pass ``\"n_groups\": 2`` and ``\"group_sizes_rel\": [x, y]``, where ``x + y\n",
    "  <= 1``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "settings = {\n",
    "    \"io\": {\n",
    "        \"input\": os.path.join(data_dir, \"lambda_soap_cleaned.npz\"),\n",
    "        \"output\": os.path.join(data_dir, \"gs_edensity.npz\"),\n",
    "        \"data_dir\": os.path.join(data_dir, \"partitions\"),\n",
    "    },\n",
    "    \"numpy\": {\n",
    "        \"random_seed\": 10,\n",
    "    },\n",
    "    # Perform a train-test-validation split, with 7, 2, 1 molecules in each\n",
    "    \"train_test_split\": {\n",
    "        \"axis\": \"samples\",\n",
    "        \"names\": [\"structure\"],\n",
    "        \"n_groups\": 3,\n",
    "        \"group_sizes_abs\": [7, 2, 1],\n",
    "        # \"group_sizes_rel\": [0.7, 0.2, 0.1],  # we could also use this argument\n",
    "    },\n",
    "    # Prepare training data partitions for 2 exercises, each with 3 subsets\n",
    "    \"data_partitions\": {\n",
    "        \"n_exercises\": 2,\n",
    "        \"n_subsets\": 3,\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rholearn.pretraining import partition_data\n",
    "\n",
    "# Runtime approx 20 seconds\n",
    "partition_data(settings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's inpsect how the data was partitioned. In the \"partitions\" folder, a numpy\n",
    "array called \"subset_sizes.npy\" was saved. This stores the sizes (i.e. number of\n",
    "training structures) of each of the training subsets.\n",
    "\n",
    "You can see that, of the 7 structures that we designated as the the total\n",
    "training set, 2, 4, and 6 structures were assigned to each of the training\n",
    "subsets to be used in a learning exercise. While these seem evenly spaced in\n",
    "linear space, in practice the ``partition_data`` function ensures that the sizes\n",
    "of training subsets are evenly spaced along a *log* (base ``e``) scale, to the\n",
    "nearest integer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.load(os.path.join(data_dir, \"partitions\", \"subset_sizes_train.npy\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each of the 3 learning exercises, the training structures indices were\n",
    "shuffled before subsets were created. Let's check this by printing the ordered\n",
    "structure indices from which the training set was partitioned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"train structure idxs:\")\n",
    "print(\"exercise 0: \", np.load(os.path.join(data_dir, \"partitions\", \"exercise_0\", \"structure_idxs_train.npy\")))\n",
    "print(\"exercise 1: \", np.load(os.path.join(data_dir, \"partitions\", \"exercise_1\", \"structure_idxs_train.npy\")))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the 7 structure indices are equivalent across all the lists, the order is\n",
    "different. That means, for instance, when the first subset of size 2 is created,\n",
    "structures 2 and 8 will be present in the training set for exercise 0,\n",
    "structures 0 and 6 for exercise 1, and structures 3 and 2 for exercise 2.\n",
    "\n",
    "Just as a sanity check, let's print the test and validation structure indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"test structure idxs: \", np.load(os.path.join(data_dir, \"partitions\", \"structure_idxs_test.npy\")))\n",
    "print(\"val structure idxs: \", np.load(os.path.join(data_dir, \"partitions\", \"structure_idxs_val.npy\")))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's address the warnings outputted when calling the ``partition_data``\n",
    "function above. \n",
    "\n",
    "The samples dimension of each TensorBlock in the cleaned $\\lambda$-SOAP\n",
    "representation contains indices for multiple structures. Let's look at the block\n",
    "indexed by key ``(0, 8)`` as an example, as this is one of the blocks we were\n",
    "warned about above when the ``partition_data`` function was called, for exercise\n",
    "0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rholearn.utils import key_tuple_to_npvoid\n",
    "\n",
    "key = key_tuple_to_npvoid((0, 8), names=[\"spherical_harmonics_l\", \"species_center\"])\n",
    "\n",
    "print(\n",
    "    f\"samples of block indexed by key {key} in the cleaned lsoap TensorMap:\\n\",\n",
    "    input_cleaned[key].samples.names, \": \", input_cleaned[key].samples, \n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The block indexed by key ``(0, 8)`` contains only structures 3, 5, 6, and 7.\n",
    "\n",
    "When the data is partitioned (i.e. in the train-test-validation split) based on\n",
    "structure index, some blocks may end up being empty. It is important that we\n",
    "still hold on to this empty block, indexed by its appropriate key, so that we\n",
    "can learn the relationship between input and output. In the case of an empty\n",
    "block, there will be nothing to learn and the weights matrix will be of size 0.\n",
    "However, using a larger training set or a different way of performing the\n",
    "train-test-validation split, the block may not be empty.\n",
    "\n",
    "Remember from above that the ordered sample indices from with the\n",
    "train-test-validation split for exercise 0 is performed is ``[(2,) (8,) (0,)\n",
    "(3,) (5,) (6,) (1,)]``. As exercise 0, subset 0 only contains 1 structure in the\n",
    "training subset, and the test and validation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the input train TensorMap from exercise 0, subset 0\n",
    "in_train = io.load(\n",
    "    os.path.join(\"data\", \"partitions\", \"exercise_0\", \"subset_0\", \"in_train.npz\")\n",
    ")\n",
    "in_train[key].samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the input test TensorMap\n",
    "in_test = io.load(\n",
    "    os.path.join(\"data\", \"partitions\", \"in_test.npz\")\n",
    ")\n",
    "in_test[key].samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the input validation TensorMap\n",
    "in_val = io.load(\n",
    "    os.path.join(\"data\", \"partitions\", \"in_val.npz\")\n",
    ")\n",
    "in_val[key].samples"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, run the cell below. The details of what this does isn't scientifically\n",
    "or computationally important - it just combines 6 TensorMaps into a single one,\n",
    "and needs to be done due to GitHub's file size limits for public repositories.\n",
    "We will go over the meaning and use of these coulomb matrices in the next\n",
    "notebook."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the data has been partitioned, we are ready to move on to setting up\n",
    "the training simulations."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rho",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "576c71a426691bc103e620abf31b98f592c88b3903fdf6bf41ae71c4b8043fe6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
