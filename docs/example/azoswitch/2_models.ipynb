{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2: ``PyTorch`` Model Training\n",
    "\n",
    "* Time to run the cells: \n",
    "    * ~ 5 mins for 5 epochs, 1 exercise, 1 subset\n",
    "\n",
    "Again, inspect `settings.py`. It is mostly the `ML_SETTINGS` that is relevant to\n",
    "this notebook, though teh `RASCAL_HYPERS` and some options from `DATA_SETTINGS`\n",
    "will be used too. `ML_SETTINGS` contains settings and hyperparameters for the\n",
    "training procedure.\n",
    "\n",
    "Create a directory at the relative path\n",
    "`/rho_learn/docs/example/azoswitch/runs/`. This is where simulations will be run\n",
    "and results stored.\n",
    "\n",
    "In this notebook we will see how to construct custom torch models that integrate\n",
    "with equistore, both linear and nonlinear. Also, we will construct a custom\n",
    "torch loss function `CoulombLoss`, again interfaced with equistore. After\n",
    "checking the equivariance condition of our structural representation and model,\n",
    "we will do some model training."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define training settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pprint\n",
    "import ase.io\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "import chemiscope\n",
    "import equistore\n",
    "\n",
    "import rholearn\n",
    "from rholearn import io, features, loss, models, pretraining, spherical, training, utils\n",
    "from settings import RASCAL_HYPERS, DATA_SETTINGS, ML_SETTINGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create simulation run directory and save simulation\n",
    "io.check_or_create_dir(ML_SETTINGS[\"run_dir\"])\n",
    "with open(os.path.join(ML_SETTINGS[\"run_dir\"], \"ml_settings.txt\"), \"a+\") as f:\n",
    "    f.write(f\"ML Settings:\\n{pprint.pformat(ML_SETTINGS)}\\n\\n\")\n",
    "\n",
    "# IMPORTANT! - set the torch default dtype\n",
    "torch.set_default_dtype(ML_SETTINGS[\"torch\"][\"dtype\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_train = io.load_tensormap_to_torch(\n",
    "    os.path.join(DATA_SETTINGS[\"data_dir\"], \"in_train.npz\"), **ML_SETTINGS[\"torch\"]\n",
    ")\n",
    "out_train = io.load_tensormap_to_torch(\n",
    "    os.path.join(DATA_SETTINGS[\"data_dir\"], \"out_train.npz\"), **ML_SETTINGS[\"torch\"]\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the ``EquiModelGlobal?`` command, we can see the arguments the global\n",
    "model class takes upon initiliazation. The first 4 args are required to build\n",
    "linear model, specifying the model type, in this case \"linear\", the keys of the\n",
    "TensorMap that the model is built for, and the properties/features labels for\n",
    "each of the blocks. These are required to map a number of input features to\n",
    "output features for each block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models.EquiModelGlobal?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can build a linear model and, using the ``model.models`` attribute, print\n",
    "out 2 of the local models that exist for the first 2 blocks, indexed by keys\n",
    "``(0, 1)`` and ``(1, 1)``. You can see that the number of in a nf out features\n",
    "of the linear layers are different, and that the model for the invariant block\n",
    "uses a bias, whereas the one for the covariant ($\\lambda = 1$) block doesn't."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_model = models.EquiModelGlobal(\n",
    "    \"linear\",\n",
    "    keys=in_train.keys,\n",
    "    in_feature_labels={key: block.properties for key, block in in_train},\n",
    "    out_feature_labels={key: block.properties for key, block in out_train},\n",
    ")\n",
    "list(linear_model.models.items())[:2]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nonlinear Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![nonlinear_model_forward](../figures/nonlinear_architecture.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to a linear model, a nonlinear model is also implemented in\n",
    "``rholearn``. As in the global linear model, the global nonlinear model is a\n",
    "collection of individual local models applied to each block. Each local model\n",
    "makes a prediction on a given equivariant (i.e. either invariant or covariant)\n",
    "block of the input TensorMap, indexed by a key.\n",
    "\n",
    "The nonlinear local model architecture is shown in the figure below. Predictions\n",
    "are made on an equivariant (blue) block, using its associated invariant to act\n",
    "as a nonlinear multiplier. For instance, the equivariant block for Carbon,\n",
    "$\\lambda = 3$ is passed to the forward method along with the invariant ($\\lambda\n",
    "= 0$) block for Carbon. The equivariant is passed through a linear model and the\n",
    "invariant through a neural network of arbitrary architecture. Then, element-wise\n",
    "multiplication of the two blocks is performed before passing the result through\n",
    "a final linear output layer to get the electron density prediction.\n",
    "\n",
    "Applying nonlinear transformations to only the invariant ensure that\n",
    "equivariance isn't broken. Upon element-wise multiplication, the component\n",
    "vectors of the equivariant block are multiplied by a vector of constant size\n",
    "(thanks to the h-stacking of the invariant, botoom-right of the figure), thus\n",
    "retaining equivariance.\n",
    "\n",
    "Performing such operations within a single custom PyTorch ``forward()`` method\n",
    "allows the operations to be tracked, and therefore the gradientsto be\n",
    "calculated. This means that model training involves optimization of the weights\n",
    "of all weights and biases seen below - in the linear input layer applied to the\n",
    "equivariant, all layers of the neural network applied to the invariant, and the\n",
    "linear output layer applied to the mixed block.\n",
    "\n",
    "Let's build a global model by hand and look at just 2 of the individual local\n",
    "models. As the keys of the TensorMap are ``('spherical_harmonics_l',\n",
    "'species_center')``, there exists one invariant block for each chemical species\n",
    "(i.e. ``species_center``: H (1), C (6), N (7), O (8), S (16)). As explained\n",
    "above, these invariants are used as nonlinear multiplier to the equivariant\n",
    "blocks, so the size of their features need to be passed to the model in the\n",
    "``in_invariant_features`` during initialization. In the figure above, these\n",
    "values correspond to $q_{\\text{in}}^{\\text{inv}}$ (bottom left of the figure).\n",
    "\n",
    "The model architecture can also be controlled. The ``activation_fn`` to use in\n",
    "alternating layers between linear layers can be specified, choosing from \"Tanh\",\n",
    "\"GELU\", or \"SiLU\". The length of the list arg ``hidden_layer_widths`` controls\n",
    "the number of pairs of hidden linear layers after the first input layer appear\n",
    "in the neural network. For $n$ hidden linear layers, there will be $n-1$\n",
    "nonlinear activation functions. The values in the list control the width\n",
    "of these layers.\n",
    "\n",
    "In the cell below, we are initializing the neural network to have 3 hidden\n",
    "linear layers of widths 8, 8, and 16, with 2 nonlinear activation functions\n",
    "sandwiched between them. Run the cell and look for the ``(invariant_nn)``\n",
    "``Sequential`` layer, containing alternating Linear and SiLU functions. Note\n",
    "also how for the ``EquiLocalModel`` for key ``('spherical_harmonics_l',\n",
    "'species_center')`` == ``(0, 1)`` a bias is used on the input and output linear\n",
    "layers, but for local model ``(1, 1)`` a bias isn't used. This is because for\n",
    "covariant blocks ($\\lambda > 0$) covariance is broken by applying a bias, but\n",
    "for invariants it isn't. A bias is applied **in the neural network layers** for\n",
    "local models, however, as only the invariant blocks supporting the forward\n",
    "method are passed through the NN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(in_train.keys[\"species_center\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_invariant_features_by_species = {\n",
    "    specie: len(in_train.block(spherical_harmonics_l=0, species_center=specie).properties)\n",
    "    for specie in np.unique(in_train.keys[\"species_center\"])\n",
    "}\n",
    "in_invariant_features = {\n",
    "    key: in_invariant_features_by_species[key[in_train.keys.names.index(\"species_center\")]]\n",
    "    for key in in_train.keys\n",
    "}\n",
    "nonlinear_model = models.EquiModelGlobal(\n",
    "    \"nonlinear\",\n",
    "    keys=in_train.keys,\n",
    "    in_feature_labels={key: block.properties for key, block in in_train},\n",
    "    out_feature_labels={key: block.properties for key, block in out_train},\n",
    "    in_invariant_features=in_invariant_features,\n",
    "    hidden_layer_widths=[8, 8, 16],\n",
    "    activation_fn=\"SiLU\",\n",
    ")\n",
    "\n",
    "list(nonlinear_model.models.items())[:2]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking the Equivariance Condition\n",
    "\n",
    "Before going any further, it is important that we check that our structural\n",
    "representations and machine learning models are equivariant.\n",
    "\n",
    "In order for a structural representation to be equivariant, the irreducible\n",
    "spherical components that comprise it must transform like spherical harmonics.\n",
    "\n",
    "Spherical harmonics have known behviour under rotations, such that any spherical\n",
    "component $\\mu$ of order $\\lambda$ transforms into new component $\\mu'$ according to the\n",
    "action of the Wigner D-Matrix of order $\\lambda$, $D^{\\lambda}_{\\mu\\mu'}$, which\n",
    "is constructed for a given arbitrary rotation matrix in Cartesian space.\n",
    "\n",
    "In order to check that our $\\lambda$-SOAP feature vector is equivariant, we can\n",
    "run the following test:\n",
    "\n",
    "1. Take an ``.xyz`` file of a given structure in the training set\n",
    "2. Build an ASE frame of this structure\n",
    "3. Generate a random cartesian rotation matrix\n",
    "4. Rotate the xyz coordinates of the structure according to the random rotation\n",
    "   matrix, storing the rotated structure in a new ASE frame\n",
    "5. Generate a $\\lambda$-SOAP representation for the unrotated and rotated\n",
    "   structures\n",
    "6. For each $\\lambda$ channel of the representation of the unrotated structure,\n",
    "   extract a selection of $(2\\lambda + 1)$-sized irreducible spherical component (ISC)\n",
    "   vectors.\n",
    "7. Rotate each of these ISC vectors using the Wigner D-Matrix constucted\n",
    "   according to cartesian rotation matrix defined in step 3\n",
    "8. Check for exact equivalence between the rotated ISC vectors of the unrotated\n",
    "   structure and the corresponding ISC vectors of the rotated structure.\n",
    "\n",
    "First, let's load a random structure from the training set, construct rotated\n",
    "and unrotated ASE frames, and visualize them using chemiscope. By moving the\n",
    "slider, you can see that they are rigidly rotated versions of eachother."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick a random index between 0 and 10 as the test structure\n",
    "structure_idx = np.random.randint(0, 10)\n",
    "\n",
    "# Load the xyz file corresponding to this structure\n",
    "with open(os.path.join(DATA_SETTINGS[\"data_dir\"], \"molecule_list.dat\"), \"r\") as molecule_list:\n",
    "    structure_xyz = molecule_list.read().splitlines()[structure_idx]\n",
    "\n",
    "# Read xyz file into an ASE frame\n",
    "unrotated = ase.io.read(os.path.join(DATA_SETTINGS[\"data_dir\"], \"xyz\", structure_xyz))\n",
    "\n",
    "# Generated a randomly rotated copy of the ASE frame\n",
    "rotated, (alpha, beta, gamma) = spherical.rotate_ase_frame(unrotated)\n",
    "\n",
    "# Visualize the frames using chemiscope\n",
    "chemiscope.show([unrotated, rotated], mode=\"structure\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now generate $\\lambda$-SOAP representations of the rotated and unrotated\n",
    "structures and pass these through a function that checks for equivariance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate lambda-SOAP descriptors. We want to do this individually for the\n",
    "# rotated and unrotated structures to keep the structure indices consistent\n",
    "lsoap_unrotated = features.lambda_soap_vector(\n",
    "    [unrotated], RASCAL_HYPERS, even_parity_only=True, neighbor_species=[1, 6, 7, 8, 16]\n",
    ")\n",
    "lsoap_rotated = features.lambda_soap_vector(\n",
    "    [rotated], RASCAL_HYPERS, even_parity_only=True, neighbor_species=[1, 6, 7, 8, 16]\n",
    ")\n",
    "\n",
    "# Convert tensors to torch\n",
    "lsoap_unrotated = utils.tensor_to_torch(lsoap_unrotated, **ML_SETTINGS[\"torch\"])\n",
    "lsoap_rotated = utils.tensor_to_torch(lsoap_rotated, **ML_SETTINGS[\"torch\"])\n",
    "\n",
    "# Perform the equivariance check - this returns a bool. Use the rotation matrix\n",
    "# previosuly defined to produce the lambda-SOAP of the rotated and unrotated\n",
    "# structures\n",
    "is_equi = spherical.check_equivariance(\n",
    "    lsoap_unrotated,\n",
    "    lsoap_rotated,\n",
    "    lmax=RASCAL_HYPERS[\"max_angular\"],\n",
    "    alpha=alpha,\n",
    "    beta=beta,\n",
    "    gamma=gamma,\n",
    "    n_checks_per_block=5000,\n",
    ")\n",
    "if is_equi:\n",
    "    print(\"Our lambda-SOAP is equivariant!\")\n",
    "else:\n",
    "    print(\"Oops, our lambda-SOAP is not equivariant...\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to check that our model is equivariant. When passing input tensors\n",
    "through a model, certain tensor operations contained within the model\n",
    "architecture (no matter how simple or complex) can break equivariance. It would\n",
    "be a shame to go through the effort of generating an equivariant representation\n",
    "if something as simple as applying a bias in our linear model breaks\n",
    "equivariance!\n",
    "\n",
    "In order for our model to be equivariant, it must satisfy the equivariant\n",
    "condition: \n",
    "\n",
    "$\\hat{R} y(A) = y(\\hat{R} A)$\n",
    "\n",
    "where $\\hat{R}$ is an arbitrary rotation matrix of the SO(3) group, $A$ is a\n",
    "trial structural representation, and $y(A)$ is the output property (i.e.\n",
    "electron density) of the model, predicting on structure $A$.\n",
    "\n",
    "In plainer words, the condition states (under the assumption that our structural\n",
    "representation is equivariant) that our model is equivariant if the property\n",
    "(electron density) we predict on an unrotated structure, subsequently rotated is\n",
    "**exactly equivalent** to the property we get if predict on the rotated\n",
    "structure, if the rotation matrix used in both is equal.\n",
    "\n",
    "We can therefore construct a test in the following way, using the $\\lambda$-SOAP\n",
    "representations for the rotated and unrotated structures generated above:\n",
    "\n",
    "1. Pass the $\\lambda$-SOAP representation of the unrotated structure through the\n",
    "   ML model to generate a predicted electron density.\n",
    "2. Do the same for the $\\lambda$-SOAP representation of the rotated structure.\n",
    "3. For each $\\lambda$ channel of the electron density of the unrotated\n",
    "   structure, extract a selection of $(2\\lambda + 1)$-sized irreducible\n",
    "   spherical component (ISC) vectors.\n",
    "4. Rotate each of these ISC vectors using the Wigner D-Matrix constucted\n",
    "   according to cartesian rotation matrix defined above\n",
    "5. Check for exact equivalence between the rotated ISC vectors of the unrotated\n",
    "   electron density and the corresponding ISC vectors of the rotated electron\n",
    "   density.\n",
    "\n",
    "First, we need to do some cleaning (similar to as we did in the first notebook)\n",
    "and padding of the TensorMaps to make the dimensions consistent - don't worry\n",
    "too much about this. The, predict the electron density for the rotated and\n",
    "unrotated structures, using the linear and nonlinear models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from equistore import Labels\n",
    "\n",
    "# There may be some blocks in the training set that are not present in the\n",
    "# validation structure; pad the validation structure with empty blocks\n",
    "lsoap_rotated = utils.pad_with_empty_blocks(lsoap_rotated, in_train)\n",
    "lsoap_unrotated = utils.pad_with_empty_blocks(lsoap_unrotated, in_train)\n",
    "\n",
    "# Pass the rotated and unrotated structures through the linear model\n",
    "out_pred_linear_unrot = linear_model(lsoap_unrotated)\n",
    "out_pred_linear_rot = linear_model(lsoap_rotated)\n",
    "\n",
    "# Pass the rotated and unrotated structures through the nonlinear model\n",
    "out_pred_nonlin_unrot = nonlinear_model(lsoap_unrotated)\n",
    "out_pred_nonlin_rot = nonlinear_model(lsoap_rotated)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform the equivariance check on both the linear and nonlinear model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform the equivariance check on the linear and nonlinear models\n",
    "for i, (unrot, rot) in enumerate(\n",
    "    [\n",
    "        (out_pred_linear_unrot, out_pred_linear_rot),\n",
    "        (out_pred_nonlin_unrot, out_pred_nonlin_rot),\n",
    "    ]\n",
    "):\n",
    "    is_equi = spherical.check_equivariance(\n",
    "        unrot,\n",
    "        rot,\n",
    "        lmax=RASCAL_HYPERS[\"max_angular\"],\n",
    "        alpha=alpha,\n",
    "        beta=beta,\n",
    "        gamma=gamma,\n",
    "        n_checks_per_block=None,  # None checks on all ISC vectors\n",
    "    )\n",
    "    if is_equi:\n",
    "        print(f\"Our {['linear', 'nonlinear'][i]} model is equivariant!\")\n",
    "    else:\n",
    "        print(\n",
    "            f\"Oops, something in our {['linear', 'nonlinear'][i]} model is breaking equivariance...\"\n",
    "        )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good stuff! Now that we've performed those checks, let's do some model training."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct ``torch`` objects used in training"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prior to model training, it can be beneficial to pre-initialize and store some\n",
    "of the torch objects that will be used, especially if some pre-processing\n",
    "calculations can be performed.\n",
    "\n",
    "### ML Model\n",
    "\n",
    "PyTorch model training is based on torch tensor operations. In order to\n",
    "interface with equistore and allow tracking of all the metadata useful in\n",
    "atomistic ML, custom model classes have been built in ``rholearn`` to allow\n",
    "predictions to be made on TensorMaps as a whole. The class ``EquiModelGlobal``\n",
    "stores individual models for each input/output block in the data, as seen above.\n",
    "\n",
    "### Loss Function\n",
    "\n",
    "A function that calculates a difference metric between a predicted (or 'input')\n",
    "and reference (or 'target') tensor. At the torch-equistore interface this is a\n",
    "custom torch module that calculates this difference on the TensorMap level.\n",
    "\n",
    "Currently implemented are the ``MSELoss`` (otherwise called L2 loss) and the\n",
    "``CoulombLoss`` metrics. \n",
    "\n",
    "As detailed in the paper [__\"Impact of quantum-chemical metrics on the machine\n",
    "learning prediction of electron\n",
    "density\"__](https://aip.scitation.org/doi/10.1063/5.0055393), use of a\n",
    "physically-inspired loss function such as the Coulomb repulsion metric can lead\n",
    "to better model performance when predicting properties derived from the electron\n",
    "density.\n",
    "\n",
    "While the MSE loss needs no-preprocessing or initialization, when using a\n",
    "CoulombLoss the speed of simulations can benefit greatly from pre-processing.\n",
    "The Coulomb matrices can be distilled to contain data only for structures present\n",
    "in the training and test data sets they will evaluate the loss for. Especially\n",
    "as multiple training exercises on various subsets will be performed,\n",
    "initializing and storing both a train loss and test loss object for each\n",
    "training subset, to be loaded at runtime, can speed things up.\n",
    "\n",
    "provided can be heavly pre-processed prior to use at\n",
    "runtime of model training. The custom torch module ``CoulombLoss`` pre-processes\n",
    "\n",
    "### Optimizer\n",
    "\n",
    "An algorithm, such as stochastic gradient descent (SGD) or LBFGS that performs\n",
    "gradient descent on the loss landscape with respect to the model parameters.\n",
    "This is generally a lightweight object that doesn't need to be pre-constructed."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing a CoulombLoss object\n",
    "\n",
    "Let's build train and test CoulombLoss objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the output train data from the exercise 0 subset 0 training directory\n",
    "out_train = rholearn.io.load_tensormap_to_torch(\n",
    "    os.path.join(DATA_SETTINGS[\"data_dir\"], \"exercise_0\", \"subset_0\", \"out_train.npz\"),\n",
    "    **ML_SETTINGS[\"torch\"]\n",
    ")\n",
    "# Load the output test data - is independent of training subset\n",
    "out_test = rholearn.io.load_tensormap_to_torch(\n",
    "    os.path.join(DATA_SETTINGS[\"data_dir\"], \"out_test.npz\"), **ML_SETTINGS[\"torch\"]\n",
    ")\n",
    "\n",
    "# Load the Coulomb matrices from file\n",
    "coulomb_matrices = rholearn.io.load_tensormap_to_torch(\n",
    "    DATA_SETTINGS[\"coulomb_metrics\"], **ML_SETTINGS[\"torch\"]\n",
    ")\n",
    "\n",
    "# Construct the train and test loss CoulombLoss functions\n",
    "train_loss_fn = loss.CoulombLoss(coulomb_matrices, output_like=out_train)\n",
    "test_loss_fn = loss.CoulombLoss(coulomb_matrices, output_like=out_test)\n",
    "\n",
    "# Construct the MSELoss - no special initialization needed\n",
    "mse_loss_fn = loss.MSELoss(reduction=\"sum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the imput train and test data\n",
    "in_train = rholearn.io.load_tensormap_to_torch(\n",
    "    os.path.join(DATA_SETTINGS[\"data_dir\"], \"exercise_0\", \"subset_0\", \"in_train.npz\"),\n",
    "    **ML_SETTINGS[\"torch\"],\n",
    ")\n",
    "in_test = rholearn.io.load_tensormap_to_torch(\n",
    "    os.path.join(DATA_SETTINGS[\"data_dir\"], \"in_test.npz\"),\n",
    "    **ML_SETTINGS[\"torch\"],\n",
    ")\n",
    "\n",
    "# Make a prediction with the untrained linear model\n",
    "out_train_pred = linear_model(in_train)\n",
    "out_test_pred = linear_model(in_test)\n",
    "\n",
    "# Calculate and print losses\n",
    "print(\"linear model\")\n",
    "print(\n",
    "    f\"    CoulombLoss: train = {train_loss_fn(input=out_train_pred, target=out_train)},\"\n",
    "    + f\" test = {test_loss_fn(input=out_test_pred, target=out_test)} Ha\"\n",
    "    + f\"\\n    MSELoss: train = {mse_loss_fn(input=out_train_pred, target=out_train)},\"\n",
    "    + f\" test = {mse_loss_fn(input=out_test_pred, target=out_test)}\"\n",
    ")\n",
    "\n",
    "# Make a prediction with the untrained nonlinear model, print losses\n",
    "out_train_pred = nonlinear_model(in_train)\n",
    "out_test_pred = nonlinear_model(in_test)\n",
    "print(\"nonlinear model\")\n",
    "print(\n",
    "    f\"    CoulombLoss: train = {train_loss_fn(input=out_train_pred, target=out_train)},\"\n",
    "    + f\" test = {test_loss_fn(input=out_test_pred, target=out_test)} Ha\"\n",
    "    + f\"\\n    MSELoss: train = {mse_loss_fn(input=out_train_pred, target=out_train)},\"\n",
    "    + f\" test = {mse_loss_fn(input=out_test_pred, target=out_test)}\"\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that losses are quite high. This is to be expected - these are untrained\n",
    "models!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a Linear Model\n",
    "\n",
    "As every training subsets (whether belonging to the same or different\n",
    "learning exercise) are independent, model training can be performed separately\n",
    "and in principle in parallel. Here we will perform subset training sequentially.\n",
    "\n",
    "First, define the training settings."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now prepare each training subdirectory by contructing models and loss functions\n",
    "in them, ready for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-construct the appropriate torch objects (i.e. models, loss fxns)\n",
    "pretraining.construct_torch_objects(\n",
    "    DATA_SETTINGS,\n",
    "    ML_SETTINGS,\n",
    "    coulomb_path=DATA_SETTINGS[\"coulomb_metrics\"],\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect the directory structure in the ``simulations/`` folder - it mirrors the\n",
    "nested directory structure of the ``data/`` folder, but contains only torch\n",
    "objects corresponding to the torch model ``model.pt``, the coulomb loss function\n",
    "object ``loss_fn.pt`` and that of the test loss function ``loss_fn_test.pt``."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the test data is not dependent on the training subset, this can be loaded\n",
    "first. The torch settings (i.e. requires_grad, device, dtype) from the settings\n",
    "dict are used to load the TensorMaps to torch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the test data, which is independent of the training subdirectory\n",
    "in_test = rholearn.io.load_tensormap_to_torch(\n",
    "    os.path.join(DATA_SETTINGS[\"data_dir\"], \"in_test.npz\"), **ML_SETTINGS[\"torch\"]\n",
    ")\n",
    "out_test = rholearn.io.load_tensormap_to_torch(\n",
    "    os.path.join(DATA_SETTINGS[\"data_dir\"], \"out_test.npz\"), **ML_SETTINGS[\"torch\"]\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we iterate over the exercises and subsets and train models for each subset\n",
    "sequentially. Every epoch, the train and test losses are written to `log.txt` in\n",
    "the training directory. Inspect the file during runtime and observe the\n",
    "minimization of the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Runtime for 10 epochs, 2 exercises, 3 subsets:\n",
    "# linear ~ 15 min\n",
    "# nonlinear ~ 20 min\n",
    "from itertools import product\n",
    "\n",
    "exercises = [0]\n",
    "subsets = [0, 1, 2]\n",
    "\n",
    "for exercise, subset in product(exercises, subsets):\n",
    "\n",
    "    # Define the training subdirectory\n",
    "    train_rel_dir = f\"exercise_{exercise}/subset_{subset}\"\n",
    "    train_run_dir = os.path.join(ML_SETTINGS[\"run_dir\"], train_rel_dir)\n",
    "\n",
    "    # Load training data and torch objects\n",
    "    data, model, loss_fn, optimizer = pretraining.load_training_objects(\n",
    "        train_rel_dir, DATA_SETTINGS[\"data_dir\"], ML_SETTINGS, ML_SETTINGS[\"training\"][\"restart_epoch\"]\n",
    "    )\n",
    "\n",
    "    # Unpack the data\n",
    "    in_train, in_test, out_train, out_test = data\n",
    "\n",
    "    # Execute model training\n",
    "    print(f\"\\nTraining in subdirectory {train_rel_dir}\")\n",
    "    training.train(\n",
    "        in_train=in_train,\n",
    "        out_train=out_train,\n",
    "        in_test=in_test,\n",
    "        out_test=out_test,\n",
    "        model=model,\n",
    "        loss_fn=loss_fn,\n",
    "        optimizer=optimizer,\n",
    "        n_epochs=ML_SETTINGS[\"training\"][\"n_epochs\"],\n",
    "        save_interval=ML_SETTINGS[\"training\"][\"save_interval\"],\n",
    "        save_dir=train_run_dir,\n",
    "        restart=ML_SETTINGS[\"training\"][\"restart_epoch\"],\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a Nonlinear Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use the same notebook to train a nonlinear model, by changing only a\n",
    "few lines of code. Make the following changes to `ML_SETTINGS` in `settings.py`.\n",
    "\n",
    "1. Change the run directory:\n",
    "\n",
    "    ``\"run_dir\": os.path.join(RHOLEARN_DIR, \"docs/example/azoswitch/runs/02_nonlinear\"),``\n",
    "\n",
    "\n",
    "2. Change model type to \"nonlinear\" and uncomment the neural network args:\n",
    "  \n",
    "        ```\n",
    "        \"type\": \"nonlinear\",\n",
    "        \"args\": {\n",
    "            \"hidden_layer_widths\": [4, 4],\n",
    "            \"activation_fn\": \"SiLU\"\n",
    "        },\n",
    "        ```\n",
    "\n",
    "Then run the subsequent cells again, in order."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rho",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "576c71a426691bc103e620abf31b98f592c88b3903fdf6bf41ae71c4b8043fe6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
