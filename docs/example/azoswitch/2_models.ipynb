{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2: ``PyTorch`` Model Training\n",
    "\n",
    "* Time to run the cells: ~ "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define training settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import rholearn.io\n",
    "from rholearn.features import lambda_soap_vector\n",
    "\n",
    "root_dir = \"/Users/joe.abbott/Documents/phd/code/qml/rho_learn/docs/example/azoswitch\"\n",
    "data_dir = os.path.join(root_dir, \"data/\")\n",
    "run_dir = os.path.join(root_dir, \"simulations/\")\n",
    "rholearn.io.check_or_create_dir(run_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "settings = {\n",
    "    \"io\": {\n",
    "        \"data_dir\": os.path.join(data_dir, \"partitions/\"),\n",
    "        \"run_dir\": os.path.join(run_dir, \"01_linear\"),\n",
    "        \"coulomb\": os.path.join(data_dir, \"coulomb_matrices.npz\"),\n",
    "    },\n",
    "    \"data_partitions\": {\n",
    "        \"n_exercises\": 2,\n",
    "        \"n_subsets\": 3,\n",
    "    },\n",
    "    \"torch\": {\n",
    "        \"requires_grad\": True,\n",
    "        \"dtype\": torch.float64,\n",
    "        \"device\": torch.device(\"cpu\"),\n",
    "    },\n",
    "    \"model\": {\n",
    "        \"type\": \"linear\",\n",
    "        \"args\": {\n",
    "            # \"hidden_layer_widths\": [8, 8, 8],\n",
    "            # \"activation_fn\": \"SiLU\"\n",
    "        },\n",
    "    },\n",
    "    \"optimizer\": {\n",
    "        \"algorithm\": torch.optim.LBFGS,\n",
    "        \"args\": {\n",
    "            \"lr\": 0.25,\n",
    "        },\n",
    "    },\n",
    "    \"loss\": {\n",
    "        \"fn\": \"CoulombLoss\",\n",
    "        \"args\": {\n",
    "            # \"reduction\": \"sum\",\n",
    "        },\n",
    "    },\n",
    "    \"training\": {\n",
    "        \"n_epochs\": 30,\n",
    "        \"save_interval\": 15,\n",
    "        \"restart_epoch\": None,\n",
    "    },\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before going any further, it is important to set the torch default dtype. This\n",
    "needs to be consistent throughout all operations otherwise things will break.\n",
    "Typically, we want the precision of 64-bit floats, but the torch out-of-box\n",
    "default is 32-bit, so we need to explicitly set it to 64-bit here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTANT!\n",
    "torch.set_default_dtype(settings[\"torch\"][\"dtype\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import rholearn.io\n",
    "from rholearn.models import EquiModelGlobal\n",
    "\n",
    "in_train = rholearn.io.load_tensormap_to_torch(\n",
    "    os.path.join(settings[\"io\"][\"data_dir\"], \"in_train.npz\"), **settings[\"torch\"]\n",
    ")\n",
    "out_train = rholearn.io.load_tensormap_to_torch(\n",
    "    os.path.join(settings[\"io\"][\"data_dir\"], \"out_train.npz\"), **settings[\"torch\"]\n",
    ")\n",
    "\n",
    "EquiModelGlobal?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_model = EquiModelGlobal(\n",
    "    \"linear\",\n",
    "    keys=in_train.keys,\n",
    "    in_feature_labels={key: block.properties for key, block in in_train},\n",
    "    out_feature_labels={key: block.properties for key, block in out_train},\n",
    ")\n",
    "list(linear_model.models.items())[:2]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nonlinear Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![nonlinear_model_forward](../figures/nonlinear_architecture.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to a linear model, a nonlinear model is also implemented in\n",
    "``rholearn``. As in the global linear model, the global nonlinear model is a\n",
    "collection of individual local models applied to each block. Each local model\n",
    "makes a prediction on a given equivariant (i.e. either invariant or covariant)\n",
    "block of the input TensorMap, indexed by a key.\n",
    "\n",
    "The nonlinear local model architecture is shown in the figure below. Predictions\n",
    "are made on an equivariant (blue) block, using its associated invariant to act\n",
    "as a nonlinear multiplier. For instance, the equivariant block for Carbon,\n",
    "$\\lambda = 3$ is passed to the forward method along with the invariant ($\\lambda\n",
    "= 0$) block for Carbon. The equivariant is passed through a linear model and the\n",
    "invariant through a neural network of arbitrary architecture. Then, element-wise\n",
    "multiplication of the two blocks is performed before passing the result through\n",
    "a final linear output layer to get the electron density prediction.\n",
    "\n",
    "Applying nonlinear transformations to only the invariant ensure that\n",
    "equivariance isn't broken. Upon element-wise multiplication, the component\n",
    "vectors of the equivariant block are multiplied by a vector of constant size\n",
    "(thanks to the h-stacking of the invariant, botoom-right of the figure), thus\n",
    "retaining equivariance.\n",
    "\n",
    "Performing such operations within a single custom PyTorch ``forward()`` method\n",
    "allows the operations to be tracked, and therefore the gradientsto be\n",
    "calculated. This means that model training involves optimization of the weights\n",
    "of all weights and biases seen below - in the linear input layer applied to the\n",
    "equivariant, all layers of the neural network applied to the invariant, and the\n",
    "linear output layer applied to the mixed block.\n",
    "\n",
    "Let's build a global model by hand and look at just 2 of the individual local\n",
    "models. As the keys of the TensorMap are ``('spherical_harmonics_l',\n",
    "'species_center')``, there exists one invariant block for each chemical species\n",
    "(i.e. ``species_center``: H (1), C (6), N (7), O (8), S (16)). As explained\n",
    "above, these invariants are used as nonlinear multiplier to the equivariant\n",
    "blocks, so the size of their features need to be passed to the model in the\n",
    "``in_invariant_features`` during initialization. In the figure above, these\n",
    "values correspond to $q_{\\text{in}}^{\\text{inv}}$ (bottom left of the figure).\n",
    "\n",
    "The model architecture can also be controlled. The ``activation_fn`` to use in\n",
    "alternating layers between linear layers can be specified, choosing from \"Tanh\",\n",
    "\"GELU\", or \"SiLU\". The length of the list arg ``hidden_layer_widths`` controls\n",
    "the number of pairs of (nonlinear, linear) layers after the first input layer\n",
    "(i.e. all the hidden layers), whilst the values in the list control the width of\n",
    "them.\n",
    "\n",
    "In the cell below, we are initializing the neural network to have 3 pairs of\n",
    "layer, of widths 8, 8, and 16. Run the cell and look for the ``(invariant_nn)``\n",
    "``Sequential`` layer, containing alternating Linear and SiLU functions. Note\n",
    "also how for the ``EquiLocalModel`` for key \n",
    "``('spherical_harmonics_l', 'species_center')`` == ``(0, 1)`` a bias is used on\n",
    "the input and output linear layers, but for local model ``(1, 1)`` a bias isn't\n",
    "used. This is because for covariant blocks ($\\lambda > 0$) covariance is broken\n",
    "by applying a bias, but for invariants it isn't. A bias is applied **in the neural\n",
    "network layers** for local models, however, as only the invariant blocks\n",
    "supporting the forward method are passed through the NN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_invariant_features_by_species = {\n",
    "    specie: len(in_train.block(spherical_harmonics_l=0, species_center=specie).properties)\n",
    "    for specie in np.unique(in_train.keys[\"species_center\"])\n",
    "}\n",
    "in_invariant_features = {\n",
    "    key: in_invariant_features_by_species[key[in_train.keys.names.index(\"species_center\")]]\n",
    "    for key in in_train.keys\n",
    "}\n",
    "nonlinear_model = EquiModelGlobal(\n",
    "    \"nonlinear\",\n",
    "    keys=in_train.keys,\n",
    "    in_feature_labels={key: block.properties for key, block in in_train},\n",
    "    out_feature_labels={key: block.properties for key, block in out_train},\n",
    "    in_invariant_features=in_invariant_features,\n",
    "    hidden_layer_widths=[8, 8, 16],\n",
    "    activation_fn=\"SiLU\",\n",
    ")\n",
    "\n",
    "list(nonlinear_model.models.items())[:2]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking the Equivariance Condition\n",
    "\n",
    "Before going any further, it is paramount that we check that our structural\n",
    "representations and machine learning models are equivariant.\n",
    "\n",
    "In order for a structural representation to be equivariant, the irreducible\n",
    "spherical components that comprise it must transform like spherical harmonics.\n",
    "\n",
    "Spherical harmonics have known behviour under rotations, such that any spherical\n",
    "component $\\mu$ of order $\\lambda$ transforms into new component $\\mu'$ according to the\n",
    "action of the Wigner D-Matrix of order $\\lambda$, $D^{\\lambda}_{\\mu\\mu'}$. Which\n",
    "is constructed for a given arbitrary rotation matrix in Cartesian space.\n",
    "\n",
    "In order to check that our $\\lambda$-SOAP feature vector is equivariant, we can\n",
    "run the following test:\n",
    "\n",
    "1. Take an ``.xyz`` file of a given structure in the training set\n",
    "2. Build an ASE frame of this structure\n",
    "3. Generate a random rotation matrix\n",
    "4. Rotate the structure using the random rotation matrix, storing the new\n",
    "   structure in a new ASE frame\n",
    "5. Generate a $\\lambda$-SOAP representation for the unrotated and rotated\n",
    "   structures\n",
    "6. For each $\\lambda$ channel of the representation of the unrotated structure,\n",
    "   extract a selection of $(2\\lambda + 1)$-sized irreducible spherical component (ISC)\n",
    "   vectors.\n",
    "7. Rotate each of these ISC vectors using the Wigner D-Matrix constucted, using\n",
    "   the same random euler angles of rotation\n",
    "8. Check for exact equivalence between the rotated ISC vectors of the unrotated\n",
    "   structure and the corresponding ISC vectors of the rotated structure.\n",
    "\n",
    "First, let's load a random structure from the training set, construct rotated\n",
    "and unrotated ASE frames, and visualize them using chemiscope."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ase.io\n",
    "import numpy as np\n",
    "import chemiscope\n",
    "from rholearn import spherical\n",
    "\n",
    "# Pick a random index between 0 and 10 as the test structure\n",
    "structure_idx = np.random.randint(0, 10)\n",
    "\n",
    "# Load the xyz file corresponding to this structure\n",
    "with open(os.path.join(data_dir, \"molecule_list.dat\"), \"r\") as molecule_list:\n",
    "    structure_xyz = molecule_list.read().splitlines()[structure_idx]\n",
    "\n",
    "# Read xyz file into an ASE frame\n",
    "unrotated = ase.io.read(os.path.join(data_dir, \"xyz\", structure_xyz))\n",
    "\n",
    "# Generated a randomly rotated copy of the ASE frame\n",
    "rotated, (alpha, beta, gamma) = spherical.rotate_ase_frame(unrotated)\n",
    "\n",
    "# Visualize the frames using chemiscope\n",
    "cs = chemiscope.show([unrotated, rotated], mode=\"structure\")\n",
    "display(cs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now generate $\\lambda$-SOAP representations of the rotated and unrotated structures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rascaline hypers\n",
    "rascal_hypers = {\n",
    "    \"cutoff\": 5.0,  # Angstrom\n",
    "    \"max_radial\": 6,  # Exclusive\n",
    "    \"max_angular\": 5,  # Inclusive\n",
    "    \"atomic_gaussian_width\": 0.2,\n",
    "    \"radial_basis\": {\"Gto\": {}},\n",
    "    \"cutoff_function\": {\"ShiftedCosine\": {\"width\": 0.5}},\n",
    "    \"center_atom_weight\": 1.0,\n",
    "}\n",
    "\n",
    "# Generate lambda-SOAP descriptors. We want to do this individually for the\n",
    "# rotated and unrotated structures to keep the structure indices consistent\n",
    "lsoap_unrotated = lambda_soap_vector(\n",
    "    [unrotated], rascal_hypers, neighbor_species=[1, 6, 7, 8, 16]\n",
    ")\n",
    "lsoap_rotated = lambda_soap_vector(\n",
    "    [rotated], rascal_hypers, neighbor_species=[1, 6, 7, 8, 16]\n",
    ")\n",
    "\n",
    "# Convert tensors to torch\n",
    "lsoap_unrotated = utils.tensor_to_torch(lsoap_unrotated, **settings[\"torch\"])\n",
    "lsoap_rotated = utils.tensor_to_torch(lsoap_rotated, **settings[\"torch\"])\n",
    "\n",
    "# Perform the equivariance check - this returns a bool\n",
    "is_equi = spherical.check_equivariance(\n",
    "    lsoap_unrotated,\n",
    "    lsoap_rotated,\n",
    "    lmax=rascal_hypers[\"max_angular\"],\n",
    "    alpha=alpha,\n",
    "    beta=beta,\n",
    "    gamma=gamma,\n",
    "    n_checks_per_block=5000,\n",
    ")\n",
    "if is_equi:\n",
    "    print(\"Our lambda-SOAP is equivariant!\")\n",
    "else:\n",
    "    print(\"Oops, our lambda-SOAP is not equivariant...\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to check that our model is equivariant. When passing input tensors\n",
    "through a model, certain tensor operations contained within the model\n",
    "architecture (no matter how simple or complex) can break equivariance. It would\n",
    "be a shame to go through the effort of generating an equivariant representation\n",
    "if something as simple as applying a bias in our linear model breaks\n",
    "equivariance!\n",
    "\n",
    "In order for our model to be equivariant, it must satisfy the equivariant\n",
    "condition: \n",
    "\n",
    "$\\hat{R} y(A) = y(\\hat{R} A)$\n",
    "\n",
    "where $\\hat{R}$ is an arbitrary rotation matrix of the SO(3) group, $A$ is a\n",
    "trial structural representation, and $y(A)$ is the output property (i.e.\n",
    "electron density) of the model, predicting on structure $A$.\n",
    "\n",
    "In plainer words, the condition states (under the assumption that our structural\n",
    "representation is equivariant) that our model is equivariant if the property\n",
    "(electron density) we predict on an unrotated structure, subsequently rotated is\n",
    "**exactly equivalent** to the property we get if predict on the rotated\n",
    "structure, if the rotation matrix used in both is equal.\n",
    "\n",
    "We can therefore construct a test in the following way:\n",
    "\n",
    "1. \n",
    "\n",
    "Or, written in another way:\n",
    "\n",
    "$\\tilde{y}(A) = \\hat{R}^{-1} \\tilde{y}(\\hat{R} A)$\n",
    "\n",
    "In plainer words, the equivariance condition states that the output property of\n",
    "the model must transform equivariantly with the input structure. \n",
    "\n",
    "If the equivariance condition does not hold, it means that an operation\n",
    "somewhere in the workflow breaks equivariance. This could be either in the\n",
    "generation of the structural representation or in the model prediction. Since we\n",
    "just want to test our ML model for equivariance, we will take $A$ as the\n",
    "$\\lambda$-SOAP representation of our validation structure, but to check\n",
    "equivariance of the whole workflow one could take $A$ as the starting ``.xyz``\n",
    "file of the validation structure.\n",
    "\n",
    "\n",
    "We can check this by doing the following:\n",
    "\n",
    "1. Take the xyz file of the validation structure and rotate it with a\n",
    "random rotation matrix, $\\hat{R}$\n",
    "2. Generate a $\\lambda$-SOAP representation, $\\hat{R} A$, of the rotated structure\n",
    "3. Use the model to make a prediction on the $\\lambda$-SOAP of the rotated\n",
    "   structure, getting a predicted electron density, $\\tilde{y}(\\hat{R} A)$\n",
    "4. Perform the inverse rotation on this electron density to get \n",
    "   $\\hat{R}^{-1} \\tilde{y}(\\hat{R} A)$\n",
    "5. Check that this quantity is exactly equivalent to the electron density\n",
    "   prediction of the unrotated strutcure, $\\tilde{y}(A)$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "First, we need to do some cleaning (similar to the first notebook)and padding of the\n",
    "TensorMaps to make the dimensions consistent - don't worry too much about this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azoswitch_utils import clean_azoswitch_lambda_soap\n",
    "\n",
    "# Pad with empty blocks\n",
    "lsoap_rotated = utils.pad_with_empty_blocks(\n",
    "    clean_azoswitch_lambda_soap(lsoap_rotated), in_train\n",
    ")\n",
    "lsoap_unrotated = utils.pad_with_empty_blocks(\n",
    "    clean_azoswitch_lambda_soap(lsoap_unrotated), in_train\n",
    ")\n",
    "\n",
    "# Check equivariance of linear model\n",
    "out_pred_linear_unrot = linear_model(lsoap_unrotated)\n",
    "out_pred_linear_rot = linear_model(lsoap_rotated)\n",
    "\n",
    "out_pred_nonlin_unrot = nonlinear_model(lsoap_unrotated)\n",
    "out_pred_nonlin_rot = nonlinear_model(lsoap_rotated)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform the equivariance check on both the linear and nonlinear model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform the equivariance check on the linear and nonlinear models\n",
    "for i, (unrot, rot) in enumerate(\n",
    "    [\n",
    "        (out_pred_linear_unrot, out_pred_linear_rot),\n",
    "        (out_pred_nonlin_unrot, out_pred_nonlin_rot),\n",
    "    ]\n",
    "):\n",
    "    is_equi = spherical.check_equivariance(\n",
    "        unrot,\n",
    "        rot,\n",
    "        lmax=rascal_hypers[\"max_angular\"],\n",
    "        alpha=alpha,\n",
    "        beta=beta,\n",
    "        gamma=gamma,\n",
    "        n_checks_per_block=None,  # None checks on all ISC vectors\n",
    "    )\n",
    "    if is_equi:\n",
    "        print(f\"Our {['linear', 'nonlinear'][i]} model is equivariant!\")\n",
    "    else:\n",
    "        print(\n",
    "            f\"Oops, something in our {['linear', 'nonlinear'][i]} model is breaking equivariance...\"\n",
    "        )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good stuff! Now that we've performed those checks, let's do some model training."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct ``torch`` objects used in training"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ML Model**\n",
    "\n",
    "PyTorch model training is based on torch tensor operations. In order to\n",
    "interface with equistore and allow tracking of all the metadata useful in\n",
    "atomistic ML, custom model classes have been built in ``rholearn`` to allow\n",
    "predictions to be made on TensorMaps as a whole. The class ``EquiModelGlobal``\n",
    "stores individual models for each input/output block in the data.\n",
    "\n",
    "**Loss Function**\n",
    "\n",
    "A function that calculates a difference metric between a predicted (or 'input')\n",
    "and reference (or 'target') tensor. At the torch-equistore interface this is a\n",
    "custom torch module that calculates this difference on the TensorMap level.\n",
    "\n",
    "Currently implemented are the ``MSELoss`` (otherwise called L2 loss) and the\n",
    "``CoulombLoss`` metrics. \n",
    "\n",
    "As detailed in the paper [__\"Impact of quantum-chemical metrics on the machine\n",
    "learning prediction of electron\n",
    "density\"__](https://aip.scitation.org/doi/10.1063/5.0055393), use of a\n",
    "physically-inspired loss function such as the Coulomb repulsion metric can lead\n",
    "to better model performance when predicted properties derived from the electron\n",
    "density.\n",
    "\n",
    "**Optimizer**\n",
    "\n",
    "An algorithm, such as stochastic gradient descent (SGD) or LBFGS that performs\n",
    "gradient descent on the loss landscape with respect to the model parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rholearn.pretraining import construct_torch_objects\n",
    "\n",
    "construct_torch_objects(settings)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect the directory structure in the ``simulations/`` folder - it mirrors the\n",
    "nested directory structure of the ``data/`` folder, but contains only torhc\n",
    "objects corresponding to the torch model ``model.pt``, the coulomb loss function\n",
    "object ``loss_fn.pt`` and that of the test loss function ``loss_fn_test.pt``."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a Linear Model\n",
    "\n",
    "Though we have shuffled and partitioned our data ready for 3 learning exercises,\n",
    "each with 4 training subsets, for computational brevity we will perform a single\n",
    "learning exercise. As every training subsets (whether belonging to the same or different\n",
    "learning exercise) are independent, model training can be performed separately\n",
    "and in principle in parallel. Here we will perform subset training sequentially"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the range of exercises and subsets to learn on\n",
    "exercises = range(settings[\"data_partitions\"][\"n_exercises\"])\n",
    "subsets = range(settings[\"data_partitions\"][\"n_subsets\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the test data is not dependent on the training subset, this can be loaded\n",
    "first. The torch settings (i.e. requires_grad, device, dtype) from the settings\n",
    "dict are used to load the TensorMaps to torch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rholearn.io import load_tensormap_to_torch\n",
    "\n",
    "# Load the test data, which is independent of the training subdirectory\n",
    "in_test = load_tensormap_to_torch(\n",
    "    os.path.join(settings[\"io\"][\"data_dir\"], \"in_test.npz\"), **settings[\"torch\"]\n",
    ")\n",
    "out_test = load_tensormap_to_torch(\n",
    "    os.path.join(settings[\"io\"][\"data_dir\"], \"out_test.npz\"), **settings[\"torch\"]\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we iterate over the exercises and subsets and train models for each subset\n",
    "sequentially. To train on the 4 subsets for 10 epochs each should take roughly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Runtime for 30 epochs, 2 exercises, 3 subsets:\n",
    "# linear ~ 35 min\n",
    "# nonlinear ~\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "from rholearn.pretraining import load_training_objects\n",
    "from rholearn.training import train\n",
    "\n",
    "for exercise in exercises:\n",
    "    for subset in subsets:\n",
    "\n",
    "        # Start timer\n",
    "        t0 = time.time()\n",
    "\n",
    "        # Define the training subdirectory\n",
    "        train_dir = os.path.join(\n",
    "            settings[\"io\"][\"run_dir\"], f\"exercise_{exercise}\", f\"subset_{subset}\"\n",
    "        )\n",
    "\n",
    "        # Load training data and torch objects\n",
    "        in_train, out_train, model, loss_fn, optimizer = load_training_objects(\n",
    "            settings, exercise, subset, settings[\"training\"][\"restart_epoch\"]\n",
    "        )\n",
    "\n",
    "        print(f\"\\nTraining in subdirectory {train_dir}\")\n",
    "\n",
    "        # Execute model training\n",
    "        train(\n",
    "            in_train=in_train,\n",
    "            out_train=out_train,\n",
    "            in_test=in_test,\n",
    "            out_test=out_test,\n",
    "            model=model,\n",
    "            loss_fn=loss_fn,\n",
    "            optimizer=optimizer,\n",
    "            n_epochs=settings[\"training\"][\"n_epochs\"],\n",
    "            save_interval=settings[\"training\"][\"save_interval\"],\n",
    "            save_dir=train_dir,\n",
    "            restart=settings[\"training\"][\"restart_epoch\"],\n",
    "        )\n",
    "\n",
    "        # Report on timings\n",
    "        dt = time.time() - t0\n",
    "        num_epochs_run = (\n",
    "            settings[\"training\"][\"n_epochs\"]\n",
    "            if settings[\"training\"][\"restart_epoch\"] is None\n",
    "            else settings[\"training\"][\"n_epochs\"]\n",
    "            - settings[\"training\"][\"restart_epoch\"]\n",
    "        )\n",
    "        # TODO: write timing to log.txt\n",
    "        print(\n",
    "            f\"\\nTraining finished in {np.round(dt, 2)} s = {np.round(dt / num_epochs_run, 2)} s per epoch\"\n",
    "            + f\"\\n(Timed over {num_epochs_run} epochs, perhaps since restart)\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a Nonlinear Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use the same notebook to train a nonlinear model, by changing only a\n",
    "few lines of code. Make the following changes:\n",
    "\n",
    "In the ``settings`` dict:\n",
    "\n",
    "1. In the nested dict under key ``\"io\"``, change the run directory to \"02_nonlinear\":\n",
    "\n",
    "    ``\"run_dir\": os.path.join(run_dir, \"02_nonlinear\"),``\n",
    "\n",
    "\n",
    "2. Under key ``\"model\"``, change the type to \"nonlinear\" and uncomment the neural\n",
    "  network args:\n",
    "  \n",
    "  \n",
    "        ```\n",
    "        \"type\": \"linear\",\n",
    "            \"args\": {\n",
    "                \"hidden_layer_widths\": [32, 32, 32],\n",
    "                \"activation_fn\": \"SiLU\"\n",
    "            },\n",
    "        ```\n",
    "\n",
    "Then run all the notebook cells again, in order.\n",
    "\n",
    "After doing so, we can observe the model architecture for one of the blocks:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rho",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "576c71a426691bc103e620abf31b98f592c88b3903fdf6bf41ae71c4b8043fe6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
