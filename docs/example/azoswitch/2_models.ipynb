{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2: ``PyTorch`` Model Training\n",
    "\n",
    "* Time to run the cells: \n",
    "    * ~ 5 mins for 5 epochs, 1 exercise, 1 subset\n",
    "    * ~ 40 mins for 10 epochs, 2 exercises, 3 subsets\n",
    "\n",
    "First thing's first; set the absolute path of the ``rho_learn`` directory on\n",
    "your local machine, for instance:\n",
    "\n",
    "``RHOLEARN_DIR = \"/Users/joe.abbott/Documents/phd/code/qml/rho_learn/\"``\n",
    "\n",
    "We also need to import a ``rholearn`` module here for use later.\n",
    "\n",
    "In this notebook we will see how to construct linear and nonlinear global (i.e.\n",
    "on the TensorMap level) models and a CoulombLoss function using the custom\n",
    "PyTorch classes built to interface with equistore. After checking the\n",
    "equivariance condition of our structural representation and model, we will do\n",
    "some model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rholearn.features import lambda_soap_vector\n",
    "\n",
    "RHOLEARN_DIR = \"/Users/joe.abbott/Documents/phd/code/rho/rho_learn/\"  # for example\n",
    "# RHOLEARN_DIR = \"/path/to/rho_learn/\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define training settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "import rholearn.io\n",
    "\n",
    "data_dir = os.path.join(RHOLEARN_DIR, \"docs/example/azoswitch/data\")\n",
    "run_dir = os.path.join(RHOLEARN_DIR, \"docs/example/azoswitch/simulations\")\n",
    "rholearn.io.check_or_create_dir(run_dir)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define some preliminary settings and set the torch default dtype. This needs to\n",
    "be consistent throughout all operations otherwise things will break. Typically,\n",
    "we want the precision of 64-bit floats, but the torch out-of-box default is\n",
    "32-bit, so we need to explicitly set it to 64-bit here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "settings = {\n",
    "    \"io\": {\n",
    "        \"data_dir\": os.path.join(data_dir, \"partitions/\"),\n",
    "        \"run_dir\": os.path.join(run_dir, \"01_linear\"),\n",
    "        \"coulomb\": os.path.join(data_dir, \"coulomb_matrices.npz\"),\n",
    "    },\n",
    "    \"data_partitions\": {\n",
    "        \"n_exercises\": 2,\n",
    "        \"n_subsets\": 3,\n",
    "    },\n",
    "    # Torch settings: which device tensors should be loaded to\n",
    "    \"torch\": {\n",
    "        \"requires_grad\": True,  # needed to track gradients\n",
    "        \"dtype\": torch.float64,  # recommended\n",
    "        \"device\": torch.device(\"cpu\"),  # which device to load tensors to\n",
    "    },\n",
    "}\n",
    "\n",
    "# IMPORTANT!\n",
    "torch.set_default_dtype(settings[\"torch\"][\"dtype\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import rholearn.io\n",
    "from rholearn.models import EquiModelGlobal\n",
    "\n",
    "in_train = rholearn.io.load_tensormap_to_torch(\n",
    "    os.path.join(settings[\"io\"][\"data_dir\"], \"in_train.npz\"), **settings[\"torch\"]\n",
    ")\n",
    "out_train = rholearn.io.load_tensormap_to_torch(\n",
    "    os.path.join(settings[\"io\"][\"data_dir\"], \"out_train.npz\"), **settings[\"torch\"]\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the ``EquiModelGlobal?`` command, we can see the arguments the global\n",
    "model class takes upon initiliazation. The first 4 args are required to build\n",
    "linear model, specifying the model type, in this case \"linear\", the keys of the\n",
    "TensorMap that the model is built for, and the properties/features labels for\n",
    "each of the blocks. These are required to map a number of input features to\n",
    "output features for each block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EquiModelGlobal?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can build a linear model and, using the ``model.models`` attribute, print\n",
    "out 2 of the local models that exist for the first 2 blocks, indexed by keys\n",
    "``(0, 1)`` and ``(1, 1)``. You can see that the number of in a nf out features\n",
    "of the linear layers are different, and that the model for the invariant block\n",
    "uses a bias, whereas the one for the covariant ($\\lambda = 1$) block doesn't."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_model = EquiModelGlobal(\n",
    "    \"linear\",\n",
    "    keys=in_train.keys,\n",
    "    in_feature_labels={key: block.properties for key, block in in_train},\n",
    "    out_feature_labels={key: block.properties for key, block in out_train},\n",
    ")\n",
    "list(linear_model.models.items())[:2]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nonlinear Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![nonlinear_model_forward](../figures/nonlinear_architecture.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to a linear model, a nonlinear model is also implemented in\n",
    "``rholearn``. As in the global linear model, the global nonlinear model is a\n",
    "collection of individual local models applied to each block. Each local model\n",
    "makes a prediction on a given equivariant (i.e. either invariant or covariant)\n",
    "block of the input TensorMap, indexed by a key.\n",
    "\n",
    "The nonlinear local model architecture is shown in the figure below. Predictions\n",
    "are made on an equivariant (blue) block, using its associated invariant to act\n",
    "as a nonlinear multiplier. For instance, the equivariant block for Carbon,\n",
    "$\\lambda = 3$ is passed to the forward method along with the invariant ($\\lambda\n",
    "= 0$) block for Carbon. The equivariant is passed through a linear model and the\n",
    "invariant through a neural network of arbitrary architecture. Then, element-wise\n",
    "multiplication of the two blocks is performed before passing the result through\n",
    "a final linear output layer to get the electron density prediction.\n",
    "\n",
    "Applying nonlinear transformations to only the invariant ensure that\n",
    "equivariance isn't broken. Upon element-wise multiplication, the component\n",
    "vectors of the equivariant block are multiplied by a vector of constant size\n",
    "(thanks to the h-stacking of the invariant, botoom-right of the figure), thus\n",
    "retaining equivariance.\n",
    "\n",
    "Performing such operations within a single custom PyTorch ``forward()`` method\n",
    "allows the operations to be tracked, and therefore the gradientsto be\n",
    "calculated. This means that model training involves optimization of the weights\n",
    "of all weights and biases seen below - in the linear input layer applied to the\n",
    "equivariant, all layers of the neural network applied to the invariant, and the\n",
    "linear output layer applied to the mixed block.\n",
    "\n",
    "Let's build a global model by hand and look at just 2 of the individual local\n",
    "models. As the keys of the TensorMap are ``('spherical_harmonics_l',\n",
    "'species_center')``, there exists one invariant block for each chemical species\n",
    "(i.e. ``species_center``: H (1), C (6), N (7), O (8), S (16)). As explained\n",
    "above, these invariants are used as nonlinear multiplier to the equivariant\n",
    "blocks, so the size of their features need to be passed to the model in the\n",
    "``in_invariant_features`` during initialization. In the figure above, these\n",
    "values correspond to $q_{\\text{in}}^{\\text{inv}}$ (bottom left of the figure).\n",
    "\n",
    "The model architecture can also be controlled. The ``activation_fn`` to use in\n",
    "alternating layers between linear layers can be specified, choosing from \"Tanh\",\n",
    "\"GELU\", or \"SiLU\". The length of the list arg ``hidden_layer_widths`` controls\n",
    "the number of pairs of (nonlinear, linear) layers after the first input layer\n",
    "(i.e. all the hidden layers), whilst the values in the list control the width of\n",
    "them.\n",
    "\n",
    "In the cell below, we are initializing the neural network to have 3 pairs of\n",
    "layer, of widths 8, 8, and 16. Run the cell and look for the ``(invariant_nn)``\n",
    "``Sequential`` layer, containing alternating Linear and SiLU functions. Note\n",
    "also how for the ``EquiLocalModel`` for key \n",
    "``('spherical_harmonics_l', 'species_center')`` == ``(0, 1)`` a bias is used on\n",
    "the input and output linear layers, but for local model ``(1, 1)`` a bias isn't\n",
    "used. This is because for covariant blocks ($\\lambda > 0$) covariance is broken\n",
    "by applying a bias, but for invariants it isn't. A bias is applied **in the neural\n",
    "network layers** for local models, however, as only the invariant blocks\n",
    "supporting the forward method are passed through the NN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_invariant_features_by_species = {\n",
    "    specie: len(in_train.block(spherical_harmonics_l=0, species_center=specie).properties)\n",
    "    for specie in np.unique(in_train.keys[\"species_center\"])\n",
    "}\n",
    "in_invariant_features = {\n",
    "    key: in_invariant_features_by_species[key[in_train.keys.names.index(\"species_center\")]]\n",
    "    for key in in_train.keys\n",
    "}\n",
    "nonlinear_model = EquiModelGlobal(\n",
    "    \"nonlinear\",\n",
    "    keys=in_train.keys,\n",
    "    in_feature_labels={key: block.properties for key, block in in_train},\n",
    "    out_feature_labels={key: block.properties for key, block in out_train},\n",
    "    in_invariant_features=in_invariant_features,\n",
    "    hidden_layer_widths=[8, 8, 16],\n",
    "    activation_fn=\"SiLU\",\n",
    ")\n",
    "\n",
    "list(nonlinear_model.models.items())[:2]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking the Equivariance Condition\n",
    "\n",
    "Before going any further, it is important that we check that our structural\n",
    "representations and machine learning models are equivariant.\n",
    "\n",
    "In order for a structural representation to be equivariant, the irreducible\n",
    "spherical components that comprise it must transform like spherical harmonics.\n",
    "\n",
    "Spherical harmonics have known behviour under rotations, such that any spherical\n",
    "component $\\mu$ of order $\\lambda$ transforms into new component $\\mu'$ according to the\n",
    "action of the Wigner D-Matrix of order $\\lambda$, $D^{\\lambda}_{\\mu\\mu'}$, which\n",
    "is constructed for a given arbitrary rotation matrix in Cartesian space.\n",
    "\n",
    "In order to check that our $\\lambda$-SOAP feature vector is equivariant, we can\n",
    "run the following test:\n",
    "\n",
    "1. Take an ``.xyz`` file of a given structure in the training set\n",
    "2. Build an ASE frame of this structure\n",
    "3. Generate a random cartesian rotation matrix\n",
    "4. Rotate the xyz coordinates of the structure according to the random rotation\n",
    "   matrix, storing the rotated structure in a new ASE frame\n",
    "5. Generate a $\\lambda$-SOAP representation for the unrotated and rotated\n",
    "   structures\n",
    "6. For each $\\lambda$ channel of the representation of the unrotated structure,\n",
    "   extract a selection of $(2\\lambda + 1)$-sized irreducible spherical component (ISC)\n",
    "   vectors.\n",
    "7. Rotate each of these ISC vectors using the Wigner D-Matrix constucted\n",
    "   according to cartesian rotation matrix defined in step 3\n",
    "8. Check for exact equivalence between the rotated ISC vectors of the unrotated\n",
    "   structure and the corresponding ISC vectors of the rotated structure.\n",
    "\n",
    "First, let's load a random structure from the training set, construct rotated\n",
    "and unrotated ASE frames, and visualize them using chemiscope. By moving the\n",
    "slider, you can see that they are rigidly rotated versions of eachother."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ase.io\n",
    "import numpy as np\n",
    "import chemiscope\n",
    "from rholearn import spherical\n",
    "\n",
    "# Pick a random index between 0 and 10 as the test structure\n",
    "structure_idx = np.random.randint(0, 10)\n",
    "\n",
    "# Load the xyz file corresponding to this structure\n",
    "with open(os.path.join(data_dir, \"molecule_list.dat\"), \"r\") as molecule_list:\n",
    "    structure_xyz = molecule_list.read().splitlines()[structure_idx]\n",
    "\n",
    "# Read xyz file into an ASE frame\n",
    "unrotated = ase.io.read(os.path.join(data_dir, \"xyz\", structure_xyz))\n",
    "\n",
    "# Generated a randomly rotated copy of the ASE frame\n",
    "rotated, (alpha, beta, gamma) = spherical.rotate_ase_frame(unrotated)\n",
    "\n",
    "# Visualize the frames using chemiscope\n",
    "cs = chemiscope.show([unrotated, rotated], mode=\"structure\")\n",
    "display(cs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now generate $\\lambda$-SOAP representations of the rotated and unrotated\n",
    "structures and pass these through a function that checks for equivariance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rholearn import utils\n",
    "\n",
    "# Rascaline hypers\n",
    "rascal_hypers = {\n",
    "    \"cutoff\": 5.0,  # Angstrom\n",
    "    \"max_radial\": 6,  # Exclusive\n",
    "    \"max_angular\": 5,  # Inclusive\n",
    "    \"atomic_gaussian_width\": 0.2,\n",
    "    \"radial_basis\": {\"Gto\": {}},\n",
    "    \"cutoff_function\": {\"ShiftedCosine\": {\"width\": 0.5}},\n",
    "    \"center_atom_weight\": 1.0,\n",
    "}\n",
    "\n",
    "# Generate lambda-SOAP descriptors. We want to do this individually for the\n",
    "# rotated and unrotated structures to keep the structure indices consistent\n",
    "lsoap_unrotated = lambda_soap_vector(\n",
    "    [unrotated], rascal_hypers, neighbor_species=[1, 6, 7, 8, 16]\n",
    ")\n",
    "lsoap_rotated = lambda_soap_vector(\n",
    "    [rotated], rascal_hypers, neighbor_species=[1, 6, 7, 8, 16]\n",
    ")\n",
    "\n",
    "# Convert tensors to torch\n",
    "lsoap_unrotated = utils.tensor_to_torch(lsoap_unrotated, **settings[\"torch\"])\n",
    "lsoap_rotated = utils.tensor_to_torch(lsoap_rotated, **settings[\"torch\"])\n",
    "\n",
    "# Perform the equivariance check - this returns a bool. Use the rotation matrix\n",
    "# previosuly defined to produce the lambda-SOAP of the rotated and unrotated\n",
    "# structures\n",
    "is_equi = spherical.check_equivariance(\n",
    "    lsoap_unrotated,\n",
    "    lsoap_rotated,\n",
    "    lmax=rascal_hypers[\"max_angular\"],\n",
    "    alpha=alpha,\n",
    "    beta=beta,\n",
    "    gamma=gamma,\n",
    "    n_checks_per_block=5000,\n",
    ")\n",
    "if is_equi:\n",
    "    print(\"Our lambda-SOAP is equivariant!\")\n",
    "else:\n",
    "    print(\"Oops, our lambda-SOAP is not equivariant...\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to check that our model is equivariant. When passing input tensors\n",
    "through a model, certain tensor operations contained within the model\n",
    "architecture (no matter how simple or complex) can break equivariance. It would\n",
    "be a shame to go through the effort of generating an equivariant representation\n",
    "if something as simple as applying a bias in our linear model breaks\n",
    "equivariance!\n",
    "\n",
    "In order for our model to be equivariant, it must satisfy the equivariant\n",
    "condition: \n",
    "\n",
    "$\\hat{R} y(A) = y(\\hat{R} A)$\n",
    "\n",
    "where $\\hat{R}$ is an arbitrary rotation matrix of the SO(3) group, $A$ is a\n",
    "trial structural representation, and $y(A)$ is the output property (i.e.\n",
    "electron density) of the model, predicting on structure $A$.\n",
    "\n",
    "In plainer words, the condition states (under the assumption that our structural\n",
    "representation is equivariant) that our model is equivariant if the property\n",
    "(electron density) we predict on an unrotated structure, subsequently rotated is\n",
    "**exactly equivalent** to the property we get if predict on the rotated\n",
    "structure, if the rotation matrix used in both is equal.\n",
    "\n",
    "We can therefore construct a test in the following way, using the $\\lambda$-SOAP\n",
    "representations for the rotated and unrotated structures generated above:\n",
    "\n",
    "1. Pass the $\\lambda$-SOAP representation of the unrotated structure through the\n",
    "   ML model to generate a predicted electron density.\n",
    "2. Do the same for the $\\lambda$-SOAP representation of the rotated structure.\n",
    "3. For each $\\lambda$ channel of the electron density of the unrotated\n",
    "   structure, extract a selection of $(2\\lambda + 1)$-sized irreducible\n",
    "   spherical component (ISC) vectors.\n",
    "4. Rotate each of these ISC vectors using the Wigner D-Matrix constucted\n",
    "   according to cartesian rotation matrix defined above\n",
    "5. Check for exact equivalence between the rotated ISC vectors of the unrotated\n",
    "   electron density and the corresponding ISC vectors of the rotated electron\n",
    "   density.\n",
    "\n",
    "First, we need to do some cleaning (similar to as we did in the first notebook)\n",
    "and padding of the TensorMaps to make the dimensions consistent - don't worry\n",
    "too much about this. The, predict the electron density for the rotated and\n",
    "unrotated structures, using the linear and nonlinear models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azoswitch_utils import clean_azoswitch_lambda_soap\n",
    "\n",
    "# Clean and pad with empty blocks for size consistency\n",
    "lsoap_rotated = utils.pad_with_empty_blocks(\n",
    "    clean_azoswitch_lambda_soap(lsoap_rotated), in_train\n",
    ")\n",
    "lsoap_unrotated = utils.pad_with_empty_blocks(\n",
    "    clean_azoswitch_lambda_soap(lsoap_unrotated), in_train\n",
    ")\n",
    "\n",
    "# Pass the rotated and unrotated structures through the linear model\n",
    "out_pred_linear_unrot = linear_model(lsoap_unrotated)\n",
    "out_pred_linear_rot = linear_model(lsoap_rotated)\n",
    "\n",
    "# Pass the rotated and unrotated structures through the nonlinear model\n",
    "out_pred_nonlin_unrot = nonlinear_model(lsoap_unrotated)\n",
    "out_pred_nonlin_rot = nonlinear_model(lsoap_rotated)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform the equivariance check on both the linear and nonlinear model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform the equivariance check on the linear and nonlinear models\n",
    "for i, (unrot, rot) in enumerate(\n",
    "    [\n",
    "        (out_pred_linear_unrot, out_pred_linear_rot),\n",
    "        (out_pred_nonlin_unrot, out_pred_nonlin_rot),\n",
    "    ]\n",
    "):\n",
    "    is_equi = spherical.check_equivariance(\n",
    "        unrot,\n",
    "        rot,\n",
    "        lmax=rascal_hypers[\"max_angular\"],\n",
    "        alpha=alpha,\n",
    "        beta=beta,\n",
    "        gamma=gamma,\n",
    "        n_checks_per_block=None,  # None checks on all ISC vectors\n",
    "    )\n",
    "    if is_equi:\n",
    "        print(f\"Our {['linear', 'nonlinear'][i]} model is equivariant!\")\n",
    "    else:\n",
    "        print(\n",
    "            f\"Oops, something in our {['linear', 'nonlinear'][i]} model is breaking equivariance...\"\n",
    "        )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good stuff! Now that we've performed those checks, let's do some model training."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct ``torch`` objects used in training"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prior to model training, it can be beneficial to pre-initialize and store some\n",
    "of the torch objects that will be used, especially if some pre-processing\n",
    "calculations can be performed.\n",
    "\n",
    "### ML Model\n",
    "\n",
    "PyTorch model training is based on torch tensor operations. In order to\n",
    "interface with equistore and allow tracking of all the metadata useful in\n",
    "atomistic ML, custom model classes have been built in ``rholearn`` to allow\n",
    "predictions to be made on TensorMaps as a whole. The class ``EquiModelGlobal``\n",
    "stores individual models for each input/output block in the data, as seen above.\n",
    "\n",
    "### Loss Function\n",
    "\n",
    "A function that calculates a difference metric between a predicted (or 'input')\n",
    "and reference (or 'target') tensor. At the torch-equistore interface this is a\n",
    "custom torch module that calculates this difference on the TensorMap level.\n",
    "\n",
    "Currently implemented are the ``MSELoss`` (otherwise called L2 loss) and the\n",
    "``CoulombLoss`` metrics. \n",
    "\n",
    "As detailed in the paper [__\"Impact of quantum-chemical metrics on the machine\n",
    "learning prediction of electron\n",
    "density\"__](https://aip.scitation.org/doi/10.1063/5.0055393), use of a\n",
    "physically-inspired loss function such as the Coulomb repulsion metric can lead\n",
    "to better model performance when predicting properties derived from the electron\n",
    "density.\n",
    "\n",
    "While the MSE loss needs no-preprocessing or initialization, when using a\n",
    "CoulombLoss the speed of simulations can benefit greatly from pre-processing.\n",
    "The Coulomb matrices can be distilled to contain data only for structures present\n",
    "in the training and test data sets they will evaluate the loss for. Especially\n",
    "as multiple training exercises on various subsets will be performed,\n",
    "initializing and storing both a train loss and test loss object for each\n",
    "training subset, to be loaded at runtime, can speed things up.\n",
    "\n",
    "provided can be heavly pre-processed prior to use at\n",
    "runtime of model training. The custom torch module ``CoulombLoss`` pre-processes\n",
    "\n",
    "### Optimizer\n",
    "\n",
    "An algorithm, such as stochastic gradient descent (SGD) or LBFGS that performs\n",
    "gradient descent on the loss landscape with respect to the model parameters.\n",
    "This is generally a lightweight object that doesn't need to be pre-constructed."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing a CoulombLoss object\n",
    "\n",
    "Let's build train and test CoulombLoss objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rholearn.loss import CoulombLoss, MSELoss\n",
    "\n",
    "# Load the output train data from the exercise 0 subset 0 training directory\n",
    "out_train = rholearn.io.load_tensormap_to_torch(\n",
    "    os.path.join(settings[\"io\"][\"data_dir\"], \"exercise_0\", \"subset_0\", \"out_train.npz\"),\n",
    "    **settings[\"torch\"]\n",
    ")\n",
    "# Load the output test data - is independent of training subset\n",
    "out_test = rholearn.io.load_tensormap_to_torch(\n",
    "    os.path.join(settings[\"io\"][\"data_dir\"], \"out_test.npz\"), **settings[\"torch\"]\n",
    ")\n",
    "\n",
    "# Load the Coulomb matrices from file\n",
    "coulomb_matrices = rholearn.io.load_tensormap_to_torch(\n",
    "    settings[\"io\"][\"coulomb\"], **settings[\"torch\"]\n",
    ")\n",
    "\n",
    "# Construct the train and test loss CoulombLoss functions\n",
    "train_loss_fn = CoulombLoss(coulomb_matrices, output_like=out_train)\n",
    "test_loss_fn = CoulombLoss(coulomb_matrices, output_like=out_test)\n",
    "\n",
    "# Construct the MSELoss - no special initialization needed\n",
    "mse_loss_fn = MSELoss(reduction=\"sum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the imput train and test data\n",
    "in_train = rholearn.io.load_tensormap_to_torch(\n",
    "    os.path.join(settings[\"io\"][\"data_dir\"], \"exercise_0\", \"subset_0\", \"in_train.npz\"),\n",
    "    **settings[\"torch\"],\n",
    ")\n",
    "in_test = rholearn.io.load_tensormap_to_torch(\n",
    "    os.path.join(settings[\"io\"][\"data_dir\"], \"in_test.npz\"),\n",
    "    **settings[\"torch\"],\n",
    ")\n",
    "\n",
    "# Make a prediction with the untrained linear model\n",
    "out_train_pred = linear_model(in_train)\n",
    "out_test_pred = linear_model(in_test)\n",
    "\n",
    "# Calculate and print losses\n",
    "print(\"linear model\")\n",
    "print(\n",
    "    f\"    CoulombLoss: train = {train_loss_fn(input=out_train_pred, target=out_train)},\"\n",
    "    + f\" test = {test_loss_fn(input=out_test_pred, target=out_test)} Ha\"\n",
    "    + f\"\\n    MSELoss: train = {mse_loss_fn(input=out_train_pred, target=out_train)},\"\n",
    "    + f\" test = {mse_loss_fn(input=out_test_pred, target=out_test)}\"\n",
    ")\n",
    "\n",
    "# Make a prediction with the untrained nonlinear model, print losses\n",
    "out_train_pred = nonlinear_model(in_train)\n",
    "out_test_pred = nonlinear_model(in_test)\n",
    "print(\"nonlinear model\")\n",
    "print(\n",
    "    f\"    CoulombLoss: train = {train_loss_fn(input=out_train_pred, target=out_train)},\"\n",
    "    + f\" test = {test_loss_fn(input=out_test_pred, target=out_test)} Ha\"\n",
    "    + f\"\\n    MSELoss: train = {mse_loss_fn(input=out_train_pred, target=out_train)},\"\n",
    "    + f\" test = {mse_loss_fn(input=out_test_pred, target=out_test)}\"\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that losses are quite high. This is to be expected - these are untrained\n",
    "models!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a Linear Model\n",
    "\n",
    "As every training subsets (whether belonging to the same or different\n",
    "learning exercise) are independent, model training can be performed separately\n",
    "and in principle in parallel. Here we will perform subset training sequentially.\n",
    "\n",
    "First, define the training settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "settings = {\n",
    "    \"io\": {\n",
    "        \"data_dir\": os.path.join(data_dir, \"partitions/\"),\n",
    "        \"run_dir\": os.path.join(run_dir, \"03_demo\"),\n",
    "        \"coulomb\": os.path.join(data_dir, \"coulomb_matrices.npz\"),\n",
    "    },\n",
    "    \"data_partitions\": {\n",
    "        \"n_exercises\": 2,\n",
    "        \"n_subsets\": 3,\n",
    "    },\n",
    "    # Torch settings: which device tensors should be loaded to\n",
    "    \"torch\": {\n",
    "        \"requires_grad\": True,  # needed to track gradients\n",
    "        \"dtype\": torch.float64,  # recommended\n",
    "        \"device\": torch.device(\"cpu\"),  # which device to load tensors to\n",
    "    },\n",
    "    # Model settings\n",
    "    \"model\": {\n",
    "        \"type\": \"linear\",  # linear or nonlinear\n",
    "        \"args\": {\n",
    "            # \"hidden_layer_widths\": [4, 4, 4],\n",
    "            # \"activation_fn\": \"SiLU\"\n",
    "        },\n",
    "    },\n",
    "    \"optimizer\": {\n",
    "        \"algorithm\": torch.optim.LBFGS,\n",
    "        \"args\": {\n",
    "            \"lr\": 0.25,\n",
    "        },\n",
    "    },\n",
    "    \"loss\": {\n",
    "        \"fn\": \"CoulombLoss\",  # CoulombLoss or MSELoss\n",
    "        \"args\": {\n",
    "            # \"reduction\": \"sum\",  # reduction can be used with MSELoss\n",
    "        },\n",
    "    },\n",
    "    \"training\": {\n",
    "        \"n_epochs\": 5,  # number of total epochs to run \n",
    "        \"save_interval\": 5,  # save model and optimizer state every x intervals\n",
    "        \"restart_epoch\": None,  # pass as the epoch checkpoint num. if restarting\n",
    "    },\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now prepare each training subdirectory by contructing models and loss functions\n",
    "in them, ready for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rholearn.pretraining import construct_torch_objects\n",
    "\n",
    "construct_torch_objects(settings)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect the directory structure in the ``simulations/`` folder - it mirrors the\n",
    "nested directory structure of the ``data/`` folder, but contains only torch\n",
    "objects corresponding to the torch model ``model.pt``, the coulomb loss function\n",
    "object ``loss_fn.pt`` and that of the test loss function ``loss_fn_test.pt``."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the test data is not dependent on the training subset, this can be loaded\n",
    "first. The torch settings (i.e. requires_grad, device, dtype) from the settings\n",
    "dict are used to load the TensorMaps to torch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the test data, which is independent of the training subdirectory\n",
    "in_test = rholearn.io.load_tensormap_to_torch(\n",
    "    os.path.join(settings[\"io\"][\"data_dir\"], \"in_test.npz\"), **settings[\"torch\"]\n",
    ")\n",
    "out_test = rholearn.io.load_tensormap_to_torch(\n",
    "    os.path.join(settings[\"io\"][\"data_dir\"], \"out_test.npz\"), **settings[\"torch\"]\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we iterate over the exercises and subsets and train models for each subset\n",
    "sequentially."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Runtime for 10 epochs, 2 exercises, 3 subsets:\n",
    "# linear ~ 15 min\n",
    "# nonlinear ~ 20 min\n",
    "import time\n",
    "\n",
    "from rholearn.pretraining import load_training_objects\n",
    "from rholearn.training import train\n",
    "\n",
    "exercises = [0]\n",
    "subsets = [0]\n",
    "\n",
    "for exercise in exercises:\n",
    "    for subset in subsets:\n",
    "\n",
    "        # Start timer\n",
    "        t0 = time.time()\n",
    "\n",
    "        # Define the training subdirectory\n",
    "        train_dir = os.path.join(\n",
    "            settings[\"io\"][\"run_dir\"], f\"exercise_{exercise}\", f\"subset_{subset}\"\n",
    "        )\n",
    "\n",
    "        # Load training data and torch objects\n",
    "        in_train, out_train, model, loss_fn, optimizer = load_training_objects(\n",
    "            settings, exercise, subset, settings[\"training\"][\"restart_epoch\"]\n",
    "        )\n",
    "\n",
    "        # Execute model training\n",
    "        print(f\"\\nTraining in subdirectory {train_dir}\")\n",
    "        train(\n",
    "            in_train=in_train,\n",
    "            out_train=out_train,\n",
    "            in_test=in_test,\n",
    "            out_test=out_test,\n",
    "            model=model,\n",
    "            loss_fn=loss_fn,\n",
    "            optimizer=optimizer,\n",
    "            n_epochs=settings[\"training\"][\"n_epochs\"],\n",
    "            save_interval=settings[\"training\"][\"save_interval\"],\n",
    "            save_dir=train_dir,\n",
    "            restart=settings[\"training\"][\"restart_epoch\"],\n",
    "        )\n",
    "\n",
    "        # Report on timings\n",
    "        dt = time.time() - t0\n",
    "        num_epochs_run = (\n",
    "            settings[\"training\"][\"n_epochs\"]\n",
    "            if settings[\"training\"][\"restart_epoch\"] is None\n",
    "            else settings[\"training\"][\"n_epochs\"]\n",
    "            - settings[\"training\"][\"restart_epoch\"]\n",
    "        )\n",
    "        msg = (\n",
    "            f\"\\nTraining finished in {np.round(dt, 2)} s = {np.round(dt / num_epochs_run, 2)} s per epoch\"\n",
    "            + f\"\\n(Timed over {num_epochs_run} epochs, perhaps since restart)\"\n",
    "        )\n",
    "        print(msg)\n",
    "        with open(os.path.join(train_dir, \"log.txt\"), \"a+\") as log:\n",
    "            log.write(msg)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a Nonlinear Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use the same notebook to train a nonlinear model, by changing only a\n",
    "few lines of code. Make the following changes in this section of the notebook\n",
    "'Training a Linear Model:\n",
    "\n",
    "In the ``settings`` dict:\n",
    "\n",
    "1. In the nested dict under key ``\"io\"``, change the run directory to \"02_nonlinear\":\n",
    "\n",
    "    ``\"run_dir\": os.path.join(run_dir, \"02_nonlinear\"),``\n",
    "\n",
    "\n",
    "2. Under key ``\"model\"``, change the type to \"nonlinear\" and uncomment the neural\n",
    "  network args:\n",
    "  \n",
    "  \n",
    "        ```\n",
    "        \"type\": \"linear\",\n",
    "            \"args\": {\n",
    "                \"hidden_layer_widths\": [32, 32, 32],\n",
    "                \"activation_fn\": \"SiLU\"\n",
    "            },\n",
    "        ```\n",
    "\n",
    "Then run the subsequent cells again, in order."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rho",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "576c71a426691bc103e620abf31b98f592c88b3903fdf6bf41ae71c4b8043fe6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
