{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# How to Train Your Model (Live Demo #1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "![M-stack ecosystem](../figures/m_stack_ecosystem.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1. Import M-stack packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Useful standard and scientific ML libraries\n",
    "import os\n",
    "import time\n",
    "import ase.io\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pyscf\n",
    "import py3Dmol\n",
    "import torch\n",
    "\n",
    "# M-Stack packages\n",
    "import equistore   # storage format for atomistic ML\n",
    "import chemiscope  # interactive molecular visualization\n",
    "import rascaline   # generating structural representations\n",
    "import qstack      # quantum chemistry toolkit\n",
    "\n",
    "from equistore import Labels, TensorBlock, TensorMap\n",
    "\n",
    "# Torch-based density leaning\n",
    "from rholearn import io, data, features, loss, models, plots, training, utils\n",
    "from settings import RASCAL_HYPERS, DATA_SETTINGS, ML_SETTINGS"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2. Visualize and explore dataset: `chemiscope`\n",
    "\n",
    "* `chemiscope` is an interactive structure and property viewer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Read the water molecules from file\n",
    "frames = ase.io.read(os.path.join(DATA_SETTINGS[\"data_dir\"], \"water_monomers_1k.xyz\"), index=\":\")\n",
    "structure_idxs = np.arange(len(frames))\n",
    "# structure_idxs\n",
    "\n",
    "# Display molecules with chemiscope\n",
    "# chemiscope.show(\n",
    "#     frames,\n",
    "#     properties={\n",
    "#         \"Mean O-H bond length, Angstrom\": [np.mean([f.get_distance(0, 1), f.get_distance(0, 2)]) for f in frames],\n",
    "#         \"H-O-H angle, degrees\": [f.get_angle(1, 0, 2) for f in frames],\n",
    "#     },\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the grouped indices for train/test(/val) splits\n",
    "train_idxs, test_idxs, val_idxs = data.group_idxs(\n",
    "    structure_idxs,\n",
    "    n_groups=DATA_SETTINGS[\"n_groups\"],\n",
    "    group_sizes=DATA_SETTINGS[\"group_sizes\"],\n",
    "    shuffle=DATA_SETTINGS[\"shuffle\"],\n",
    "    seed=DATA_SETTINGS[\"seed\"],\n",
    ")\n",
    "\n",
    "# Define new dir for storing standardized features\n",
    "rho_std_dir = os.path.join(DATA_SETTINGS[\"data_dir\"], \"rho_std\")\n",
    "if not os.path.exists(rho_std_dir):\n",
    "    os.mkdir(rho_std_dir)\n",
    "\n",
    "np.savez(\n",
    "    os.path.join(DATA_SETTINGS[\"data_dir\"], \"rho\", \"grouped_idxs.npz\"),\n",
    "    train_idxs=train_idxs,\n",
    "    test_idxs=test_idxs,\n",
    "    val_idxs=val_idxs,\n",
    ")\n",
    "np.savez(\n",
    "    os.path.join(rho_std_dir, \"grouped_idxs.npz\"),\n",
    "    train_idxs=train_idxs,\n",
    "    test_idxs=test_idxs,\n",
    "    val_idxs=val_idxs,\n",
    ")\n",
    "\n",
    "# Load the training set and join into a single TensorMap\n",
    "c_list = [\n",
    "    equistore.load(os.path.join(DATA_SETTINGS[\"data_dir\"], \"rho\", f\"{i}\", \"c.npz\"))\n",
    "    for i in train_idxs\n",
    "]\n",
    "c_train = equistore.join(c_list, axis=\"samples\")\n",
    "\n",
    "# Get the invariant means\n",
    "inv_means = features.get_invariant_means(c_train)\n",
    "equistore.save(os.path.join(DATA_SETTINGS[\"data_dir\"], \"rho\", \"inv_means.npz\"), inv_means)\n",
    "equistore.save(os.path.join(rho_std_dir, \"inv_means.npz\"), inv_means)\n",
    "\n",
    "# Standardize the invariants\n",
    "for A in range(1000):\n",
    "    c = equistore.load(os.path.join(DATA_SETTINGS[\"data_dir\"], \"rho\", f\"{A}\", \"c.npz\"))\n",
    "    c_std = features.standardize_invariants(c, inv_means)\n",
    "    c_dir = os.path.join(rho_std_dir, f\"{A}\")\n",
    "    if not os.path.exists(c_dir):\n",
    "        os.mkdir(c_dir)\n",
    "    equistore.save(os.path.join(c_dir, \"c.npz\"), c_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build density dataset\n",
    "rho_data = data.RhoData(\n",
    "    structure_idxs,\n",
    "    input_dir=os.path.join(DATA_SETTINGS[\"data_dir\"], \"lsoap\"),\n",
    "    output_dir=os.path.join(DATA_SETTINGS[\"data_dir\"], \"rho_std\"),\n",
    "    overlap_dir=os.path.join(DATA_SETTINGS[\"data_dir\"], \"rho\"),\n",
    "    **ML_SETTINGS[\"torch\"],\n",
    ")\n",
    "\n",
    "# Build the train and test dataloaders\n",
    "train_loader = data.RhoLoader(\n",
    "    rho_data, subset_idxs=train_idxs, **ML_SETTINGS[\"loading\"]\n",
    ")\n",
    "test_loader = data.RhoLoader(rho_data, subset_idxs=test_idxs, **ML_SETTINGS[\"loading\"])\n",
    "\n",
    "# Load a dummy batch for initializing the model\n",
    "a_list, x_list, c_list, s_list = next(iter(train_loader))\n",
    "\n",
    "# Initialize model\n",
    "keys = x_list[0].keys\n",
    "model = models.RhoModel(\n",
    "    model_type=ML_SETTINGS[\"model\"][\"type\"],\n",
    "    keys=keys,\n",
    "    in_features=[x_list[0][key].properties for key in keys],\n",
    "    out_features=[c_list[0][key].properties for key in keys],\n",
    "    **ML_SETTINGS[\"model\"][\"args\"],\n",
    ")\n",
    "\n",
    "# Initialize optimizer and scheduler\n",
    "optimizer = ML_SETTINGS[\"optimizer\"][\"algorithm\"](\n",
    "    params=model.parameters(),\n",
    "    **ML_SETTINGS[\"optimizer\"][\"args\"],\n",
    ")\n",
    "scheduler = torch.optim.lr_scheduler.MultiStepLR(\n",
    "    optimizer=optimizer,\n",
    "    milestones=np.array([3, 6]),\n",
    "    gamma=0.1,\n",
    ")\n",
    "\n",
    "# Initialize loss function\n",
    "rho_loss_fn = loss.RhoLoss()\n",
    "coeff_loss_fn = loss.CoeffLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "io.log(\"log.txt\", \"# Start training\")\n",
    "\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "for epoch in range(ML_SETTINGS[\"training\"][\"n_epochs\"]):\n",
    "\n",
    "    io.log(\"log.txt\", f\"epoch {epoch}\")\n",
    "    t0 = time.time()\n",
    "\n",
    "    # Set some epoch-dependent settings\n",
    "    check_args = True if epoch == 0 else False\n",
    "    use_rho_loss = True if epoch > 5 else False\n",
    "\n",
    "    # Iterate over training batches\n",
    "    for train_batch in train_loader:\n",
    "\n",
    "        # Reset gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Make a prediction and evaluate loss for each train structure in the batch\n",
    "        train_loss = 0\n",
    "        for a_train, x_train, c_train, s_train in zip(*train_batch):\n",
    "\n",
    "            # Make a prediction\n",
    "            c_train_pred = model(x_train, check_args=check_args)\n",
    "\n",
    "            # Evaluate the loss with either CoeffLoss or RhoLoss\n",
    "            if use_rho_loss:\n",
    "                train_loss += rho_loss_fn(c_train_pred, c_train, s_train, check_args=check_args)\n",
    "            else:  # use CoeffLoss\n",
    "                train_loss += coeff_loss_fn(c_train_pred, c_train, check_args=check_args)\n",
    "\n",
    "        # Calculate gradient and update parameters\n",
    "        train_loss.backward(retain_graph=True)\n",
    "        optimizer.step()\n",
    "\n",
    "        # Store training loss\n",
    "        train_losses.append(train_loss.detach().numpy())\n",
    "\n",
    "    # Iterate over test batches\n",
    "    for test_batch in test_loader:\n",
    "\n",
    "        # Make a prediction and evaluate loss for each test structure in the batch\n",
    "        test_loss = 0\n",
    "        for a_test, x_test, c_test, s_test in zip(*test_batch):\n",
    "\n",
    "            # Make a prediction and evaluate the los *on the density*\n",
    "            with torch.no_grad():\n",
    "                c_test_pred = model(x_test, check_args=check_args)\n",
    "                test_loss += rho_loss_fn(c_test_pred, c_test, s_test, check_args=check_args)\n",
    "\n",
    "        test_losses.append(test_loss.detach().numpy())\n",
    "\n",
    "    # Write log for the epoch\n",
    "    io.log(\n",
    "        \"log.txt\",\n",
    "        f\"  learning_on {'rho' if use_rho_loss else 'coeff'}\"\n",
    "        f\"  train_loss {np.round(train_losses[-1], 5)}\"\n",
    "        f\"  test_loss {np.round(test_losses[-1], 5)}\"\n",
    "        f\"  lr {np.round(scheduler.get_last_lr()[0], 3)}\"\n",
    "        f\"  time {time.time() - t0}\",\n",
    "    )\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the points on the x-axis - these are different for train and test as\n",
    "# the number of batches per epoch is different\n",
    "x_epoch_train = ML_SETTINGS[\"training\"][\"n_epochs\"] * np.linspace(0, 1, len(train_losses))\n",
    "x_epoch_test = ML_SETTINGS[\"training\"][\"n_epochs\"] * np.linspace(0, 1, len(test_losses))\n",
    "\n",
    "# Plot train and test loss versus epoch\n",
    "fig, ax = plt.subplots()\n",
    "ax.loglog(x_epoch_train, np.array(train_losses) / ML_SETTINGS[\"loading\"][\"batch_size\"], label=\"train\")\n",
    "ax.loglog(x_epoch_test, np.array(test_losses) / ML_SETTINGS[\"loading\"][\"batch_size\"], label=\"test\")\n",
    "ax.legend()\n",
    "ax.set_xlabel(\"epoch\")\n",
    "ax.set_ylabel(\"loss per batch\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load reference electron density coefficients: `Q-stack` + `equistore`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 5. Prepare data: `equistore`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Train-test-validation split"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Run Directory"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 6. Train model: `equistore` interfacing with PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the training subdirectory\n",
    "train_rel_dir = \"\"\n",
    "train_run_dir = os.path.join(ML_SETTINGS[\"run_dir\"], train_rel_dir)\n",
    "\n",
    "# Load training data and torch objects\n",
    "data, model, loss_fn, optimizer = pretraining.load_training_objects(\n",
    "    train_rel_dir, DATA_SETTINGS[\"data_dir\"], ML_SETTINGS, ML_SETTINGS[\"training\"][\"restart_epoch\"]\n",
    ")\n",
    "\n",
    "# Unpack the data\n",
    "in_train, in_test, out_train, out_test = data\n",
    "\n",
    "# Execute model training\n",
    "print(f\"\\nTraining in subdirectory:\\n\\n{train_run_dir}\\n\")\n",
    "training.train(\n",
    "    in_train=in_train,\n",
    "    out_train=out_train,\n",
    "    in_test=in_test,\n",
    "    out_test=out_test,\n",
    "    model=model,\n",
    "    loss_fn=loss_fn,\n",
    "    optimizer=optimizer,\n",
    "    n_epochs=ML_SETTINGS[\"training\"][\"n_epochs\"],\n",
    "    save_interval=ML_SETTINGS[\"training\"][\"save_interval\"],\n",
    "    save_dir=train_run_dir,\n",
    "    restart=ML_SETTINGS[\"training\"][\"restart_epoch\"],\n",
    "    print_level=1,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss vs epoch plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the train and test losses\n",
    "losses = np.load(os.path.join(ML_SETTINGS[\"run_dir\"], \"losses.npz\"))\n",
    "\n",
    "# Plot losses\n",
    "fig, ax = plt.subplots(1, 1, sharey=True)\n",
    "ax.loglog(\n",
    "    losses[\"train\"] / 500, \n",
    "    label=\"linear, train\", \n",
    "    color=\"blue\",\n",
    ")\n",
    "ax.loglog(\n",
    "    losses[\"test\"] / 300, \n",
    "    label=\"linear, test\", \n",
    "    color=\"blue\",\n",
    "    linestyle=\"dashed\"\n",
    ")\n",
    "ax.set_xlabel(\"Epoch\")\n",
    "ax.set_ylabel(\"MSE Loss per structure\")\n",
    "ax.set_ylim(1e-5, 1e-2)\n",
    "ax.legend()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Make a prediction on the validation structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the input and output validation TensorMaps\n",
    "in_val = io.load_tensormap_to_torch(\n",
    "    os.path.join(DATA_SETTINGS[\"data_dir\"], \"in_val.npz\"), **ML_SETTINGS[\"torch\"]\n",
    ")\n",
    "out_val = equistore.load(os.path.join(DATA_SETTINGS[\"data_dir\"], \"out_val.npz\"))\n",
    "\n",
    "# Retrieve the unique structure\n",
    "val_idx = equistore.unique_metadata(in_val, axis=\"samples\", names=\"structure\")[0][0]\n",
    "val_frame = ase.io.read(\n",
    "    os.path.join(DATA_SETTINGS[\"data_dir\"], \"water_monomers_1k.xyz\"), index=val_idx\n",
    ")\n",
    "\n",
    "# Build a pyscf Molecule object\n",
    "val_mol = pyscf.gto.Mole().build(\n",
    "    atom=[\n",
    "        (i, j) for i, j in zip(val_frame.get_chemical_symbols(), val_frame.positions)\n",
    "    ],\n",
    "    basis=\"ccpvqz jkfit\",\n",
    ")\n",
    "\n",
    "# Predict the density\n",
    "out_val_pred, coeffs = predictor.predict_density_from_mol(\n",
    "    in_val,\n",
    "    val_mol,\n",
    "    model_path=os.path.join(ML_SETTINGS[\"run_dir\"], \"epoch_10\", \"model.pt\"),\n",
    "    inv_means_path=os.path.join(DATA_SETTINGS[\"data_dir\"], \"inv_means.npz\"),\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parity plot: target vs predicted coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Calculate the MSE Error\n",
    "with torch.no_grad():\n",
    "    val_loss = loss.MSELoss(reduction=\"sum\")(\n",
    "        utils.tensor_to_torch(out_val, **ML_SETTINGS[\"torch\"]), \n",
    "        utils.tensor_to_torch(out_val_pred, **ML_SETTINGS[\"torch\"])\n",
    "    ).detach().numpy()\n",
    "\n",
    "# Plot the target vs predicted coefficients, standardized\n",
    "fig, ax = plots.parity_plot(\n",
    "    target=out_val,\n",
    "    predicted=out_val_pred,\n",
    "    color_by=\"spherical_harmonics_l\",\n",
    ")\n",
    "lim = [-0.05, 0.1]\n",
    "ax.set_xlim(lim)\n",
    "ax.set_ylim(lim)\n",
    "ax.set_aspect(\"equal\")\n",
    "ax.set_xlabel(\"target density coefficient\")\n",
    "ax.set_ylabel(\"predicted density coefficient\")\n",
    "ax.set_title(f\"Validation MSE Error: {round(val_loss * 1e6, 3)}\"r\" $\\times 10^{-6}$\")\n",
    "ax.legend()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Process densities with `Q-stack` and visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Build a delta density TensorMap\n",
    "out_val_delta = equistore.abs(equistore.subtract(out_val_pred, out_val))\n",
    "\n",
    "# Vectorize the coefficients from each of the TensorMaps\n",
    "new_key_names = [\"spherical_harmonics_l\", \"element\"]\n",
    "vect_coeffs_target = qstack.equio.tensormap_to_vector(\n",
    "    val_mol,\n",
    "    utils.rename_tensor(\n",
    "        utils.drop_metadata_name(out_val, \"samples\", \"structure\"),\n",
    "        keys_names=new_key_names,\n",
    "    ),\n",
    ")\n",
    "vect_coeffs_input = qstack.equio.tensormap_to_vector(\n",
    "    val_mol,\n",
    "    utils.rename_tensor(\n",
    "        utils.drop_metadata_name(out_val_pred, \"samples\", \"structure\"),\n",
    "        keys_names=new_key_names,\n",
    "    ),\n",
    ")\n",
    "vect_coeffs_delta = qstack.equio.tensormap_to_vector(\n",
    "    val_mol,\n",
    "    utils.rename_tensor(\n",
    "        utils.drop_metadata_name(out_val_delta, \"samples\", \"structure\"),\n",
    "        keys_names=new_key_names,\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Convert the basis function coefficients to a cube file\n",
    "plot_dir = os.path.join(ML_SETTINGS[\"run_dir\"], \"plots\")\n",
    "io.check_or_create_dir(plot_dir)\n",
    "n = 60  # grid points per dimension\n",
    "for (coeffs, filename) in [\n",
    "    (vect_coeffs_target, \"out_val.cube\"),\n",
    "    (vect_coeffs_input, \"out_val_pred.cube\"),\n",
    "    (vect_coeffs_delta, \"out_val_delta.cube\"),\n",
    "]:\n",
    "    qstack.fields.density2file.coeffs_to_cube(\n",
    "        val_mol,\n",
    "        coeffs,\n",
    "        os.path.join(plot_dir, filename),\n",
    "        nx=n,\n",
    "        ny=n,\n",
    "        nz=n,\n",
    "        resolution=None,\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Predicted electron density"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Visualize the predicted density\n",
    "v = py3Dmol.view()\n",
    "v.addModelsAsFrames(open(os.path.join(plot_dir, \"out_val_pred.cube\"), \"r\").read(), \"cube\")\n",
    "v.setStyle({\"stick\": {}})\n",
    "v.addVolumetricData(\n",
    "    open(os.path.join(plot_dir, \"out_val_pred.cube\"), \"r\").read(),\n",
    "    \"cube\",\n",
    "    {\"isoval\": 0.05, \"color\": \"blue\", \"opacity\": 0.8},\n",
    ")\n",
    "v.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \"Delta electron density\" - i.e. the ML error (100x magnification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Visualize the delta density\n",
    "v = py3Dmol.view()\n",
    "v.addModelsAsFrames(open(os.path.join(plot_dir, \"out_val_delta.cube\"), \"r\").read(), \"cube\")\n",
    "v.setStyle({\"stick\": {}})\n",
    "v.addVolumetricData(\n",
    "    open(os.path.join(plot_dir, \"out_val_delta.cube\"), \"r\").read(),\n",
    "    \"cube\",\n",
    "    {\"isoval\": 0.0005, \"color\": \"blue\", \"opacity\": 0.8},\n",
    ")\n",
    "v.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra Material"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* DIY session at poster #14\n",
    "\n",
    "* Torch-based electron density learning at https://github.com/m-stack-org/rho_learn ...\n",
    "\n",
    "* ... with examples/tutorials for:\n",
    "\n",
    "    * water\n",
    "    \n",
    "    * azoswitch molecules\n",
    "    \n",
    "    \n",
    "![azoswitch density](../figures/azoswitch_density.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rho",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
