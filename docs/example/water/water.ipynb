{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import ase.io\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "import chemiscope  # interactive molecular visualization\n",
    "import equistore   # storage format for atomistic ML\n",
    "import qstack      # quantum chemistry toolkit\n",
    "import rascaline   # generating structural representations\n",
    "import rholearn    # torch-based density leaning\n",
    "\n",
    "RHOLEARN_DIR = \"/Users/joe.abbott/Documents/phd/code/rho/rho_learn/\"  # for example\n",
    "data_dir = os.path.join(RHOLEARN_DIR, \"docs/example/water/data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the water molecules from file\n",
    "n_structures = 1000\n",
    "frames = ase.io.read(\n",
    "    os.path.join(data_dir, \"water_monomers_1k.xyz\"), index=f\":{n_structures}\"\n",
    ")\n",
    "\n",
    "# Turn off periodic boundary conditions\n",
    "for f in frames:\n",
    "    f.set_pbc(False)\n",
    "\n",
    "# Display molecules with chemiscope\n",
    "cs = chemiscope.show(\n",
    "    frames,\n",
    "    mode=\"default\",\n",
    "    properties={\n",
    "        \"Mean O-H bond length, Angstrom\": [np.mean([f.get_distance(0, 1), f.get_distance(0, 2)]) for f in frames],\n",
    "        \"H-O-H angle, degrees\": [f.get_angle(1, 0, 2) for f in frames],\n",
    "    },\n",
    ")\n",
    "display(cs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import equistore.io\n",
    "from equistore import Labels\n",
    "\n",
    "from rholearn import features, utils\n",
    "\n",
    "rascal_hypers = {\n",
    "    \"cutoff\": 3.0,  # Angstrom\n",
    "    \"max_radial\": 6,  # Exclusive\n",
    "    \"max_angular\": 5,  # Inclusive\n",
    "    \"atomic_gaussian_width\": 0.2,\n",
    "    \"radial_basis\": {\"Gto\": {}},\n",
    "    \"cutoff_function\": {\"ShiftedCosine\": {\"width\": 0.5}},\n",
    "    \"center_atom_weight\": 1.0,\n",
    "}\n",
    "\n",
    "# Compute lambda-SOAP: uses rascaline to compute a SphericalExpansion\n",
    "# Runtime approx 25 seconds\n",
    "input = features.lambda_soap_vector(\n",
    "    frames, rascal_hypers, even_parity_only=True\n",
    ")\n",
    "\n",
    "# Drop the block for l=5, Hydrogen as this isn't included in the output electron density\n",
    "input = utils.drop_blocks(input, keys=Labels(input.keys.names, np.array([[5, 1]])))\n",
    "\n",
    "# Load the electron density data\n",
    "output = equistore.io.load(os.path.join(data_dir, \"e_densities.npz\"))\n",
    "\n",
    "# Check that the metadata of input and output match along the samples and components axes\n",
    "assert utils.equal_metadata(input, output, check=[\"samples\", \"components\"])\n",
    "\n",
    "# Save lambda-SOAP descriptor to file\n",
    "equistore.io.save(os.path.join(data_dir, \"lambda_soap.npz\"), input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import equistore.io\n",
    "\n",
    "# Load lambda-SOAP descriptor from file\n",
    "input = equistore.io.load(os.path.join(data_dir, \"lambda_soap.npz\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rholearn import pretraining\n",
    "\n",
    "# Define setting for the data partitioning\n",
    "settings = {\n",
    "    \"io\": {\n",
    "        \"input\": os.path.join(data_dir, \"lambda_soap.npz\"),\n",
    "        \"output\": os.path.join(data_dir, \"e_densities.npz\"),\n",
    "        \"data_dir\": os.path.join(data_dir, \"partitions\"),\n",
    "    },\n",
    "    \"numpy\": {\n",
    "        \"random_seed\": 10,\n",
    "    },\n",
    "    \"train_test_split\": {\n",
    "        \"axis\": \"samples\",\n",
    "        \"names\": [\"structure\"],\n",
    "        \"n_groups\": 3,\n",
    "        \"group_sizes\": [0.7, 0.2, 0.1],\n",
    "    },\n",
    "    \"data_partitions\": {\n",
    "        \"n_exercises\": 3,\n",
    "        \"n_subsets\": 4,\n",
    "    },\n",
    "}\n",
    "# Partition the data\n",
    "pretraining.partition_data(settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rholearn import io, pretraining\n",
    "\n",
    "run_dir = os.path.join(RHOLEARN_DIR, \"docs/example/water/runs\")\n",
    "io.check_or_create_dir(run_dir)\n",
    "\n",
    "settings = {\n",
    "    \"io\": {\n",
    "        \"data_dir\": os.path.join(data_dir, \"partitions\"),\n",
    "        \"run_dir\": os.path.join(run_dir, \"02_linear_std\"),\n",
    "    },\n",
    "    \"data_partitions\": {\n",
    "        \"n_exercises\": 3,\n",
    "        \"n_subsets\": 4,\n",
    "    },\n",
    "    \"torch\": {\n",
    "        \"requires_grad\": True,  # needed to track gradients\n",
    "        \"dtype\": torch.float64,  # recommended\n",
    "        \"device\": torch.device(\"cpu\"),  # which device to load tensors to\n",
    "    },\n",
    "    \"model\": {\n",
    "        \"type\": \"linear\",  # linear or nonlinear\n",
    "        \"args\": {\n",
    "            # \"hidden_layer_widths\": [16, 16, 16],\n",
    "            # \"activation_fn\": \"SiLU\"\n",
    "        },\n",
    "    },\n",
    "    \"optimizer\": {\n",
    "        \"algorithm\": torch.optim.LBFGS,\n",
    "        \"args\": {\n",
    "            \"lr\": 0.25,\n",
    "        },\n",
    "    },\n",
    "    \"loss\": {\n",
    "        \"fn\": \"MSELoss\",  # CoulombLoss or MSELoss\n",
    "        \"args\": {\n",
    "            \"reduction\": \"sum\",  # reduction can be used with MSELoss\n",
    "        },\n",
    "    },\n",
    "    \"training\": {\n",
    "        \"n_epochs\": 100,  # number of total epochs to run \n",
    "        \"save_interval\": 10,  # save model and optimizer state every x intervals\n",
    "        \"restart_epoch\": None,  # None, or the epoch checkpoint number if restarting\n",
    "        \"standardize_invariant_features\": True, \n",
    "    },\n",
    "}\n",
    "\n",
    "# IMPORTANT! - set the torch default dtype\n",
    "torch.set_default_dtype(settings[\"torch\"][\"dtype\"])\n",
    "\n",
    "# Construct the appropriate torch objects (i.e. models, loss fxns) prior to training\n",
    "pretraining.construct_torch_objects(settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the exercises and subsets to train\n",
    "exercises = [0]\n",
    "subsets = [0, 1, 2, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from rholearn import training, utils\n",
    "\n",
    "for exercise in exercises:\n",
    "    for subset in subsets:\n",
    "\n",
    "        # Start timer\n",
    "        t0 = time.time()\n",
    "\n",
    "        # Define the training subdirectory\n",
    "        train_dir = os.path.join(\n",
    "            settings[\"io\"][\"run_dir\"], f\"exercise_{exercise}\", f\"subset_{subset}\"\n",
    "        )\n",
    "\n",
    "        # Load training data and torch objects\n",
    "        data, model, loss_fn, optimizer = pretraining.load_training_objects(\n",
    "            settings, exercise, subset, settings[\"training\"][\"restart_epoch\"]\n",
    "        )\n",
    "\n",
    "        # Unpack the data\n",
    "        in_train, in_test, out_train, out_test = data\n",
    "\n",
    "        # Execute model training\n",
    "        print(f\"\\nTraining in subdirectory {train_dir}\")\n",
    "        training.train(\n",
    "            in_train=in_train,\n",
    "            out_train=out_train,\n",
    "            in_test=in_test,\n",
    "            out_test=out_test,\n",
    "            model=model,\n",
    "            loss_fn=loss_fn,\n",
    "            optimizer=optimizer,\n",
    "            n_epochs=settings[\"training\"][\"n_epochs\"],\n",
    "            save_interval=settings[\"training\"][\"save_interval\"],\n",
    "            save_dir=train_dir,\n",
    "            restart=settings[\"training\"][\"restart_epoch\"],\n",
    "        )\n",
    "\n",
    "        # Report on timings\n",
    "        dt = time.time() - t0\n",
    "        num_epochs_run = (\n",
    "            settings[\"training\"][\"n_epochs\"]\n",
    "            if settings[\"training\"][\"restart_epoch\"] is None\n",
    "            else settings[\"training\"][\"n_epochs\"]\n",
    "            - settings[\"training\"][\"restart_epoch\"]\n",
    "        )\n",
    "        msg = (\n",
    "            f\"\\nTraining finished in {np.round(dt, 2)} s = {np.round(dt / num_epochs_run, 2)} s per epoch\"\n",
    "            + f\"\\n(Timed over {num_epochs_run} epochs, perhaps since restart)\"\n",
    "        )\n",
    "        print(msg)\n",
    "        with open(os.path.join(train_dir, \"log.txt\"), \"a+\") as log:\n",
    "            log.write(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from rholearn import analysis, plots\n",
    "\n",
    "run_dir_1 = os.path.join(RHOLEARN_DIR, \"docs/example/water/runs\", \"01_linear\")\n",
    "run_dir_2 = os.path.join(RHOLEARN_DIR, \"docs/example/water/runs\", \"02_linear_std\")\n",
    "\n",
    "plot_dir_1 = os.path.join(run_dir_1, \"plots\")\n",
    "plot_dir_2 = os.path.join(run_dir_2, \"plots\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile data - linear\n",
    "train_1, test_1 = analysis.compile_loss_data(run_dir_1, exercises, subsets)\n",
    "mean_train_1 = analysis.average_losses(train_1)\n",
    "mean_test_1 = analysis.average_losses(test_1)\n",
    "\n",
    "# compile data - nonlinear\n",
    "train_2, test_2 = analysis.compile_loss_data(run_dir_2, exercises, subsets)\n",
    "mean_train_2 = analysis.average_losses(train_2)\n",
    "mean_test_2 = analysis.average_losses(test_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log-log plot of loss vs epoch\n",
    "fig, ax = plots.loss_vs_epoch(\n",
    "    [mean_train_1[3], mean_test_1[3], mean_train_2[3], mean_test_2[3]],\n",
    "    sharey=True,\n",
    "    mutliple_traces=False,\n",
    ")\n",
    "\n",
    "# Format\n",
    "fig.tight_layout()\n",
    "fig.set_figheight(7)\n",
    "fig.set_figwidth(12)\n",
    "ax[0].legend(labels=[f\"subset {s}\" for s in np.sort(list(mean_test_1.keys()))])\n",
    "ax[0].set_ylabel(r\"train loss (MSE)\")\n",
    "ax[1].set_ylabel(r\"test loss (MSE)\")\n",
    "\n",
    "\n",
    "# Save\n",
    "# plots.save_fig_mpltex(fig, os.path.join(plot_dir, \"loss_vs_epoch\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log-log learning curve plot of loss vs training set size\n",
    "point = \"final\"  # take the \"final\" or \"best\" epoch loss\n",
    "fig, ax = plots.learning_curve(\n",
    "    [mean_train_1, mean_test_1, mean_train_2, mean_test_2],\n",
    "    np.load(os.path.join(settings[\"io\"][\"data_dir\"], \"subset_sizes_train.npy\")),\n",
    "    point=point,\n",
    ")\n",
    "\n",
    "# Format\n",
    "fig.tight_layout()\n",
    "ax.set_ylabel(point + r\" loss\")\n",
    "ax.legend(labels=[\"train_1\", \"test_2\", \"train_2\", \"test_2\"])\n",
    "\n",
    "# Save\n",
    "# plots.save_fig_mpltex(fig, os.path.join(plot_dir, \"learning_curve\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rho",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "576c71a426691bc103e620abf31b98f592c88b3903fdf6bf41ae71c4b8043fe6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
