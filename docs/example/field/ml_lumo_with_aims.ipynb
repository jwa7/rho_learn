{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning a custom scalar field with `rho_learn`/`FHI-aims`: LUMO of water\n",
    "\n",
    "**NOTE**: this notebook has only been tested on the HPC system \"jed\". Due to\n",
    "specific compilation of the quantum chemistry package `FHI-aims`, the Python\n",
    "interface to running QC calculations may not be generalized to other operating\n",
    "systems or hardware."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import shutil\n",
    "import time\n",
    "from functools import partial\n",
    "\n",
    "# Useful standard and scientific ML libraries\n",
    "import ase.io\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "# import py3Dmol\n",
    "import torch\n",
    "\n",
    "# M-Stack packages\n",
    "import metatensor   # storage format for atomistic ML\n",
    "import chemiscope  # interactive molecular visualization\n",
    "import rascaline   # generating structural representations\n",
    "from metatensor import Labels, TensorBlock, TensorMap\n",
    "from rascaline.utils import clebsch_gordan\n",
    "\n",
    "# Interfacing with FHI-aims\n",
    "from rhocalc.aims import aims_calc, aims_parser\n",
    "\n",
    "# Torch-based density leaning\n",
    "from rholearn import io, data, loss, models, predictor\n",
    "import settings"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Visualize structures in dataset\n",
    "\n",
    "* Use `chemiscope`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from settings import TOP_DIR, DATA_DIR, ML_DIR, DATA_SETTINGS\n",
    "\n",
    "print(\"Top directory defined as: \", TOP_DIR)\n",
    "\n",
    "# Load the frames in the complete dataset\n",
    "all_frames = DATA_SETTINGS[\"all_frames\"]\n",
    "\n",
    "# Shuffle the total set of structure indices\n",
    "idxs = np.arange(len(all_frames))\n",
    "np.random.default_rng(seed=DATA_SETTINGS[\"seed\"]).shuffle(idxs)\n",
    "\n",
    "# Take a subset of the frames if desired\n",
    "idxs = idxs[:DATA_SETTINGS[\"n_frames\"]]\n",
    "frames = [all_frames[A] for A in idxs]\n",
    "\n",
    "# Set the ri restart and rebuild indices\n",
    "ri_restart_idx = 0\n",
    "ri_rebuild_idx = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chemiscope.show(\n",
    "    frames,\n",
    "    properties={\n",
    "        \"Mean O-H bond length, Angstrom\": [np.mean([f.get_distance(0, 1), f.get_distance(0, 2)]) for f in frames],\n",
    "        \"H-O-H angle, degrees\": [f.get_angle(1, 0, 2) for f in frames],\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate learning targets\n",
    "\n",
    "* These are the RI coefficients of the HOMO eigenstate\n",
    "* We generate these in 2 steps: 1) SCF and 2) RI fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A callable that takes structure idx as an argument, returns path to AIMS SCF\n",
    "# output data\n",
    "def scf_dir(A):\n",
    "    return os.path.join(DATA_DIR, f\"{A}\")\n",
    "\n",
    "# A callable that takes structure idx as an argument, returns path to AIMS RI\n",
    "# output data\n",
    "def ri_dir(A, restart_idx):\n",
    "    return os.path.join(scf_dir(A), f\"{restart_idx}\")\n",
    "\n",
    "# A callable that takes structure idx as an argument, returns path to processed\n",
    "# data (i.e. metatensor-format)\n",
    "def processed_dir(A, restart_idx):\n",
    "    return os.path.join(ri_dir(A, restart_idx), \"processed\")\n",
    "\n",
    "# Define callable for saving predictions made during runtime\n",
    "def eval_dir(A, epoch):\n",
    "    return os.path.join(ML_DIR, \"evaluation\", f\"epoch_{epoch}\", f\"{A}\")\n",
    "\n",
    "# Create dirs\n",
    "if not os.path.exists(DATA_DIR):\n",
    "    os.makedirs(DATA_DIR)\n",
    "if not os.path.exists(ML_DIR):\n",
    "    os.makedirs(ML_DIR)\n",
    "if not os.path.exists(os.path.join(ML_DIR, \"evaluation\")):\n",
    "    os.makedirs(os.path.join(ML_DIR, \"evaluation\"))\n",
    "\n",
    "chkpt_dir = os.path.join(ML_DIR, \"checkpoints\")\n",
    "if not os.path.exists(chkpt_dir):\n",
    "    os.makedirs(chkpt_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Converge SCF for each structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the settings needed to run FHI-aims\n",
    "from settings import AIMS_PATH, BASE_AIMS_KWARGS, SCF_KWARGS, RI_KWARGS, SBATCH_KWARGS\n",
    "\n",
    "# Build a dict of settings for each calculation (i.e. structure)\n",
    "# IMPORTANT: zip() is used to pair up the structure index and the structure\n",
    "calcs = {\n",
    "    A: {\"atoms\": frame, \"run_dir\": scf_dir(A)} for A, frame in zip(idxs, frames)\n",
    "}\n",
    "\n",
    "# And the general settings for all calcs\n",
    "aims_kwargs = BASE_AIMS_KWARGS.copy()\n",
    "aims_kwargs.update(SCF_KWARGS)\n",
    "\n",
    "# Define paths to the aims.out files for RI calcs\n",
    "all_aims_outs = [os.path.join(scf_dir(A), \"aims.out\") for A in idxs]\n",
    "for aims_out in all_aims_outs:\n",
    "    if os.path.exists(aims_out):\n",
    "        os.remove(aims_out)\n",
    "\n",
    "# Run the SCF in AIMS\n",
    "aims_calc.run_aims_array(\n",
    "    calcs=calcs,\n",
    "    aims_path=AIMS_PATH,\n",
    "    aims_kwargs=aims_kwargs,\n",
    "    sbatch_kwargs=SBATCH_KWARGS,\n",
    "    run_dir=scf_dir,\n",
    ")\n",
    "\n",
    "# Wait until all AIMS calcs have finished\n",
    "all_finished = False\n",
    "while len(all_aims_outs) > 0:\n",
    "    for aims_out in all_aims_outs:\n",
    "        if os.path.exists(aims_out):\n",
    "            with open(aims_out, \"r\") as f:\n",
    "                # Basic check to see if AIMS calc has finished\n",
    "                if \"Leaving FHI-aims.\" in f.read():\n",
    "                    all_aims_outs.remove(aims_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Perform RI fitting on the scalar field of interest (LUMO)\n",
    "\n",
    "* First we need to identify the LUMO\n",
    "\n",
    "* This can be done by parsing the Kohn Sham orbital information from the\n",
    "  converged SCF calculation. \n",
    "  \n",
    "* This info has been written to file \"ks_orbital_info.out\" above by using the\n",
    "  `ri_fit_write_ks_orb_info: True` keyword set in `SCF_KWARGS`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write KSO weights for just the HOMO to file\n",
    "for A in idxs:\n",
    "    # Parse the Kohn-Sham\n",
    "    ks_info = aims_parser.get_ks_orbital_info(\n",
    "        os.path.join(scf_dir(A), \"ks_orbital_info.out\")\n",
    "    )\n",
    "    weights = np.zeros(ks_info.shape[0])\n",
    "    # kso_idxs = aims_parser.find_homo_kso_idxs(ks_info)  # HOMO\n",
    "    kso_idxs = aims_parser.find_lumo_kso_idxs(ks_info)  # LUMO\n",
    "\n",
    "    # Add a weighting of 1.0\n",
    "    for kso_idx in kso_idxs:  # these are 1-indexed in AIMS\n",
    "        weights[kso_idx - 1] = 1.0\n",
    "\n",
    "    # Save these to file so AIMS can read them in\n",
    "    np.savetxt(os.path.join(scf_dir(A), \"ks_orbital_weights.in\"), weights)\n",
    "\n",
    "# And the general settings for all calcs\n",
    "aims_kwargs = BASE_AIMS_KWARGS.copy()\n",
    "aims_kwargs.update(RI_KWARGS)\n",
    "\n",
    "# Define paths to the aims.out files for RI calcs\n",
    "all_aims_outs = [os.path.join(ri_dir(A, ri_restart_idx), \"aims.out\") for A in idxs]\n",
    "for aims_out in all_aims_outs:\n",
    "    if os.path.exists(aims_out):\n",
    "        os.remove(aims_out)\n",
    "\n",
    "# Copy restart files\n",
    "for A in idxs:\n",
    "    if not os.path.exists(ri_dir(A, ri_restart_idx)):\n",
    "        os.makedirs(ri_dir(A, ri_restart_idx))\n",
    "    for density_matrix in glob.glob(os.path.join(scf_dir(A), \"D*.csc\")):\n",
    "        shutil.copy(density_matrix, ri_dir(A, ri_restart_idx))\n",
    "    shutil.copy(\n",
    "            os.path.join(scf_dir(A), \"ks_orbital_weights.in\"), \n",
    "            ri_dir(A, ri_restart_idx),\n",
    "        )\n",
    "\n",
    "# Run the RI fitting procedure in AIMS\n",
    "aims_calc.run_aims_array(\n",
    "    calcs=calcs,\n",
    "    aims_path=AIMS_PATH,\n",
    "    aims_kwargs=aims_kwargs,\n",
    "    sbatch_kwargs=SBATCH_KWARGS,\n",
    "    run_dir=partial(ri_dir, restart_idx=ri_restart_idx),\n",
    ")\n",
    "\n",
    "# Wait until all AIMS calcs have finished\n",
    "all_finished = False\n",
    "while len(all_aims_outs) > 0:\n",
    "    for aims_out in all_aims_outs:\n",
    "        if os.path.exists(aims_out):\n",
    "            with open(aims_out, \"r\") as f:\n",
    "                # Basic check to see if AIMS calc has finished\n",
    "                if \"Leaving FHI-aims.\" in f.read():\n",
    "                    all_aims_outs.remove(aims_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process AIMS results and print density fitting error\n",
    "# Run in serial: takes approx 0.5 seconds per structure\n",
    "df_errors = []\n",
    "for A, frame in zip(idxs, frames):\n",
    "    aims_parser.process_aims_ri_results(\n",
    "        frame=frame,\n",
    "        aims_output_dir=ri_dir(A, ri_restart_idx),  # includes the restart idx\n",
    "        process_what=[\"coeffs\", \"ovlp\"],\n",
    "        structure_idx=A,\n",
    "    )\n",
    "    calc_info = io.unpickle_dict(os.path.join(processed_dir(A, ri_restart_idx), \"calc_info.pickle\"))\n",
    "    df_errors.append(calc_info[\"df_error_percent\"][\"total\"])\n",
    "print(\"Mean density fitting error across all structures (%):\", np.mean(df_errors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try a rebuild of the density from the target RI coefficients\n",
    "from functools import partial\n",
    "import shutil\n",
    "from settings import REBUILD_KWARGS\n",
    "\n",
    "# And the general settings for all calcs\n",
    "aims_kwargs = BASE_AIMS_KWARGS.copy()\n",
    "aims_kwargs.update(REBUILD_KWARGS)\n",
    "\n",
    "# Define paths to the aims.out files for RI calcs\n",
    "all_aims_outs = [os.path.join(ri_dir(A, ri_rebuild_idx), \"aims.out\") for A in idxs]\n",
    "for aims_out in all_aims_outs:\n",
    "    if os.path.exists(aims_out):\n",
    "        os.remove(aims_out)\n",
    "\n",
    "for A in idxs:\n",
    "    if not os.path.exists(ri_dir(A, ri_rebuild_idx)):\n",
    "        os.makedirs(ri_dir(A, ri_rebuild_idx))\n",
    "\n",
    "    # Copy restart files\n",
    "    shutil.copy(\n",
    "        os.path.join(ri_dir(A, ri_restart_idx), \"ri_coeffs.out\"), \n",
    "        os.path.join(ri_dir(A, ri_rebuild_idx), \"ri_coeffs.in\"),\n",
    "    )\n",
    "\n",
    "# Run the RI fitting procedure in AIMS\n",
    "aims_calc.run_aims_array(\n",
    "    calcs=calcs,\n",
    "    aims_path=AIMS_PATH,\n",
    "    aims_kwargs=aims_kwargs,\n",
    "    sbatch_kwargs=SBATCH_KWARGS,\n",
    "    run_dir=partial(ri_dir, restart_idx=ri_rebuild_idx),\n",
    ")\n",
    "\n",
    "# Wait until all AIMS calcs have finished\n",
    "all_finished = False\n",
    "while len(all_aims_outs) > 0:\n",
    "    for aims_out in all_aims_outs:\n",
    "        if os.path.exists(aims_out):\n",
    "            with open(aims_out, \"r\") as f:\n",
    "                # Basic check to see if AIMS calc has finished\n",
    "                if \"Leaving FHI-aims.\" in f.read():\n",
    "                    all_aims_outs.remove(aims_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from rhocalc.cube.rho_cube import RhoCube\n",
    "\n",
    "# q = RhoCube(\"/home/abbott/rho/rho_learn/docs/example/field/data/89/1/rho_rebuilt.cube\")\n",
    "\n",
    "# x, y, z = q.get_slab_slice(axis=2, center_coord=q.ase_frame.positions[:, 2].max(), thickness=25.0)\n",
    "\n",
    "# # Plot contours\n",
    "# fig, axes = plt.subplots(1, 1, figsize=(5, 5))\n",
    "# cs = axes.contourf(x, y, np.tanh(z.T), cmap='gray')\n",
    "# cbar = fig.colorbar(cs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Structural Descriptors \n",
    "\n",
    "* Here we construct $\\lambda$-SOAP equivariant descriptors for each structure\n",
    "* First, find the angular orders present in the decomposition of the target\n",
    "  scalar field onto the RI basis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The basis set definition should is consistent for all atoms in the dataset,\n",
    "# given consistent AIMS settings. Print this definiton from the parsed outputs\n",
    "# from one of the structures\n",
    "basis_set = io.unpickle_dict(\n",
    "    os.path.join(processed_dir(A, ri_restart_idx), \"calc_info.pickle\")\n",
    ")[\"basis_set\"]\n",
    "\n",
    "print(basis_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from settings import RASCAL_SETTINGS, CG_SETTINGS\n",
    "\n",
    "# Generate a rascaline SphericalExpansion (2 body) representation. As we want to\n",
    "# retain the original structure indices, we are going to pass * all * of the\n",
    "# 1000 frames to rascaline, but only compute for the subset of structures in\n",
    "# `idxs`.\n",
    "calculator = rascaline.SphericalExpansion(**RASCAL_SETTINGS[\"hypers\"])\n",
    "density = calculator.compute(\n",
    "    all_frames, \n",
    "    selected_samples=Labels(names=[\"structure\"], values=idxs.reshape(-1, 1)),\n",
    "    **RASCAL_SETTINGS[\"compute\"],\n",
    ")\n",
    "density = density.keys_to_properties(\n",
    "    keys_to_move=Labels(\n",
    "        names=[\"species_neighbor\"], values=np.array(DATA_SETTINGS[\"global_species\"]).reshape(-1, 1)\n",
    "    )\n",
    ")\n",
    "\n",
    "# Build a lambda-SOAP descriptor by a Clebsch-Gordan combination\n",
    "# Build lambda-SOAP vector\n",
    "lsoap = clebsch_gordan.correlate_density(\n",
    "    density,\n",
    "    correlation_order=CG_SETTINGS[\"correlation_order\"],\n",
    "    angular_cutoff=CG_SETTINGS[\"angular_cutoff\"],\n",
    "    selected_keys=Labels(\n",
    "        names=[\"spherical_harmonics_l\", \"inversion_sigma\"],\n",
    "        values=np.array(CG_SETTINGS[\"selected_keys\"], dtype=np.int32),\n",
    "    ),\n",
    "    skip_redundant=CG_SETTINGS[\"skip_redundant\"],\n",
    ")\n",
    "\n",
    "# Check the resulting structure indices match those in `idxs`\n",
    "assert np.all(\n",
    "    np.sort(idxs)\n",
    "    == metatensor.unique_metadata(lsoap, \"samples\", \"structure\").values.reshape(-1)\n",
    ")\n",
    "\n",
    "# Split into per-structure TensorMaps and save into separate directories.\n",
    "# This is useful for batched training.\n",
    "for A in idxs:\n",
    "    lsoap_A = metatensor.slice(\n",
    "        lsoap,\n",
    "        \"samples\",\n",
    "        labels=Labels(names=\"structure\", values=np.array([A]).reshape(-1, 1)),\n",
    "    )\n",
    "    metatensor.save(os.path.join(processed_dir(A, ri_restart_idx), \"lsoap.npz\"), lsoap_A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The settings used to build the descriptor from an `.xyz` file, as well as build\n",
    "the desired target property (the real-space scalar field) from the model\n",
    "prediction need to be stored so that the perform can make a true end-to-end\n",
    "prediction.\n",
    "\n",
    "In the `rholearn` module \"predictor.py\", the functions `descriptor_builder` and\n",
    "`target_builder` are implemented to perform these transformations on the input\n",
    "and output side of the model, respectively. Both take a\n",
    "variable input that depends on the structure being predicted on, and some\n",
    "settings for performing the relevant transformations of the data. The key\n",
    "physics-related settings used in predicting on an unseen structure shouldbe the\n",
    "same as were used for generating the data the model was trained on.\n",
    "\n",
    "Here we store the relevant settings in dictionaries, which will be used to\n",
    "initialize the ML model later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build cross-validation `IndexedDataset` objects\n",
    "\n",
    "* For cross-validation we create a train-test-val split of the data.\n",
    "* Data is stored on the per-structure basis to help with mini-batching in\n",
    "  training.\n",
    "* `metatensor-learn` is used here to build the datasets and dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from settings import CROSSVAL_SETTINGS, ML_SETTINGS, TORCH_SETTINGS\n",
    "\n",
    "# Perform a train/test/val split of structure idxs\n",
    "train_idxs, test_idxs, val_idxs = data.group_idxs(\n",
    "    idxs=idxs,\n",
    "    n_groups=CROSSVAL_SETTINGS[\"n_groups\"],\n",
    "    group_sizes=CROSSVAL_SETTINGS[\"group_sizes\"],\n",
    "    shuffle=CROSSVAL_SETTINGS[\"shuffle\"],\n",
    "    seed=DATA_SETTINGS[\"seed\"],\n",
    ")\n",
    "print(\n",
    "    \"num train_idxs:\",\n",
    "    len(train_idxs),\n",
    "    \"   num test_idxs:\",\n",
    "    len(test_idxs),\n",
    "    \"   num val_idxs:\",\n",
    "    len(val_idxs),\n",
    ")\n",
    "np.savez(\n",
    "    os.path.join(ML_DIR, \"idxs.npz\"),\n",
    "    idxs=idxs,\n",
    "    train_idxs=train_idxs,\n",
    "    test_idxs=test_idxs,\n",
    "    val_idxs=val_idxs,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Visualise the dataset again, this time colored by its cross-validation category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crossval_category = lambda A: 0 if A in train_idxs else (1 if A in test_idxs else 2)\n",
    "\n",
    "chemiscope.show(\n",
    "    frames,\n",
    "    properties={\n",
    "        \"Mean O-H bond length, Angstrom\": [\n",
    "            np.mean([frame.get_distance(0, 1), frame.get_distance(0, 2)])\n",
    "            for A, frame in zip(idxs, frames)\n",
    "        ],\n",
    "        \"H-O-H angle, degrees\": [\n",
    "            frame.get_angle(1, 0, 2) for A, frame in zip(idxs, frames)\n",
    "        ],\n",
    "        \"0: train, 1: test, 2: val\": [crossval_category(A) for A, frame in zip(idxs, frames)]\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from metatensor.learn.data import Dataset, DataLoader, IndexedDataset\n",
    "from settings import CROSSVAL_SETTINGS, TORCH_SETTINGS\n",
    "\n",
    "def load_to_torch(\n",
    "    path: str, torch_kwargs: dict, drop_blocks: bool = False\n",
    ") -> TensorMap:\n",
    "    \"\"\"Loads a TensorMap from file and converts its backend to torch\"\"\"\n",
    "    tensor = metatensor.io.load_custom_array(\n",
    "        path,\n",
    "        create_array=metatensor.io.create_torch_array,\n",
    "    )\n",
    "    if drop_blocks:\n",
    "        tensor = metatensor.drop_blocks(\n",
    "            tensor, \n",
    "            keys=Labels(\n",
    "                names=[\"spherical_harmonics_l\", \"species_center\"],\n",
    "                values=np.array([[5, 1]]),\n",
    "            ),\n",
    "        )\n",
    "    tensor = tensor.to(**torch_kwargs)\n",
    "    tensor = metatensor.requires_grad(tensor, True)\n",
    "    return tensor\n",
    "\n",
    "\n",
    "train_dataset, test_dataset, val_dataset = [\n",
    "    IndexedDataset(\n",
    "        sample_ids=subset_idxs,\n",
    "        frames=[all_frames[A] for A in subset_idxs],\n",
    "        descriptors=[\n",
    "            load_to_torch(\n",
    "                os.path.join(processed_dir(A, ri_restart_idx), \"lsoap.npz\"), \n",
    "                TORCH_SETTINGS, \n",
    "                drop_blocks=True\n",
    "            )\n",
    "            for A in subset_idxs\n",
    "        ],\n",
    "        targets=[\n",
    "            load_to_torch(os.path.join(processed_dir(A, ri_restart_idx), \"ri_coeffs.npz\"), TORCH_SETTINGS) \n",
    "            for A in subset_idxs\n",
    "        ],\n",
    "        auxiliaries=[\n",
    "            load_to_torch(os.path.join(processed_dir(A, ri_restart_idx), \"ri_ovlp.npz\"), TORCH_SETTINGS) \n",
    "            for A in subset_idxs\n",
    "        ],\n",
    "    ) for subset_idxs in [train_idxs, test_idxs, val_idxs]\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize model\n",
    "\n",
    "* As typically the magnitude of the invariant RI-coefficients are many orders of\n",
    "  magnitude larger than the covariant coefficients, it often helps model\n",
    "  training if the invariant block models predict a baselined quantity. The\n",
    "  baseline is then added back in on the TensorMap level before loss evaluation,\n",
    "  acting as a kind of non-learnable bias.\n",
    "\n",
    "* We can compute the mean features of the invariant blocks of the training data\n",
    "  and initialize the model with this. Alternatively, one could use free-atom\n",
    "  superpositions of the scalar field of interest as the baseline.\n",
    "\n",
    "* We can also calculate the standard deviation of the training data. This is\n",
    "  defined relative to this invariant baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ML_SETTINGS[\"model\"][\"use_invariant_baseline\"]:\n",
    "    invariant_baseline = data.get_dataset_invariant_means(\n",
    "        train_dataset, field=\"targets\", torch_kwargs=TORCH_SETTINGS,\n",
    "    )\n",
    "else:\n",
    "    invariant_baseline = None\n",
    "\n",
    "stddev = data.get_standard_deviation(\n",
    "    dataset=train_dataset,\n",
    "    field=\"targets\",\n",
    "    torch_kwargs=TORCH_SETTINGS,\n",
    "    invariant_baseline=invariant_baseline,\n",
    "    overlap_field=\"auxiliaries\",\n",
    ")\n",
    "np.savez(os.path.join(ML_DIR, \"stddev.npz\"), stddev=stddev.detach().numpy())\n",
    "stddev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In order to perform end-to-end predictions, we also need to initialize the\n",
    "  model with the settings used to build the equivariant descriptor and final\n",
    "  target property. These will be the same as above, used to generate the\n",
    "  training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from settings import RASCAL_SETTINGS, CG_SETTINGS, BASE_AIMS_KWARGS\n",
    "\n",
    "# For descriptor building, we need to store the rascaline settings for\n",
    "# generating a SphericalExpansion and performing Clebsch-Gordan combinations.\n",
    "# The `descriptor_builder` function in \"predictor.py\" contains the 'recipe' for\n",
    "# using these settings to transform an ASE Atoms object.\n",
    "descriptor_kwargs = {\n",
    "    \"rascal_settings\": RASCAL_SETTINGS,\n",
    "    \"cg_settings\": CG_SETTINGS,\n",
    "}\n",
    "\n",
    "# For target building, the base AIMS settings need to be stored, along with the\n",
    "# basis set definition.\n",
    "basis_set = io.unpickle_dict(os.path.join(processed_dir(A, ri_restart_idx), \"calc_info.pickle\"))[\"basis_set\"]\n",
    "\n",
    "target_kwargs = {\n",
    "    \"aims_kwargs\": {**BASE_AIMS_KWARGS},\n",
    "    \"basis_set\": {**basis_set},\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "model = models.RhoModel(\n",
    "    # Standard model architecture\n",
    "    model_type=ML_SETTINGS[\"model\"][\"model_type\"],  # \"linear\" or \"nonlinear\"\n",
    "    input=train_dataset[0].descriptors,   # example input data for init metadata\n",
    "    output=train_dataset[0].targets,  # example output data for init metadata\n",
    "    bias_invariants=ML_SETTINGS[\"model\"][\"bias_invariants\"],\n",
    "\n",
    "    # Architecture settings if using a nonlinear base model\n",
    "    hidden_layer_widths=ML_SETTINGS[\"model\"].get(\"hidden_layer_widths\"),\n",
    "    activation_fn=ML_SETTINGS[\"model\"].get(\"activation_fn\"),\n",
    "    bias_nn=ML_SETTINGS[\"model\"].get(\"bias_nn\"),\n",
    "\n",
    "    # Invariant baselining\n",
    "    invariant_baseline=invariant_baseline,\n",
    "\n",
    "    # Settings for descriptor/target building\n",
    "    descriptor_kwargs=descriptor_kwargs,\n",
    "    target_kwargs=target_kwargs,\n",
    "\n",
    "    # Torch tensor settings\n",
    "    **TORCH_SETTINGS\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Let's update the model with some settings needed to rebuild the\n",
    "  real-space density from RI-coefficients in AIMS, needed for computing the L1 error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from settings import AIMS_PATH, REBUILD_KWARGS, SBATCH_KWARGS\n",
    "\n",
    "# Update the AIMS and SBATCH kwargs\n",
    "tmp_aims_kwargs = {**model.target_kwargs[\"aims_kwargs\"]}\n",
    "tmp_aims_kwargs.update(REBUILD_KWARGS)\n",
    "model.update_target_kwargs(\n",
    "    {\n",
    "        \"aims_path\": AIMS_PATH,\n",
    "        \"aims_kwargs\": tmp_aims_kwargs,\n",
    "        \"sbatch_kwargs\": SBATCH_KWARGS,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize training objects: loaders, loss, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import metatensor.learn\n",
    "from metatensor.learn.data import group\n",
    "\n",
    "# Construct dataloaders\n",
    "train_loader = metatensor.learn.data.DataLoader(\n",
    "    train_dataset,\n",
    "    collate_fn=group,\n",
    "    batch_size=ML_SETTINGS[\"loading\"][\"batch_size\"],\n",
    "    **ML_SETTINGS[\"loading\"][\"args\"],\n",
    ")\n",
    "\n",
    "val_loader = metatensor.learn.data.DataLoader(\n",
    "    val_dataset,\n",
    "    collate_fn=group,\n",
    "    batch_size=ML_SETTINGS[\"loading\"][\"batch_size\"],\n",
    "    **ML_SETTINGS[\"loading\"][\"args\"],\n",
    ")\n",
    "\n",
    "test_loader = metatensor.learn.data.DataLoader(\n",
    "    test_dataset,\n",
    "    collate_fn=group,\n",
    "    batch_size=ML_SETTINGS[\"loading\"][\"batch_size\"],\n",
    "    **ML_SETTINGS[\"loading\"][\"args\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize loss fxn and optimizer (don't use a scheduler for now)\n",
    "loss_fn = loss.L2Loss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run model training\n",
    "\n",
    "* We train the model by gradient descent, evaluating the loss of the prediction\n",
    "  against the density-fitted quantity. As the learning target is the real-space\n",
    "  density, which has been expanded on a non-orthogonal basis, and not just the\n",
    "  RI coefficients, the overlap matrices must be used in loss evaluation:\n",
    "\n",
    "* For a structure $A$, the L2 loss of the predicted scalar field relative to the **RI\n",
    "  approximated scalar field** is defined as:\n",
    "\n",
    "  * $ \\mathcal{L}_A^{\\text{L2}} = \\left( \\textbf{c}_A^{\\text{ML}} - \\textbf{c}_A^{\\text{RI}} \\right) \\ . \\ \\hat{S}_A \\ . \\ \\left( \\textbf{c}_A^{\\text{ML}} - \\textbf{c}_A^{\\text{RI}} \\right)  $\n",
    "\n",
    "\n",
    "* And L1 (MAE) error of the of the ML prediction relative to the **true QM\n",
    "  scalar field** is defined as: \n",
    "\n",
    "  * $ \\mathcal{L}_A^{\\text{L1}} = \\frac{1}{\\int d\\textbf{r} \\rho_A^{\\text{QM}} (\\textbf{r})} \\int d\\textbf{r} \\vert \\rho_A^{\\text{ML}} (\\textbf{r}) - \\rho_A^{\\text{QM}} (\\textbf{r}) \\vert $\n",
    "  \n",
    "* While the L2 loss is used to train the model, we will also periodically\n",
    "  calculate the L1 loss to assess the performance of the model relative to the\n",
    "  actual true QM scalar field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rholearn import train\n",
    "from settings import ML_SETTINGS\n",
    "\n",
    "# Define a log file for writing losses at each epoch\n",
    "log_path = os.path.join(ML_DIR, \"training.log\")\n",
    "io.log(log_path, \"# epoch train_loss val_loss test_error time\")\n",
    "\n",
    "# Run training loop\n",
    "for epoch in range(1, ML_SETTINGS[\"training\"][\"n_epochs\"] + 1):\n",
    "\n",
    "    # ====== Training step ======\n",
    "    t0 = time.time()\n",
    "    train_loss_epoch, val_loss_epoch = train.training_step(\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        model=model,\n",
    "        loss_fn=loss_fn,\n",
    "        optimizer=optimizer,\n",
    "        scheduler=scheduler,\n",
    "        check_args=epoch == 1  # Check metadata only on 1st epoch\n",
    "    )\n",
    "\n",
    "    # ====== Evaluation step ======\n",
    "    test_error_epoch = np.nan\n",
    "    if epoch % ML_SETTINGS[\"evaluation\"][\"interval\"] == 0:\n",
    "        loaders = {\"train\": train_loader, \"val\": val_loader, \"test\": test_loader}\n",
    "        test_error_epoch = train.evaluation_step(\n",
    "            model,\n",
    "            dataloader=loaders[ML_SETTINGS[\"evaluation\"][\"subset\"]],\n",
    "            save_dir=partial(eval_dir, epoch=epoch),\n",
    "            calculate_error=ML_SETTINGS[\"evaluation\"][\"calculate_error\"],\n",
    "            target_type=ML_SETTINGS[\"evaluation\"][\"target_type\"],\n",
    "            reference_dir=partial(ri_dir, restart_idx=ri_restart_idx),\n",
    "        )\n",
    "\n",
    "    # ====== Log results ======\n",
    "    dt = time.time() - t0\n",
    "    io.log(\n",
    "        log_path,\n",
    "        f\"{epoch} {train_loss_epoch} {val_loss_epoch} {test_error_epoch} {dt}\",\n",
    "    )\n",
    "    if epoch % ML_SETTINGS[\"training\"][\"save_interval\"] == 0:\n",
    "        torch.save(model, os.path.join(chkpt_dir, f\"model_{epoch}.pt\"))\n",
    "        torch.save(optimizer.state_dict, os.path.join(chkpt_dir, f\"opt_{epoch}.pt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* This process is slow in a notebook - better to make use of HPC resources! The\n",
    "  training procedure can essentially be copied to a seperate python script and\n",
    "  run using a job scheduler - see \"run_training.py\".\n",
    "\n",
    "* We can load a model that has been pre-trained and validate its performance\n",
    "\n",
    "* Here we load a model trained to over 1500 epochs, only a dataset of only 10\n",
    "  water molecules."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate model performance on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model from checkpoint and the training log file\n",
    "model = torch.load(os.path.join(chkpt_dir, \"model_25.pt\"))\n",
    "log_file = np.loadtxt(os.path.join(ML_DIR, \"training.log\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unpack data from each row\n",
    "epochs, train_loss, val_loss, test_error, times = losses.T\n",
    "\n",
    "# Plot the various errors\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(epochs, train_loss, label=\"L2 Loss (ML/RI), train\")\n",
    "ax.plot(epochs, val_loss, label=\"L2 Loss (ML/RI), val\")\n",
    "ax.scatter(\n",
    "    epochs, test_error / 100, label=\"L1 Error (ML/QM), train\", marker=\".\"\n",
    ")\n",
    "\n",
    "ax.set_xlabel(\"Epochs\")\n",
    "ax.set_ylabel(\"Error per structure\")\n",
    "ax.set_yscale(\"log\")\n",
    "ax.set_title(\"HOMO-learning w/ NN | water monomer | AIMS cluster calculation\")\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End-to-end prediction on the test set\n",
    "\n",
    "* Now let's make an end-to-end prediction (from xyz -> real-space scalar field)\n",
    "  on the test set, up until now unseen by the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a directory for storing the prediction\n",
    "pred_dir = lambda A: os.path.join(ML_DIR, \"evaluation\", \"final\", f\"{A}\")\n",
    "test_sample = test_dataset[0]\n",
    "\n",
    "model.target_kwargs[\"aims_kwargs\"][\"output\"] = [\"cube ri_fit\"]\n",
    "model.target_kwargs[\"aims_kwargs\"][\"ri_fit_write_rebuilt_field_cube\"] = True\n",
    "\n",
    "predictions = model.predict(\n",
    "    frames=[test_sample.frames],\n",
    "    structure_idxs=[test_sample.sample_id],\n",
    "    build_targets=True,\n",
    "    return_targets=True,\n",
    "    save_dir=pred_dir,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate MAE\n",
    "grid = np.loadtxt(  # integration weights\n",
    "    os.path.join(ri_dir(test_sample.sample_id, ri_restart_idx), \"partition_tab.out\")\n",
    "    )\n",
    "target = np.loadtxt(  # target scalar field\n",
    "    os.path.join(ri_dir(test_sample.sample_id, ri_restart_idx), f\"rho_ref.out\")\n",
    ")\n",
    "percent_mae = aims_parser.get_percent_mae_between_fields(  # calc MAE\n",
    "    input=predictions[0],\n",
    "    target=target,\n",
    "    grid=grid,\n",
    ")\n",
    "percent_mae"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Let's visualize these predictions using the cube file outputs\n",
    "\n",
    "* Pick one validation structure and vizualize the target QM and predicted ML\n",
    "  HOMO scalar fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import py3Dmol\n",
    "\n",
    "# Visualize the predicted density of a single example structure\n",
    "A = test_sample.sample_id\n",
    "\n",
    "qm_cube = os.path.join(ri_dir(A, ri_restart_idx), \"rho_ref.cube\")\n",
    "ml_cube = os.path.join(pred_dir(A), \"rho_rebuilt.cube\")\n",
    "\n",
    "for cube_file in [qm_cube, ml_cube]:\n",
    "    v = py3Dmol.view()\n",
    "    v.addModelsAsFrames(open(cube_file, \"r\").read(), \"cube\")\n",
    "    v.setStyle({\"stick\": {}})\n",
    "    v.addVolumetricData(\n",
    "        open(cube_file, \"r\").read(),\n",
    "        \"cube\",\n",
    "        {\"isoval\": 0.002, \"color\": \"blue\", \"opacity\": 0.8},\n",
    "    )\n",
    "    v.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rho",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
