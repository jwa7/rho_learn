{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-requisites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Requirements\n",
    "\n",
    "* python > 3.10\n",
    "\n",
    "### Required packages\n",
    "\n",
    "For stability, it is best to install `rascaline` and `rho_learn` from specific commits.\n",
    "\n",
    "1. `pip install ase chemiscope metatensor torch==2.1.0 wigners`\n",
    "1. `pip install git+https://github.com/luthaf/rascaline.git@b2cedfe870541e6d037357db58de1901eb116c41`\n",
    "1. `pip install git+https://github.com/jwa7/rho_learn.git@9139fd560ff663236bc416621118fb0de9a36009`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Useful standard and scientific ML libraries\n",
    "import os\n",
    "import time\n",
    "import ase.io\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import py3Dmol\n",
    "import torch\n",
    "\n",
    "# M-Stack packages\n",
    "import metatensor   # storage format for atomistic ML\n",
    "import chemiscope  # interactive molecular visualization\n",
    "import rascaline   # generating structural representations\n",
    "from metatensor import Labels, TensorBlock, TensorMap\n",
    "from rascaline.utils import clebsch_gordan\n",
    "\n",
    "# Interfacing with FHI-aims\n",
    "from rhocalc.aims import aims_calc, aims_parser\n",
    "\n",
    "# Torch-based density leaning\n",
    "from rholearn import io, data, loss, models, predictor\n",
    "import settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from settings import TOP_DIR\n",
    "\n",
    "DATA_DIR = os.path.join(TOP_DIR, \"precalc_data\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Visualize structures in dataset\n",
    "\n",
    "* Use `chemiscope`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from settings import TOP_DIR, DATA_DIR, ML_DIR, DATA_SETTINGS\n",
    "\n",
    "print(\"Top directory defined as: \", TOP_DIR)\n",
    "\n",
    "# Load the frames in the complete dataset\n",
    "all_frames = DATA_SETTINGS[\"all_frames\"]\n",
    "\n",
    "# Shuffle the total set of structure indices\n",
    "idxs = np.arange(len(all_frames))\n",
    "np.random.default_rng(seed=DATA_SETTINGS[\"seed\"]).shuffle(idxs)\n",
    "\n",
    "# Take a subset of the frames if desired\n",
    "idxs = idxs[:DATA_SETTINGS[\"n_frames\"]]\n",
    "frames = [all_frames[A] for A in idxs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chemiscope.show(\n",
    "    frames,\n",
    "    properties={\n",
    "        \"Mean O-H bond length, Angstrom\": [np.mean([f.get_distance(0, 1), f.get_distance(0, 2)]) for f in frames],\n",
    "        \"H-O-H angle, degrees\": [f.get_angle(1, 0, 2) for f in frames],\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning targets\n",
    "\n",
    "* These have been pre-calculated, and are stored in the \"precalc_data/\"\n",
    "  directory. \n",
    "\n",
    "* In this directory are a series of subdirectories named according to the\n",
    "  indices of the structures from the original dataset\n",
    "  (`data/water_monomers_1k.xyz`) they correspond to. In each subdirectory are\n",
    "  the target RI-coefficients that expand the HOMO of the corresponding gas-phase\n",
    "  water monomer, and overlap matrix, calcualted with `FHI-aims`. Some\n",
    "  calculation info is stored in the pickled dictionary \"calc_info.pickle\".\n",
    "  \n",
    "* For demonstration purposes, data has been pre-calculated for 10 structures only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A callable that takes structure idx as an argument, returns path to processed\n",
    "# data (i.e. metatensor-format) that has been pre-calculated\n",
    "def processed_dir(A):\n",
    "     return os.path.join(TOP_DIR, \"precalc_data\", f\"{A}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Structural Descriptors \n",
    "\n",
    "* Here we construct $\\lambda$-SOAP equivariant descriptors for each structure\n",
    "* First, find the angular orders present in the decomposition of the target\n",
    "  scalar field onto the RI basis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The basis set definition should is consistent for all atoms in the dataset,\n",
    "# given consistent AIMS settings. Print this definiton from the parsed outputs\n",
    "# from one of the structures\n",
    "basis_set = io.unpickle_dict(\n",
    "    os.path.join(processed_dir(A=idxs[0]), \"calc_info.pickle\")\n",
    ")[\"basis_set\"]\n",
    "\n",
    "print(basis_set[\"def\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The maximum angular order here is $l = 8$. This should be reflected in the\n",
    "settings used to generate the equivariant descriptor - specifically in\n",
    "`CG_SETTINGS` in \"settings.py\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from settings import RASCAL_SETTINGS, CG_SETTINGS\n",
    "\n",
    "# Generate a rascaline SphericalExpansion (2 body) representation. As we want to\n",
    "# retain the original structure indices, we are going to pass * all * of the\n",
    "# 1000 frames to rascaline, but only compute for the subset of structures in\n",
    "# `idxs`.\n",
    "calculator = rascaline.SphericalExpansion(**RASCAL_SETTINGS[\"hypers\"])\n",
    "nu_1_tensor = calculator.compute(\n",
    "    all_frames, \n",
    "    selected_samples=Labels(names=[\"structure\"], values=idxs.reshape(-1, 1)),\n",
    "    **RASCAL_SETTINGS[\"compute\"],\n",
    ")\n",
    "nu_1_tensor = nu_1_tensor.keys_to_properties(\"species_neighbor\")\n",
    "\n",
    "# Build a lambda-SOAP descriptor by a CLebsch-Gordan combination\n",
    "lsoap = clebsch_gordan.lambda_soap_vector(nu_1_tensor, **CG_SETTINGS)\n",
    "\n",
    "# Check the resulting structure indices match those in `idxs`\n",
    "assert np.all(\n",
    "    np.sort(idxs)\n",
    "    == metatensor.unique_metadata(lsoap, \"samples\", \"structure\").values.reshape(-1)\n",
    ")\n",
    "\n",
    "# Split into per-structure TensorMaps and save into separate directories.\n",
    "# This is useful for batched training.\n",
    "for A in idxs:\n",
    "    lsoap_A = metatensor.slice(\n",
    "        lsoap,\n",
    "        \"samples\",\n",
    "        labels=Labels(names=\"structure\", values=np.array([A]).reshape(-1, 1)),\n",
    "    )\n",
    "    for restart_idx in [0]:\n",
    "        metatensor.save(os.path.join(processed_dir(A), \"lsoap.npz\"), lsoap_A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The settings used to build the descriptor from an `.xyz` file, as well as build\n",
    "the desired target property (the real-space scalar field) from the model\n",
    "prediction need to be stored so that the perform can make a true end-to-end\n",
    "prediction.\n",
    "\n",
    "In the `rholearn` module \"predictor.py\", the functions `descriptor_builder` and\n",
    "`target_builder` are implemented to perform these transformations on the input\n",
    "and output side of the model, respectively. Both take a\n",
    "variable input that depends on the structure being predicted on, and some\n",
    "settings for performing the relevant transformations of the data. The key\n",
    "physics-related settings used in predicting on an unseen structure shouldbe the\n",
    "same as were used for generating the data the model was trained on.\n",
    "\n",
    "Here we store the relevant settings in dictionaries, which will be used to\n",
    "initialize the ML model later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build `dataset`\n",
    "\n",
    "* For cross-validation we create a train-test-val split of the data.\n",
    "* Data is stored on the per-structure basis to help with mini-batching in\n",
    "  training.\n",
    "\n",
    "\n",
    "* Although we have generated data for both the HOMO and LUMO, let's just learn\n",
    "  the HOMO. Fix the \"restart_idx\" to 0 (for the HOMO), then we can build a\n",
    "  dataset and start training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix the restart index so that we just learn the HOMO\n",
    "restart_idx = 0\n",
    "\n",
    "# Define callable for saving predictions made during runtime\n",
    "def pred_dir(A):\n",
    "    return os.path.join(ML_DIR, \"predictions\", f\"{A}\")\n",
    "\n",
    "# Define dir where model checkpoints are saved\n",
    "chkpt_dir = os.path.join(ML_DIR, \"checkpoints\")\n",
    "\n",
    "if not os.path.exists(ML_DIR):\n",
    "    os.makedirs(ML_DIR)\n",
    "if not os.path.exists(os.path.join(ML_DIR, \"predictions\")):\n",
    "    os.makedirs(os.path.join(ML_DIR, \"predictions\"))\n",
    "if not os.path.exists(chkpt_dir):\n",
    "    os.makedirs(chkpt_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from settings import CROSSVAL_SETTINGS, ML_SETTINGS, TORCH_SETTINGS\n",
    "\n",
    "# Perform a train/test/val split of structure idxs\n",
    "train_idxs, test_idxs, val_idxs = data.group_idxs(\n",
    "    idxs=idxs,\n",
    "    n_groups=CROSSVAL_SETTINGS[\"n_groups\"],\n",
    "    group_sizes=CROSSVAL_SETTINGS[\"group_sizes\"],\n",
    "    shuffle=CROSSVAL_SETTINGS[\"shuffle\"],\n",
    "    seed=DATA_SETTINGS[\"seed\"],\n",
    ")\n",
    "print(\n",
    "    \"num train_idxs:\",\n",
    "    len(train_idxs),\n",
    "    \"   num test_idxs:\",\n",
    "    len(test_idxs),\n",
    "    \"   num val_idxs:\",\n",
    "    len(val_idxs),\n",
    ")\n",
    "np.savez(\n",
    "    os.path.join(ML_DIR, \"idxs.npz\"),\n",
    "    idxs=idxs,\n",
    "    train_idxs=train_idxs,\n",
    "    test_idxs=test_idxs,\n",
    "    val_idxs=val_idxs,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Visualise the dataset again, this time colored by its cross-validation category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crossval_category = lambda A: 0 if A in train_idxs else (1 if A in test_idxs else 2)\n",
    "\n",
    "chemiscope.show(\n",
    "    frames,\n",
    "    properties={\n",
    "        \"Mean O-H bond length, Angstrom\": [\n",
    "            np.mean([frame.get_distance(0, 1), frame.get_distance(0, 2)])\n",
    "            for A, frame in zip(idxs, frames)\n",
    "        ],\n",
    "        \"H-O-H angle, degrees\": [\n",
    "            frame.get_angle(1, 0, 2) for A, frame in zip(idxs, frames)\n",
    "        ],\n",
    "        \"0: train, 1: test, 2: val\": [crossval_category(A) for A, frame in zip(idxs, frames)]\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Although we have generated data for both the HOMO and LUMO, let's just learn\n",
    "  the HOMO. Fix the \"restart_idx\" to 0 (for the HOMO), then we can build a\n",
    "  dataset and start training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct dataset, defining callables to access the input, output, and overlap\n",
    "# data from the structure indices\n",
    "rho_data = data.RhoData(\n",
    "    idxs=idxs,\n",
    "    in_path=lambda A: os.path.join(processed_dir(A), \"lsoap.npz\"),\n",
    "    out_path=lambda A: os.path.join(processed_dir(A), \"ri_coeffs.npz\"),\n",
    "    aux_path=lambda A: os.path.join(processed_dir(A), \"ri_ovlp.npz\"),\n",
    "    keep_in_mem=ML_SETTINGS[\"loading\"][\"train\"][\"keep_in_mem\"],\n",
    "    **TORCH_SETTINGS,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize model\n",
    "\n",
    "* As typically the magnitude of the invariant RI-coefficients are many orders of\n",
    "  magnitude larger than the covariant coefficients, it often helps model\n",
    "  training if the invariant block models predict a baselined quantity. The\n",
    "  baseline is then added back in on the TensorMap level before loss evaluation,\n",
    "  acting as a kind of non-learnable bias.\n",
    "* We can compute the mean features of the invariant blocks of the training data\n",
    "  and initialize the model with this. Alternatively, one could use free-atom\n",
    "  superpositions of the scalar field of interest as the baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ML_SETTINGS[\"model\"][\"use_invariant_baseline\"]:\n",
    "    invariant_baseline = rho_data.get_invariant_means(\n",
    "        idxs=train_idxs, which_data=\"output\"\n",
    "    )\n",
    "else:\n",
    "    invariant_baseline = None\n",
    "\n",
    "for block in invariant_baseline:\n",
    "    print(block)\n",
    "    print(block.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We can also calculate the standard deviation of the training data. This is\n",
    "  defined relative to this invariant baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stddev = rho_data.get_standard_deviation(\n",
    "    idxs=train_idxs,\n",
    "    which_data=\"output\",\n",
    "    invariant_baseline=invariant_baseline,\n",
    "    use_overlaps=True,\n",
    ")\n",
    "np.savez(os.path.join(ML_DIR, \"stddev.npz\"), stddev=stddev.detach().numpy())\n",
    "stddev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In order to perform end-to-end predictions, we also need to initialize the\n",
    "  model with the settings used to build the equivariant descriptor and final\n",
    "  target property. These will be the same as above, used to generate the\n",
    "  training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from settings import RASCAL_SETTINGS, CG_SETTINGS, BASE_AIMS_KWARGS\n",
    "\n",
    "# For descriptor building, we need to store the rascaline settings for\n",
    "# generating a SphericalExpansion and performing Clebsch-Gordan combinations.\n",
    "# The `descriptor_builder` function in \"predictor.py\" contains the 'recipe' for\n",
    "# using these settings to transform an ASE Atoms object.\n",
    "descriptor_kwargs = {\n",
    "    \"RASCAL_SETTINGS\": RASCAL_SETTINGS,\n",
    "    \"CG_SETTINGS\": CG_SETTINGS,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "model = models.RhoModel(\n",
    "    # Standard model architecture\n",
    "    model_type=ML_SETTINGS[\"model\"][\"model_type\"],  # \"linear\" or \"nonlinear\"\n",
    "    input=rho_data[idxs[0]][1],   # example input data for init metadata\n",
    "    output=rho_data[idxs[0]][2],  # example output data for init metadata\n",
    "    bias_invariants=ML_SETTINGS[\"model\"][\"bias_invariants\"],\n",
    "\n",
    "    # Architecture settings if using a nonlinear base model\n",
    "    hidden_layer_widths=ML_SETTINGS[\"model\"].get(\"hidden_layer_widths\"),\n",
    "    activation_fn=ML_SETTINGS[\"model\"].get(\"activation_fn\"),\n",
    "    bias_nn=ML_SETTINGS[\"model\"].get(\"bias_nn\"),\n",
    "\n",
    "    # Invariant baselining\n",
    "    invariant_baseline=invariant_baseline,\n",
    "\n",
    "    # Settings for descriptor/target building\n",
    "    descriptor_kwargs=descriptor_kwargs,\n",
    "\n",
    "    # Torch tensor settings\n",
    "    **TORCH_SETTINGS\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize training objects: loaders, loss, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct dataloaders\n",
    "train_loader = data.RhoLoader(\n",
    "    rho_data,\n",
    "    idxs=train_idxs,\n",
    "    get_aux_data=True,  # load the overlap matrix\n",
    "    batch_size=ML_SETTINGS[\"loading\"][\"train\"][\"batch_size\"],\n",
    ")\n",
    "test_loader = data.RhoLoader(\n",
    "    rho_data,\n",
    "    idxs=test_idxs,\n",
    "    get_aux_data=True,   # load the overlap matrix\n",
    "    batch_size=ML_SETTINGS[\"loading\"][\"test\"][\"batch_size\"],\n",
    ")\n",
    "# For the validation set, we want to evaluate the performance of the model\n",
    "# against the real-space scalar field (requires calling AIMS), so the overlaps\n",
    "# do not need to be loaded.\n",
    "val_loader = data.RhoLoader(\n",
    "    rho_data,\n",
    "    idxs=val_idxs,\n",
    "    get_aux_data=False,\n",
    "    batch_size=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize loss fxn and optimizer (don't use a scheduler for now)\n",
    "loss_fn = loss.L2Loss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run model training\n",
    "\n",
    "* We train the model by gradient descent, evaluating the loss of the prediction\n",
    "  against the density-fitted quantity. As the learning target is the real-space\n",
    "  density, which has been expanded on a non-orthogonal basis, and not just the\n",
    "  RI coefficients, the overlap matrices must be used in loss evaluation:\n",
    "\n",
    "* For a structure $A$, the L2 loss of the predicted scalar field relative to the **RI\n",
    "  approximated scalar field** is defined as:\n",
    "\n",
    "  * $ \\mathcal{L}_A^{\\text{L2}} = \\left( \\textbf{c}_A^{\\text{ML}} - \\textbf{c}_A^{\\text{RI}} \\right) \\ . \\ \\hat{S}_A \\ . \\ \\left( \\textbf{c}_A^{\\text{ML}} - \\textbf{c}_A^{\\text{RI}} \\right)  $\n",
    "\n",
    "\n",
    "* And L1 (MAE) error of the of the ML prediction relative to the **true QM\n",
    "  scalar field** is defined as: \n",
    "\n",
    "  * $ \\mathcal{L}_A^{\\text{L1}} = \\frac{1}{\\int d\\textbf{r} \\rho_A^{\\text{QM}} (\\textbf{r})} \\int d\\textbf{r} \\vert \\rho_A^{\\text{ML}} (\\textbf{r}) - \\rho_A^{\\text{QM}} (\\textbf{r}) \\vert $\n",
    "  \n",
    "* While the L2 loss is used to train the model, we will also periodically\n",
    "  calculate the L1 loss to assess the performance of the model relative to the\n",
    "  actual true QM scalar field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rholearn import train\n",
    "from settings import ML_SETTINGS\n",
    "\n",
    "# Define a log file for writing losses at each epoch\n",
    "log_path = os.path.join(ML_DIR, \"training.log\")\n",
    "io.log(log_path, \"# epoch train_L2_loss test_L2_loss time\")\n",
    "\n",
    "# Run training loop\n",
    "for epoch in range(1, ML_SETTINGS[\"training\"][\"n_epochs\"] + 1):\n",
    "    # Training step\n",
    "    t0 = time.time()\n",
    "    train_loss_epoch, test_loss_epoch = train.training_step(\n",
    "        train_loader=train_loader,\n",
    "        test_loader=test_loader,\n",
    "        model=model,\n",
    "        loss_fn=loss_fn,\n",
    "        optimizer=optimizer,\n",
    "        scheduler=scheduler,\n",
    "        check_args=True\n",
    "        if epoch == 1\n",
    "        else False,  # Switch off metadata checks after 1st epoch\n",
    "    )\n",
    "\n",
    "    dt = time.time() - t0\n",
    "    print(\n",
    "        f\"epoch {epoch}  \",\n",
    "        f\"train_L2_loss {np.round(train_loss_epoch.detach().numpy(), 5)}  \"\n",
    "        f\"test_L2_loss {np.round(test_loss_epoch.detach().numpy(), 5)}  \"\n",
    "        f\"time {dt}\",\n",
    "    )\n",
    "    io.log(\n",
    "        log_path,\n",
    "        f\"{epoch} {train_loss_epoch} {test_loss_epoch} {dt}\",\n",
    "    )\n",
    "    if epoch % ML_SETTINGS[\"training\"][\"save_interval\"] == 0:\n",
    "        torch.save(model, os.path.join(chkpt_dir, f\"model_{epoch}.pt\"))\n",
    "        torch.save(optimizer.state_dict, os.path.join(chkpt_dir, f\"opt_{epoch}.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training log\n",
    "losses = np.loadtxt(os.path.join(ML_DIR, \"training.log\"))\n",
    "\n",
    "# Unpack data from each row\n",
    "epochs, train_loss, test_loss, times = losses.T\n",
    "\n",
    "# Plot the various errors\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(epochs, train_loss, label=\"L2 Loss (ML/RI), train\")\n",
    "ax.plot(epochs, test_loss, label=\"L2 Loss (ML/RI), test\")\n",
    "\n",
    "ax.set_xlabel(\"Epochs\")\n",
    "ax.set_ylabel(\"Error per structure\")\n",
    "ax.set_yscale(\"log\")\n",
    "ax.set_title(\"HOMO-learning w/ NN | water monomer | AIMS cluster calculation\")\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* This process is slow in a notebook - better to make use of HPC resources! The\n",
    "  training procedure can essentially be copied to a seperate python script and\n",
    "  run using a job scheduler - see \"run_training.py\".\n",
    "\n",
    "* We can load a model that has been pre-trained and validate its performance\n",
    "\n",
    "* Here we load a model trained to over 1500 epochs, only a dataset of only 10\n",
    "  water molecules."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate model performance on validation set\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained model and the training log file\n",
    "model = torch.load(os.path.join(TOP_DIR, \"pretrained_models\", \"model_1611.pt\"))\n",
    "log_file = np.loadtxt(os.path.join(TOP_DIR, \"pretrained_models\", \"training.log\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unpack data from each row\n",
    "epochs, train_loss, test_loss, train_mae, test_mae, val_mae, times = log_file.T\n",
    "\n",
    "# Plot the various errors\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(epochs, train_loss, label=\"L2 Loss (ML/RI), train\")\n",
    "ax.plot(epochs, test_loss, label=\"L2 Loss (ML/RI), test\")\n",
    "ax.scatter(\n",
    "    epochs[np.where(train_mae != -1)],\n",
    "    train_mae[np.where(train_mae != -1)] / 100,\n",
    "    label=\"L1 Error (ML/QM), train\",\n",
    "    marker=\".\",\n",
    ")\n",
    "ax.scatter(\n",
    "    epochs[np.where(test_mae != -1)],\n",
    "    test_mae[np.where(test_mae != -1)] / 100,\n",
    "    label=\"L1 Error (ML/QM), test\",\n",
    "    marker=\".\",\n",
    ")\n",
    "\n",
    "ax.set_xlabel(\"Epochs\")\n",
    "ax.set_ylabel(\"Error per structure\")\n",
    "ax.set_yscale(\"log\")\n",
    "ax.set_title(\"HOMO-learning w/ NN | water monomer | AIMS cluster calculation\")\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End-to-end prediction on the validation set\n",
    "\n",
    "* Now let's make an end-to-end prediction (from xyz -> real-space scalar field)\n",
    "  on the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the validation frames as ASE Atoms objects\n",
    "val_frames = [all_frames[A] for A in val_idxs]\n",
    "\n",
    "def val_dir(A):\n",
    "    return os.path.join(ML_DIR, \"validation\", f\"{A}\")\n",
    "\n",
    "# Make predictions for the validation set. Note: we could predict from the\n",
    "# descriptors that are already constructed here, but do so from ASE frames for\n",
    "# demonstrative purposes\n",
    "pred_coeffs = model.predict(\n",
    "    structure_idxs=val_idxs, frames=val_frames, build_target=False, save_dir=val_dir\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rho",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
